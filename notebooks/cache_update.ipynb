{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ff422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Cache preflight ==\n",
      "DATA_ROOT: /Volumes/Untitled/DL Project/data\n",
      "- Copernicus CDS: 1 locations, 60 dated files\n",
      "- Copernicus ERA5-Land: 1 locations, 329 dated files\n",
      "- Copernicus ERA5-Land TS: 1 locations, 0 dated files\n",
      "- Copernicus ERA5 (pressure): 1 locations, 316 dated files\n",
      "- Copernicus ERA5 (single): 1 locations, 3513 dated files\n",
      "- IEM ASOS: 5 locations, 41974 dated files\n",
      "- Meteostat: 5 locations, 27846 dated files\n",
      "- NASA POWER: 5 locations, 45375 dated files\n",
      "- NOAA ISD: 5 locations, 44666 dated files\n",
      "- NOAA LCD: 5 locations, 44654 dated files\n",
      "- Open-Meteo: 5 locations, 47205 dated files\n",
      "- OpenWeather: 5 locations, 1820 dated files\n",
      "- Tomorrow.io: 5 locations, 0 dated files\n",
      "- Visual Crossing: 5 locations, 45 dated files\n",
      "- WeatherAPI.com: 5 locations, 1830 dated files\n",
      "- Weatherbit: 5 locations, 0 dated files\n",
      "Found date span: 2000-01-01 -> 2025-11-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# ====================== Config ======================\n",
    "START_DATE = dt.date(2000, 1, 1)\n",
    "END_DATE   = dt.date(2025, 11, 5)\n",
    "LIMIT_TO_FOUND_SPAN = True  # clip x-axis to discovered min..max dates\n",
    "\n",
    "# Optional metadata for nicer titles if we recognize the folder name\n",
    "KNOWN_PROVIDER_META = {\n",
    "    \"tomorrow_io\":               (\"Tomorrow.io\",                 \"No cached data yet (expected 5m/1h timeline)\"),\n",
    "    \"open_meteo\":                (\"Open-Meteo\",                  \"Hourly cadence (1h)\"),\n",
    "    \"visual_crossing\":           (\"Visual Crossing\",             \"Hourly cadence (1h)\"),\n",
    "    \"noaa_isd\":                  (\"NOAA ISD\",                    \"Sub-hourly METAR (median ~53 min)\"),\n",
    "    \"noaa_lcd\":                  (\"NOAA LCD\",                    \"Sub-hourly LCD (median ~53 min)\"),\n",
    "    \"meteostat\":                 (\"Meteostat\",                   \"Hourly multi-source blend (1h)\"),\n",
    "    \"nasa_power\":                (\"NASA POWER\",                  \"Hourly NASA POWER (satellite/model)\"),\n",
    "    \"iem_asos\":                  (\"IEM ASOS\",                    \"1-min ASOS observations\"),\n",
    "    \"copernicus_era5_single\":    (\"Copernicus ERA5 (single)\",    \"Hourly ERA5 single levels\"),\n",
    "    \"copernicus_era5_land\":      (\"Copernicus ERA5-Land\",        \"Hourly ERA5-Land (0.1 deg grid)\"),\n",
    "    \"copernicus_era5_pressure\":  (\"Copernicus ERA5 (pressure)\",  \"Hourly ERA5 pressure levels (0.25 deg grid)\"),\n",
    "    \"copernicus_era5_land_timeseries\": (\"Copernicus ERA5-Land TS\",\"Hourly ERA5-Land point series\"),\n",
    "    \"openweather\":               (\"OpenWeather\",                 \"Hourly observations (1h)\"),\n",
    "    \"weatherbit\":                (\"Weatherbit\",                  \"No cached data yet\"),\n",
    "    \"weatherapi_com\":            (\"WeatherAPI.com\",              \"Hourly forecast/history (1h)\"),\n",
    "    \"copernicus_cds\":            (\"Copernicus CDS\",              \"Daily CSV exports\"),\n",
    "}\n",
    "\n",
    "# ====================== Locate data/ ======================\n",
    "def find_data_root(start: Optional[Path] = None, max_up: int = 6) -> Path:\n",
    "    \"\"\"\n",
    "    Look for a folder named 'data' starting at CWD and walking up.\n",
    "    Prefer a 'data' that actually contains provider subdirs or files.\n",
    "    \"\"\"\n",
    "    start = start or Path.cwd()\n",
    "    candidates = []\n",
    "    for i, base in enumerate([start, *start.parents]):\n",
    "        if i > max_up:\n",
    "            break\n",
    "        cand = base / \"data\"\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            has_subdir = any(p.is_dir() for p in cand.iterdir())\n",
    "            has_files_nested = any(cand.rglob(\"*.*\"))\n",
    "            if has_subdir or has_files_nested:\n",
    "                candidates.append(cand)\n",
    "    return candidates[0] if candidates else (start / \"data\")\n",
    "\n",
    "DATA_ROOT = find_data_root()\n",
    "LOG_DIR = DATA_ROOT.parent / \"logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / \"weather_export.log\"\n",
    "CACHE_IMAGE_PATH = DATA_ROOT / \"cache_coverage.png\"\n",
    "\n",
    "# ====================== Discovery & parsing ======================\n",
    "def pretty_label(key: str) -> str:\n",
    "    return key.replace(\"_\", \" \").replace(\"-\", \" \").title()\n",
    "\n",
    "def discover_providers(root: Path):\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    return sorted([p for p in root.iterdir() if p.is_dir()])\n",
    "\n",
    "def find_locations(provider_dir: Path):\n",
    "    if not provider_dir.exists():\n",
    "        return []\n",
    "    return sorted([p for p in provider_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "DATE_RE = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})\")\n",
    "\n",
    "def extract_date_from_name(path: Path) -> Optional[dt.date]:\n",
    "    m = DATE_RE.search(path.stem)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return dt.date.fromisoformat(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def collect_dates(location_dir: Path):\n",
    "    \"\"\"\n",
    "    Recursively glob common file types and extract dates from filenames.\n",
    "    Works with nested folders and names like '2024-07-01_12h.csv'.\n",
    "    \"\"\"\n",
    "    exts = (\"*.csv\", \"*.json\", \"*.parquet\", \"*.feather\")\n",
    "    dates = set()\n",
    "    for pat in exts:\n",
    "        for f in location_dir.rglob(pat):\n",
    "            d = extract_date_from_name(f)\n",
    "            if d:\n",
    "                dates.add(d)\n",
    "    return dates\n",
    "\n",
    "# ====================== Scan & preflight ======================\n",
    "providers = discover_providers(DATA_ROOT)\n",
    "provider_meta = []\n",
    "for pdir in providers:\n",
    "    key = pdir.name\n",
    "    label, resolution = KNOWN_PROVIDER_META.get(key, (pretty_label(key), \"\"))\n",
    "    provider_meta.append((key, label, resolution, pdir))\n",
    "\n",
    "summary_lines = []\n",
    "any_dates_found = False\n",
    "global_min: Optional[dt.date] = None\n",
    "global_max: Optional[dt.date] = None\n",
    "\n",
    "# provider_payload: (key, label, resolution, [(location_name, dates_set)])\n",
    "provider_payload = []\n",
    "\n",
    "for key, label, resolution, pdir in provider_meta:\n",
    "    loc_dirs = find_locations(pdir)\n",
    "    loc_payload = []\n",
    "    for loc_dir in loc_dirs:\n",
    "        dates = collect_dates(loc_dir)\n",
    "        loc_payload.append((loc_dir.name, dates))\n",
    "        if dates:\n",
    "            any_dates_found = True\n",
    "            dmin, dmax = min(dates), max(dates)\n",
    "            global_min = dmin if global_min is None else min(global_min, dmin)\n",
    "            global_max = dmax if global_max is None else max(global_max, dmax)\n",
    "\n",
    "    provider_payload.append((key, label, resolution, loc_payload))\n",
    "    total_files = sum(len(d) for _, d in loc_payload)\n",
    "    total_locs  = len(loc_payload)\n",
    "    summary_lines.append(f\"- {label}: {total_locs} locations, {total_files} dated files\")\n",
    "\n",
    "print(\"== Cache preflight ==\")\n",
    "print(f\"DATA_ROOT: {DATA_ROOT.resolve()}\")\n",
    "print(\"\\n\".join(summary_lines) if summary_lines else \"(no providers discovered)\")\n",
    "if any_dates_found:\n",
    "    print(f\"Found date span: {global_min} -> {global_max}\")\n",
    "else:\n",
    "    print(\"No dated files found. If this is unexpected, confirm the data root above matches your project tree.\")\n",
    "\n",
    "# ====================== Build index ======================\n",
    "if any_dates_found and LIMIT_TO_FOUND_SPAN:\n",
    "    date_index = pd.date_range(global_min, global_max, freq=\"D\")\n",
    "else:\n",
    "    date_index = pd.date_range(START_DATE, END_DATE, freq=\"D\")\n",
    "\n",
    "# Keep only panels that actually have any cached days\n",
    "panels = []\n",
    "for key, label, resolution, loc_payload in provider_payload:\n",
    "    rows = []\n",
    "    loc_names = []\n",
    "    for loc_name, dates in loc_payload:\n",
    "        if not dates:\n",
    "            continue\n",
    "        row = [1 if ts.date() in dates else 0 for ts in date_index]\n",
    "        rows.append(row)\n",
    "        loc_names.append(loc_name)\n",
    "    has_any = any(any(r) for r in rows) if rows else False\n",
    "    if has_any:\n",
    "        panels.append((label, resolution, loc_names, np.asarray(rows, dtype=int)))\n",
    "\n",
    "# ====================== Plot ======================\n",
    "if not panels:\n",
    "    fig, ax = plt.subplots(figsize=(10, 2), constrained_layout=True)\n",
    "    ax.text(0.5, 0.5, \"No cached files found in the discovered date span\",\n",
    "            ha=\"center\", va=\"center\")\n",
    "    ax.axis(\"off\")\n",
    "    CACHE_IMAGE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(CACHE_IMAGE_PATH, dpi=150)\n",
    "    plt.close(fig)\n",
    "    logging.info(\"Saved cached coverage chart to %s\", CACHE_IMAGE_PATH)\n",
    "else:\n",
    "    cmap = matplotlib.colors.ListedColormap([\"#f0f0f0\", \"#2ca02c\"])  # 0=missing, 1=cached\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "    mappable = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    mappable.set_array([])\n",
    "\n",
    "    nrows = len(panels)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, 1,\n",
    "        figsize=(16, 2.2 * nrows),\n",
    "        sharex=True,\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    if nrows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Slim, top colorbar (fallback if Matplotlib is older)\n",
    "    try:\n",
    "        cbar = fig.colorbar(\n",
    "            mappable, ax=axes, orientation=\"horizontal\",\n",
    "            fraction=0.02, pad=0.02, location=\"top\"\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Matplotlib < 3.6 doesn't support 'location'\n",
    "        cbar = fig.colorbar(\n",
    "            mappable, ax=axes, orientation=\"horizontal\",\n",
    "            fraction=0.02, pad=0.02\n",
    "        )\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels([\"Missing\", \"Cached\"])\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "    for ax, (label, resolution, loc_names, data) in zip(axes, panels):\n",
    "        ax.imshow(data, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "        ax.set_yticks(range(len(loc_names)))\n",
    "        ax.set_yticklabels(loc_names, fontsize=9)\n",
    "\n",
    "        tick_count = min(len(date_index), 8)\n",
    "        if tick_count > 0:\n",
    "            tick_positions = np.linspace(0, len(date_index) - 1, tick_count, dtype=int)\n",
    "            ax.set_xticks(tick_positions)\n",
    "            ax.set_xticklabels(\n",
    "                [date_index[i].date().isoformat() for i in tick_positions],\n",
    "                rotation=40, ha=\"right\", fontsize=9\n",
    "            )\n",
    "\n",
    "        title = f\"{label}\" + (f\" ({resolution})\" if resolution else \"\")\n",
    "        ax.set_title(title, fontsize=11)\n",
    "        ax.set_ylabel(\"Location\", fontsize=9)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Date\", fontsize=10)\n",
    "    fig.suptitle(\n",
    "        f\"Cached coverage {date_index[0].date().isoformat()} â€” {date_index[-1].date().isoformat()}\",\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "    CACHE_IMAGE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(CACHE_IMAGE_PATH, dpi=150)\n",
    "    plt.close(fig)\n",
    "    logging.info(\"Saved cached coverage chart to %s\", CACHE_IMAGE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
