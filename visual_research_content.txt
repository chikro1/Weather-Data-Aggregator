--- START OF 2009.11577v3.pdf ---
Cloud Cover Nowcasting with Deep Learning
LÂ´ea Berthomier
AI Lab
METEO FRANCE
Toulouse, France
lea.berthomier@meteo.fr
Bruno Pradel
AI Lab
METEO FRANCE
Toulouse, France
bruno.pradel@meteo.fr
Lior Perez
AI Lab
METEO FRANCE
Toulouse, France
lior.perez@meteo.fr
978-1-7281-8750-1/20/$31.00 Â©2020 IEEE
Abstractâ€”Nowcasting is a ï¬eld of meteorology which aims
at forecasting weather on a short term of up to a few hours.
In the meteorology landscape, this ï¬eld is rather speciï¬c as
it requires particular techniques, such as data extrapolation,
where conventional meteorology is generally based on physical
modeling. In this paper, we focus on cloud cover nowcasting,
which has various application areas such as satellite shots
optimisation and photovoltaic energy production forecast.
Following recent deep learning successes on multiple imagery
tasks, we applied Deep Convolutionnal Networks on Meteosat
satellite images for cloud cover nowcasting. We present the results
of several architectures specialized in image segmentation and
time series prediction. We selected the best models according to
machine learning metrics as well as meteorological metrics. All
selected architectures showed signiï¬cant improvements over per-
sistence and the well-known U-Net surpasses AROME physical
model.
Index Termsâ€”nowcasting, cloud, meteorology, deep learning
I. I NTRODUCTION
Cloud cover nowcasting aims at forecasting the position of
clouds on a short time scale of up to 3 hours. Historically, the
main task of weather services is to ensure the safety of life
and property. While having multiple applications, cloud cover
remains less critical for safety than other phenomena such
as precipitation or thunderstorms which explain its relatively
lower popularity in the literature. It remains a ï¬eld of interest
for meteorological organisations, for observation satellite man-
agement, to optimize their image shots, and for solar panels
management, to forecast their production of electricity. Put
simply, at METEO FRANCE, a nowcasting product is a fusion
of two approaches: pure data extrapolation techniques, which
perform better on the very short term (up to 1 hour), and a mix
between extrapolation and classical physical models which
remain the best option for further forecasting. Extrapolation
methods use data observed in the near past: mainly satellite
and radar images. In this paper, we will focus only on the
data extrapolation part and the experiments exhibited in the
paper only consider the next hour and a half. In order to study
the cloud cover, we will use satellite images resulting from
products of EUMETSAT, the European Organisation for the
Exploitation of Meteorological Satellites.
Section II will cover the state of the art of time series
prediction applied on images, which guided our choices of
neural networks architectures. In Section III, we describe
our methodology, metrics and choices of models. Section IV
presents a benchmark of several models on simple synthetic
data. Finally, Sections V, VI, VII and VIII present our bench-
mark and results on satellite images, including a study on the
inï¬‚uence of temporal depth as an input and a comparison with
two meteorological models.
II. B ACKGROUND AND RELATED WORK
Cloud nowcasting is mainly computed with techniques of
extrapolation of pixels or objects from wind vectors. For
instance [1] uses optical ï¬‚ow and ground-based sky imagery
for short-term solar forecasting and recent work applies ex-
trapolation of atmospheric motion vector for cloud classiï¬ca-
tion forecasting using images from geostationary satellite [2].
However, these methods require long computation times when
applied to large areas (around 10 minutes for a full scan disk),
which, added to the data reception time, often makes the ï¬rst
forecasts useless. In addition, these methods only rely on the
observed dynamics of the clouds, and therefore cannot predict
evolution in terms of size and shape of clouds, as underlined
in [3].
Deep Learning techniques, while they were only developed
in the recent years, show impressive results in the ï¬eld of
image processing such as image classiï¬cation, object detection
or image segmentation, and they require very short computa-
tion times for inference tasks. Convolutional neural networks
already yielded good results on optical ï¬‚ow computation [4]
and rainfall nowcasting [5], [6], [7], [8], [9]. Both rainfall and
cloud cover nowcasting aim at forecasting the future positions
of moving objects and are considering time series of images as
inputs and targets of computational models. While rainfall has
gained a lot of attention in the recent nowcasting literature, to
the best of our knowledge cloud cover nowcasting with deep
learning has only been explored in the case of predictions of
satellite images [10]. Consequently all rainfall deep learning
nowcasting related works will be of interest.
Nevertheless, rainfall and cloud cover differ in that cumu-
lative rainfall is a continuous variable computed on a pixel
over a period of time, while cloud cover is a discrete class
observed at a given moment on one pixel (see below for
a description of the Cloud Type classiï¬cation product by
EUMETSAT). Consequently, cloud cover classiï¬cation and
forecasting is akin to a task of image segmentation applied
to image time series. Thus, the output of a deep learning
cloud cover nowcasting model will consist in several ï¬elds
arXiv:2009.11577v3  [cs.CV]  17 Dec 2020
of probabilities of having a cloud on each pixel, one for
each time step, where rainfall nowcasting models output
only the most probable value of cumulated rainfall on each
pixel. This distribution of probability is an advantage of deep
learning techniques over extrapolation methods, which output
binary values, as it allows to take different kind of decisions
depending on the problem at hand.
Finally, cumulated rainfall is measured by doppler radar
every 5 minutes while cloud cover classiï¬cation results from
satellite images and are collected on longer time intervals,
ranging from 5 to 15 minutes depending on the satellite,
even if they offer larger views. For instance, Meteosat Second
Generation 4 (MSG4) captures a full scan disk of the planet
each 15 minutes. This difference in time step might have
consequences on the performance of the models and raises
the question of how much temporal depth should be given to
the model as an input.
III. M ETHODOLOGY
Our goal was to forecast the position of clouds on a
short time scale of 1h30â€™ from satellite images using deep
convolutional networks as our models. To this end, we used
the following data and methodology.
A. Data
Our data come from the â€Geostationary Nowcasting Cloud
Typeâ€ classiï¬cation product of EUMETSAT [11], classifying
clouds in 16 classes depending on their height and type. This
classiï¬cation is computed with various visible and infrared
channels images shot by Meteosat Second Generation (MSG),
a geostationary satellite at longitude 0 degree. The result is
a 3712 by 3712 pixels image computed every 15 minutes
and indicating the cloud type of each pixel. Our dataset was
composed of images from 2017 and 2018.
B. Experimental protocol
Our long term objective is to forecast the class, position
and height of the clouds for each pixel over the whole globe.
However, for the experiments presented in this paper, we
focused on forecasting only the position of the clouds, by
using binary images of cloud cover as our input data : the
value 1 indicates a cloud on a pixel and the value 0 indicates
a cloudless pixel. Due to computation issues, we also cropped
and projected the satellite images to keep only squares of size
256 by 256 pixels showing France. The ï¬nal resolution of the
images was 4.5 kilometers and each couple of images was
spaced by 15 minutes.
The input data of the model consisted of 4 binary images
(spanning 1 hour) and our goal was to forecast the cloud
cover for the next 1h30. Consequently, the output of the model
consisted of 6 images of values ranging between 0 and 1 and
representing the probability of having a cloud on each pixel.
Figure 1 shows an example of a sequence from the dataset.
Therefore, in order to train our models, we constituted a
dataset of sequences of 10 binary images, spanning 2h30. Data
from 2017 and the ï¬rst semester of 2018 were used as training
Fig. 1. Example of 6 images from a sequence of the test set.
Fig. 2. Diagram of the recurrent networkâ€™s principle : The xi are the modelâ€™s
input and the Ë†xi are the desired output. The output of each decoder is used
as input to compute the next time stepâ€™s forecast. Information between time
steps is communicated through the RNNâ€™s output.
set and the second semester of 2018 was used as validation
set.
Each of the models was trained 3 times on the dataset
during 20 epochs, meaning that each sample in the dataset
was seen 20 times by the network. All the following metrics
were computed as means of the 3 runs.
C. Metrics
Once the models were trained with the data, we computed
the mean squared error (MSE) of the models for each of the
6 output time steps. We compared these errors to the MSE
of the persistence, a common meteorological baseline used in
nowcasting, which consists in using the last input as prediction
for all the output time steps. Indeed, in short term forecasting,
the last observation is often the best prediction.
In addition, we also computed the MSE over the binarized
outputs of the model : each value of the output images was
rounded to 0 if it was lower than 0.5 or 1 if it was greater
than 0.5, before computing the MSE.
D. Neural networks architectures
We built 20 different models based on the following 4 types
of architecture, with several variations like the number of
Fig. 3. Reduce-LSTM - Principle of the dimension reduction.
convolutional layers and adding residual layers or inception
layers:
â€¢ Convolutional networks with stacked convolutional lay-
ers of varying kernel size.
â€¢ U-Net networks , implementing the work of [12] which
specializes in image segmentation, as well as variations
from this architecture (adding or removing convolutional
layers, removing the residuals, changing the activation
function, adding fully-connected layers, ...).
â€¢ Recurrent networks based on the diagram on Figure 2,
with variations in the architecture of the encoders and
decoders.
â€¢ LSTM networks adapted to our data type. As we deal
with time-series of 2D data, we couldnâ€™t ï¬t them directly
into a LSTM which expects 1D multi-channel data as
inputs. We therefore propose the following architecture
that we call Reduce-LSTM. We consider time as a
channel and reduce the 2D input tensors of shape Xâˆ—Y âˆ—4
to a 1D multi-channel tensors of size Y âˆ— 256 with an
encoder composed of (3,1) and (2,1) kernels for the
convolutional and max pooling layers. This dimension
reduction is illustrated in Figure 3. We then repeat this
process on the input after swapping the X and Y axes,
we stack the two resulting 1D multi-channel tensors, and
use this as an input for a traditional LSTM.
IV. B ENCHMARK ON SIMPLE MOVING SHAPES
We started our study by focusing on synthetic images of
simple moving shapes in order to check that our models could
perform a simple task of time series prediction. The dataset
was made of 5000 sequences of 10 images showing simple
shapes like squares or circles following randomly generated
rotations and linear movements. We made six datasets by
varying the speed, transparency and size of the shapes, in order
to test the limits of the models.
Each of the 20 models was run 3 times on each of the
6 datasets. Figure 4 shows the average mean squared error
(MSE) for the best model of each type and for each time
step. The MSE over the binarized outputs is also represented
with dashed lines. We can see that each model performs better
than the persistence and that the prediction gets worse as
the prediction time step is further in time, which was to be
expected. Our benchmark shows that the original U-Net model
performed better than the other models, with a MSE at 17
percent of the persistence, and a MSE over binarized outputs
at 21 percent of the persistence.
1 2 3 4 5 60
1
2
3
4
Â·10âˆ’2
Time step
MSE
MSE of the 4 best models on synthetic data
Persistence
U-Net
CNN
LSTM
RNN
Fig. 4. Average MSE of each of the 4 best models and the persistence for each
time step of the prediction. The dashed lines represent the MSE computed on
binarized outputs.
Fig. 5. Two examples of U-Netâ€™s predictions : the ï¬rst line is the input data,
the second line is the ground truth and the third line is the output of the
model.
When examining the outputs of the models, this ï¬rst study
shows promising results : Figure 5 shows predictions made by
the U-Net model, using input from the test set. The network
manages to make good predictions of the future position of
the object and its shape. It even does a fairly good job when
objects overlap in the input images, making them difï¬cult to
distinguish.
15 30 45 60 75 900
5 Â· 10âˆ’2
0.1
0.15
0.2
0.25
Time step (min)
MSE
MSE for the 4 best models on satellite data
Persistence
U-Net
CNN
LSTM
RNN
Fig. 6. Average MSE of each model and the persistence for each time step
of the prediction. The dashed lines represent the MSE computed on binarized
values.
Fig. 7. Example of prediction made by the U-Net architecture. Left ï¬gure:
last input image received by the model. Right ï¬gure: last output of the model.
This study on synthetic data also reveals that the main
source of error of the predictions is the architecture of the
neural networks. Indeed, the exact future position of the shapes
can be theoretically computed from the last two input images,
at least when the shapes donâ€™t overlap. The error cannot be
attributed to a weak signal in the inputs.
V. B ENCHMARK ON REAL SATELLITE DATA
Based on the ï¬rst study on synthetic data, we selected four
models, the best of each category, and tested them on the
satellite data described in section III.
Figure 6 shows the average MSE at each time step for
each model, as well as the MSE computed over binarized
images, compared to the persistence. We can see that each
model performed better than persistence, even on the ï¬rst
prediction, and that the U-Net got an error 53 percent smaller
than persistence. In addition, the MSE on binarized outputs
is 75 percent smaller than persistence. All models managed
signiï¬cant improvement of persistence and were able to learn
to predict the movement of cloud covers. Figure 7 shows an
15 30 45 60 75 900
5 Â· 10âˆ’2
0.1
0.15
0.2
Time step (min)
MSE
MSE for the U-Net model
Persistence
2 inputs or more
1 input
Fig. 8. Average MSE of the U-Net model and the persistence, with different
number of input images, for each time step of the prediction.
Fig. 9. Single input experiment: the left image represents the last input given
to the model; the right image is the difference between the last output (t+1h30)
and the input. Appearing clouds are represented in red and disappearing clouds
are in blue.
example of prediction made by the U-Net model, comparing
the last input of the model and its last prediction.
We can see that stationary clouds (East and South-West) are
correctly predicted. In addition, the global movement of clouds
is well represented : the North-West clouds are correctly
moving to the North. Finally, the cloud appearing in the North
is well approximated despite its blurry edges.
VI. T EMPORAL DEPTH OF INPUTS
Then, we studied the inï¬‚uence of the number of input
images on the performance of the model. For this experiment,
we only studied the U-Net model, as it performed better
than the others on the previous experiments. The idea was to
check if the model could use older input data to make better
predictions, or if the last four images considered so far were
enough to predict the future positions of clouds.
To this end, we varied the length of input data by changing
the number of input images given to the model, which ranged
between 1 and 6, corresponding to 15 minutes to 1h30 of
temporal depth. This experiment showed that using more
than 2 input images did not offer any beneï¬ts compared to
using only 2 inputs: the variations in the MSE and in the
output images were not signiï¬cant. This result supports the
conclusions of [8] on rainfall nowcasting which shows that
reducing the inputâ€™s temporal context from 90 minutes to 30
minutes does not affect the performances signiï¬cantly.
In addition, using only one input image still results in a
better performance than persistence in terms of MSE, even
when the model shouldnâ€™t have any indication of cloud move-
ments. Figure 8 represents the evolution of MSE regarding
the number of input images considered and ï¬gure 9 shows
an example of output from the U-Net trained with only one
input image. We can see that the model added clouds on the
east of existing clouds (in red), which tends to make them
move to the east. As the model was only trained on France
area, our hypothesis is that the model learned a part of the
local climatology of France where clouds are mostly moving
from west to east. Therefore, the model can predict the most
probable movement of cloud masses even with only one input
image.
VII. S IZE AND SHAPE EVOLUTION
A well known weakness of data extrapolation techniques is
their difï¬culty to model size and shape evolution of clouds. Ex-
trapolation techniques shift existing clouds, tending to preserve
initial dimensions. The ability of a model to take into account
this attribute of clouds is very hard to measure accurately but
simply observing the image outputted, we can get an idea.
As shown in Figure 10, we extracted multiple sequences in
which deep learning models increase or decrease the size of
an existing cloud. However, new clouds are not created by our
architecture (which would have been surprising).
Fig. 10. Ground truth: in the area enclosed by a red line, we observe a cloud
increasing in volume between +15min and +90min. Forecast: We exhibit U-
net predictions at the corresponding times. The model correctly enlarges the
surface of the cloud.
VIII. C OMPARISON WITH METEOROLOGICAL MODELS
Finally, we compared the performance of the U-Net network
and persistence with two other meteorological models :
â€¢ EXIM : Eumetsat image extrapolation tool [2] which uses
high resolution atmospheric wind vectors to forecast the
position and type of clouds up to 2h30 over the whole
Meteosat disk, with a time step of 15 minutes.
â€¢ AROME : METEO FRANCE non-hydrostatic very-high-
resolution model [13] [14], based on physical modelling,
which forecasts several physical parameters up to 48
hours over the French territory, with a time step of 1
hour.
It should be noted that EXIMâ€™s forecasts are available
20 minutes after the beginning of the run, and AROMEâ€™s
are available after an hour. Thus, the ï¬rst time steps of
the forecasts are unusable. Meanwhile, the inference of one
sample with the U-Net model can be computed in about 20
seconds on an average laptop, and persistenceâ€™s forecast are
available instantly, therefore all of their forecasts can be used
in practice.
For this comparison, as we want to compare the perfor-
mances of the models at equivalent runs, the forecasts are
considered available immediately after the beginning of the
run, which is favorable to EXIM and AROME.
In addition, the domain of this comparison is still the French
territory but the time span is February to May 2019, as EXIMâ€™s
data was not available on a larger span at the time of the study.
Figure 11 represents the evolution of the MSE for each of
the 4 models. The MSE over the binarized outputs is also
represented with a dashed line. Our comparison shows that
the U-Net model performed better than the other models.
In addition, we can notice that persistence surpasses EXIM
after 45 minutes, which shows the limitations of classical
extrapolation methods. As expected, AROMEâ€™s MSE is worse
15 30 45 60 75 90 105 120 135 1500
5 Â· 10âˆ’2
0.1
0.15
0.2
Time step (min)
MSE
Comparison of the U-Netâ€™s MSE with other meteorological models
Persistence
U-Net
EXIM
AROME
Fig. 11. Average MSE of the U-Net model, persistence, EXIM and AROME
for each time step of the forecast. The dashed line represent the MSE
computed on binarized values.
than the other models for the ï¬rst time steps, but itâ€™s MSE
is slowly decreasing and becomes better than EXIM and the
persistence after 2 hours. Indeed, as we could expect from a
physical model, AROME shows its strength on mid to long
term forecasts, when extrapolation methods are no longer
relevant.
IX. C ONCLUSION AND FUTURE WORK
We provided results of multiple deep learning architectures
on cloud cover nowcasting, more precisely on the data ex-
trapolation task every 15 minutes and up to an hour and
a half. All architectures proved to outperform signiï¬cantly
persistence at each step, in terms of MSE. So far, U-net
architecture has proven to be the best for this task and
we didnâ€™t ï¬nd any custom architecture to surpass it. While
persistence is a baseline of interest in meteorology, U-Net also
surpasses AROME, METEO FRANCE non-hydrostatic very-
high-resolution model, and EXIM, Eumetsat image extrapola-
tion tool, while demanding signiï¬cantly less computation time.
As of today, our neural network architectures are not capable
of predicting the emergence of new clouds. This limitation is
shared by all pure data extrapolation techniques to the best
of our knowledge. Adding other data sources in the hope of
overcoming this limitation will be the subject of another study.
We also plan to use user-tailored metrics for satellite or solar
panels applications.
Finally, it will be of interest to try extending the length of
the forecasts and measure when AROMEâ€™s physical modelling
surpasses deep learning techniques, in the hope that deep
learning could eventually perform the ï¬nal bridge between
very short-term and mid-term forecasting.
ACKNOWLEDGMENT
We would like to thank V . Chabot, M. Claudon, C. Jauffret,
G. Larvor and M. Sorel from METEO FRANCE for their help
and advice. We also thank EUMETSAT for the production and
availability of satellite images and the Centre de MÂ´etÂ´eorologie
Spatiale for the Cloud Type product. Finally, we are deeply
grateful to the Fonds de Transformation de lâ€™Action Publique
(French government program for public services moderniza-
tion) whom funding allowed the genesis of the Artiï¬cial
Intelligence Laboratory of METEO FRANCE.
REFERENCES
[1] Wood-Bradley, P., Zapata, J., and Pye, J. (2012). Cloud tracking with
optical ï¬‚ow for short-term solar forecasting. Solar Thermal Group,
Australian National University, Canberra, Australia.
[2] EUMETSAT NWCSAF (2019). Algorithm theoretical basis
document for the extrapolated imagery processor of the
NWC/GEO. http://www.nwcsaf.org/documents/27295/0/
NWC-CDOP2-GEO-ZAMG-SW-ATBD-EXIM_v2.1.pdf
[3] Papadakis, N., H Â´eas, P., and M Â´emin, E. (2007). Image assimilation for
motion estimation of atmospheric layers with shallow-water model. In
8th Asian Conference on Computer Vision (ACCV 2007).
[4] Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., and Brox,
T. (2017). Flownet 2.0: Evolution of optical ï¬‚ow estimation with deep
networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition (pp. 2462-2470).
[5] Xingjian, S. H. I., Chen, Z., Wang, H., Yeung, D. Y ., Wong, W. K.,
and Woo, W. C. (2015). Convolutional LSTM network: A machine
learning approach for precipitation nowcasting. In Advances in neural
information processing systems (pp. 802-810).
[6] Shi, X., Gao, Z., Lausen, L., Wang, H., Yeung, D. Y ., Wong, W. K.,
and Woo, W. C. (2017). Deep learning for precipitation nowcasting:
A benchmark and a new model. In Advances in neural information
processing systems (pp. 5617-5627).
[7] Agrawal, S., Barrington, L., Bromberg, C., Burge, J.,Gazen, C. and
Hickey, J. (2019). Machine learning for precipitation nowcasting from
radar images. Computer Vision and Pattern Recognition.
[8] SÃ¸nderby, C. K., Espeholt, L., Heek, J., Dehghani, M., Oliver, A.,
Salimans, T., ... and Kalchbrenner, N. (2020). MetNet: A Neural Weather
Model for Precipitation Forecasting. arXiv preprint arXiv:2003.12140.
[9] Ayzel, G., Scheffer, T., and Heistermann, M. (2020). RainNet v1. 0: a
convolutional neural network for radar-based precipitation nowcasting.
Geoscientiï¬c Model Development, 13(6), 2631-2644.
[10] Hong, S., Kim, S., Joh, M., and Song, S. K. (2017). Psique: Next
sequence prediction of satellite images using a convolutional sequence-
to-sequence network. arXiv preprint arXiv:1711.10644.
[11] EUMETSAT NWCSAF (2019). Algorithm theoretical basis
document for the cloud product processors of the NWC/GEO.
http://www.nwcsaf.org/Downloads/GEO/2018/
Documents/Scientific_Docs/
NWC-CDOP2-GEO-MFL-SCI-ATBD-Cloud_v2.1.pdf
[12] Ronneberger, O., Fischer, P., and Brox, T. (2015, October). U-net:
Convolutional networks for biomedical image segmentation. In Interna-
tional Conference on Medical image computing and computer-assisted
intervention (pp. 234-241). Springer, Cham.
[13] Seity, Y ., Brousseau, P., Malardel, S., Hello, G., B Â´enard, P., Bouttier,
F., ... and Masson, V . (2011). The AROME-France convective-scale
operational model. Monthly Weather Review, 139(3), 976-991.
[14] Brousseau, P., Seity, Y ., Ricard, D., and L Â´eger, J. (2016). Improvement
of the forecast of convective activity from the AROME-France system.
Quarterly Journal of the Royal Meteorological Society, 142(699), 2231-
2243.
--- END OF 2009.11577v3.pdf ---

--- START OF 2212.02952v2.pdf ---
Simple Baseline for Weather Forecasting Using
Spatiotemporal Context Aggregation Network
Minseok Seoâˆ—
SI Analytics
minseok.seo@si-analytics.ai
Doyi Kimâˆ—
SI Analytics
doyi@ewhain.net
Seungheon Shin
SI Analytics
shshin@si-analytics.ai
Eunbin Kim
SI Analytics
ebkim@si-analytics.ai
Sewoong Ahn
SI Analytics
anse3832@si-analytics.ai
Yeji Choiâ€ 
SI Analytics
yejichoi@si-analytics.ai
Abstract
Traditional weather forecasting relies on domain expertise and computationally
intensive numerical simulation systems. Recently, with the development of a data-
driven approach, weather forecasting based on deep learning has been receiving
attention. Deep learning-based weather forecasting has made stunning progress,
from various backbone studies using CNN, RNN, and Transformer to training
strategies using weather observations datasets with auxiliary inputs. All of this
progress has contributed to the ï¬eld of weather forecasting; however, many ele-
ments and complex structures of deep learning models prevent us from reaching
physical interpretations. This paper proposes a SImple baseline with a spatiotem-
poral context Aggregation Network (SIANet) that achieved state-of-the-art in 4
parts of 5 benchmarks of W4Câ€™22. This simple but efï¬cient structure uses only
satellite images and CNNs in an end-to-end fashion without using a multi-model
ensemble or ï¬ne-tuning. This simplicity of SIANet can be used as a solid baseline
that can be easily applied in weather forecasting using deep learning.
1 Introduction
Weather forecasting is essential to early warning and monitoring systems for natural disasters. Timely
action by accurate forecasting can mitigate the impacts. Recently, extreme precipitation events have
signiï¬cantly increased in frequency and intensity over the globe, which can lead to a high potential for
ï¬‚ooding [3]. Predicting rain events usually use weather radar and the Numerical Weather Prediction
(NWP) model. Over the last few decades, a very short-term forecast, nowcasting, has substantially
improved in a combination of that two ways.
NWP model predicts the future weather state from the current atmospheric conditions by solving
physical theories. Still, NWP models need high computational costs, and the ï¬rst two hours of
prediction are known to have a high prediction error because the model resolution is not enough
to describe cloud particle development. More recently, deep learning (DL) has been attempted to
combine with NWP to improve the accuracy of short-term predictions [ 3, 11]. However, the DL
âˆ—Equal contribution
â€ Corresponding author
arXiv:2212.02952v2  [cs.CV]  10 Dec 2022
Large Context Aggregation Module + MaxpoolTransposed Conv + Concat+ Large Context Aggregation ModuleSkip Connection
ğ’™âˆˆâ„ğ‘ªÃ—ğ‘»ğ’Šğ’Ã—ğ‘¯Ã—ğ‘¾ Conv 1Ã—1Ã—1,ReshapeCenterCropSTR
VISIRWV
VISIRWV
ğ‘»
Large Kernel Spatiotemporal Attention
Residual block
Residual block
ğ’šğ’†ğ’‚ğ’“ğ’ğ’šâˆˆâ„ğŸÃ—ğ‘»ğ’ğ’–ğ’•Ã—ğ‘¯Ã—ğ‘¾
Kernel Size=(3,3,3);BatchNorm; ReLU
Kernel Size=(3,3,3);BatchNorm; ReLU
Kernel Size=(1,1,1)
ğ’šğ’“ğ’†ğ’‡ğ’Šğ’ğ’†âˆˆâ„ğŸÃ—ğ‘»ğ’ğ’–ğ’•Ã—ğ‘¯Ã—ğ‘¾
Spatiotemporal DepthwiseConv
Spatiotemporal DepthwiseDecompositionConv
Kernel Size=(1,1,1)
Large Kernel Spatiotemporal Attention
Spatiotemporal Refinement Module(STR)
ğ¿!"##$%
ğ¿&'()(+,-./)
ğ¿&'()(1.2/3)
Sigmoid
Sigmoid
ğ‘³ğ’ğ’”ğ’”ğ’•ğ’ğ’•ğ’‚ğ’=ğ‘³ğ‘¾ğ‘©ğ‘ªğ‘¬(ğ’‡ğ’Šğ’ğ’‚ğ’)+ğœ¶ğ‘³ğ’”ğ’ğ’ğ’ğ’•ğ’‰+ğœ·ğ‘³ğ‘¾ğ‘©ğ‘ªğ‘¬(ğ’†ğ’‚ğ’“ğ’ğ’š)
ğ’šâˆˆâ„ğŸÃ—ğ‘»ğ’ğ’–ğ’•Ã—ğ‘¯Ã—ğ‘¾
Target Region
Kernel Size=(3,1,1)Dilation=(1,1,1)
Kernel Size=(3,3,3)Dilation=(1,1,1)Large Context Aggregation Module
Kernel Size=(1,3,3)Dilation=(1,3,3)Kernel Size=(1,5,5)Dilation=(1,3,3)Kernel Size=(1,7,7)Dilation=(1,3,3)Kernel Size=(1,9,9)Dilation=(1,3,3)
Channels Split=4
Kernel Size=(1,3,3)Dilation=(1,1,1)
Figure 1: An overview of SIANet. SIANet is composed of pure 3D-CNN and uses a large context
aggregation module as a basic block. Also, the spatiotemporal reï¬nement module is trained end-to-end
fashion.
models still do not fully understand the physical theories of the atmosphere, so NWP data is used just
as ancillary data [2, 9] in the training process.
The current nowcasting approach in operation is based on radar extrapolation methods. Weather
radar produces a high-resolution precipitation map, which includes the motion and intensity of rain
events. Classical methods extrapolate the movement of precipitation systems from radar maps, so
the accuracy of rainfall intensity decreases as the prediction time and observation distance increase.
Several data-driven DL algorithms have been used for rain forecasting to overcome these limitations
( [10]). However, the radar can detect water droplets larger than a speciï¬c size ( â‰¥2mm) that are
matured cloud particles and may miss smaller droplets. That means the model cannot recognize the
convective initiation.
Thus, the models have been developed by considering a broader spatiotemporal context. The LSTM
(long-short-term memory) architecture uses memory blocks that capture spatiotemporal dependencies
among sequential data. Recently, ConvLSTM by [ 13] has been employed for 2D images and
exploited in weather prediction models without physical theories [ 2, 14, 1, 15]. [ 2] proposed a
multi-data-based prediction model, MetNet-2, which uses an axial attention module to efï¬ciently
capture the longer spatial dependencies in the data. However, using additional static data, ConvLSTM,
or temporal embedding for temporal modeling could expand complicated structures.
In this paper, we proposed a simple but efï¬cient structure, a SImple baseline with a spatiotemporal
context Aggregation Network (SIANet). SIANet only consists of 3D-CNN compared to others that
use a mixture of several models [ 13, 4]. We use satellite image data as an input without other
static data such as latitude, longitude, and topological height. Despite this simple structure, SIANet
achieved state-of-the-art in 4 out of 5 of the W4Câ€™22 benchmark datasets.
2 Method
In this section, we describe the architecture of the SIANet in detail. SIANet predicts rainfall locations
for the next 8 hours with 32-time slots from an input sequence of 4-time slots of the preceding hour.
The input sequence consists of four consecutive satellite images from 11 spectral band. These 11
2
t0 t1 t2 t3t-1
â€¢ â€¢ â€¢â€¢ â€¢ â€¢
Figure 2: Motivation of the Spatiotemporal Reï¬nement Module (STR).
channels consists of satellite radiances covering so-called visible (VIS), water vapor (WV), and
infrared (IR) bands. Each satellite image covers a 15-minute period and its pixels correspond to a
spatial area of about 12 km Ã—12 km. The prediction output is a sequence of 32 images representing
rain rate from radar reï¬‚ectivity. Output images also have a temporal resolution of 15 minutes but
have higher spatial resolution than input data, with each pixel corresponding to a spatial area of about
2 km x 2 km. Thus, in the training process, the proposed model consider how to convert the coarse
satellite images to the ï¬ne radar images for target regions in addition to predicting the weather in the
future.
2.1 Kernel Decomposition
If there is a 3D convolutional ï¬lter with a receptive ï¬eld size of Kt Ã—Kh Ã—Kw size, the number of
Flops of the CNN is as follows:
Flops = HÃ—W Ã—Cin Ã—Cout Ã—(Kt Ã—Kh Ã—Kw) (1)
where H,W are the height and width of the image, Cin is the input channel, and Cout is the number
of output channels. If 3D convolution is separated into spatial 2D convolution and temporal 1D
convolution through kernel decomposition, the number of parameters is as follows:
Flopsd = HÃ—W Ã—Cin Ã—Cout Ã—(Kt + Kh Ã—Kw) (2)
2.2 Overview
Weather forecasting aims to infer the future weather state based on previous conditions. When
satellite inputs xâˆˆRCÃ—TinÃ—HÃ—W and ground-radar outputs yâˆˆR1Ã—ToutÃ—H
6 Ã—W
6 are given, SIANet
F(.) learns to predict yby inputting xthrough loss functions Las shown in the following equation:
Lbce = âˆ’
âˆ‘
h,w
(ylog F(x) + (1âˆ’y) log(1âˆ’F(x)) (3)
Figure 1 shows an overview of SIANet. The model has a simple U-Net-based structure that does
not use the complex structures of recent leading approaches such as RNN, CNN+RNN, transformer
series, or additional information such as time embedding, latitude, longitude, and topological height.
SIANet uses the Large Spatial Context Aggregation Module (LCAM) as a basic block. It is an
architecture design based on matrix decomposition [ 5, 6], and the amount of calculation is small
compared to parameters. The ï¬nal component of SIANet is the spatiotemporal reï¬nement module,
which is trained end-to-end without additional training. It was designed with inspiration from the
Markov chain that post-processes through spatio-temporal correlation in the conventional weather
forecasting ï¬eld.
2.3 Large Spatial Context Aggregation Module
The work most similar to the LCAM is a Visual Attention Network (V AN) [7] that recently achieved
higher performance than the transformer architecture with decomposition convolution. LCAM uses
decomposition convolutions similar to V AN, but uses Multi Kernel Split Convolutions (MKSC)
instead of depthwise convolutions.
3
As depicted in Fig. 1, LCAM contains three parts: a multi-kernel split convolutions to capture multi
receptive ï¬eld, spatiotemporal decomposition convolutions to capture spatiotemporal decomposed
information, and an 3 Ã—3 Ã—3 convolution to aggregate spatiotemporal information. When a feature
map F âˆˆRCÃ—TÃ—HÃ—W is given, LCAM is expressed by the following algorithm: Note that the size
Algorithm 1 LCAM
Input: F âˆˆRCÃ—TÃ—HÃ—W
F = SpatioConv1Ã—3Ã—3(F)
F1,F2,F3,F4 = ChannelsSplit(F) âŠ¿Divide the channels at even intervals
for iin 4 do
Ë†Fi = DilateConv1Ã—n[i]Ã—n[i](Fi) âŠ¿n = [3,5,7,9]
end for
Ë†F = Concat( Ë†F1, Ë†F2, Ë†F3, Ë†F4)
Ë†F = TemporalConv3Ã—1Ã—1( Ë†F)
Ë†F = Conv3Ã—3Ã—3( Ë†F) âŠ¿Spatiotemporal aggregation
of the dividing channels is a hyperparameter. In the W4Câ€™22 dataset, the highest performance was
achieved when the channel division size was set to 2 (3x3,5x5). However, we empirically found that
dividing the channel size by 4 performed best on video prediction dataset [16].
2.4 Spatiotemporal Reï¬nement Module
Conditional random ï¬eld (CRF) has been widely used as a post-processing algorithm in object
semantic segmentation and has shown robust results against various noises. Recently, a deep learning-
based post-processing method has also been proposed and has shown promising results [ 8]. The
accuracy of weather forecasting is signiï¬cantly affected by the post-processing method. In particular,
the post-processing algorithm is essential because weather is highly spatially and temporally related.
We propose the Spatiotemporal Reï¬nement Module (STR) to solve this problem. Figure 2 shows
our motivation. As shown in the ï¬gure, since adjacent pixels are correlated with each other, using
the spatiotemporal modeling can improve the performance of the weather forecasting model. The
STR is trained for the purpose of reï¬nement of the output yearly âˆˆR1Ã—TÃ—HÃ—W from the 3D-UNet
structure based on the LCAM block. yearly is generated through reshaping after channel reduction
with target region crop and 1 Ã—1 Ã—1 convolution. Figure 3 shows the process how the channels and
the time dimensions are multiplied to create the target time dimension used for weather forecasting.
ğ‘Š
ğ»
ğ¶ğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$
ğ¶Ã—ğ‘‡Ã—ğ»Ã—ğ‘Š 1Ã—(ğ‘‡ğ¶)Ã—ğ»Ã—ğ‘ŠReshape
Figure 3: The reshape method adopted by SIANet.
As shown in Fig. 1, the STR consists of a large kernel spatiotemporal attention and two sequential
residual blocks. The large kernel spatiotemporal attention re-weights yearly by considering the
receptive ï¬eld as much as 7 Ã—7 Ã—7 on the spatiotemporal axis of the softmax value of each pixel of
yearly. Note that large kernel spatiotemporal attention is a temporally extended version of V AN [7].
After that, yfinal âˆˆR1Ã—TÃ—HÃ—W , the result of reï¬nement of yearly, is output by aggregating the
surrounding spatiotemporal information through the residual block with 3 Ã—3 Ã—3 kernel. The loss
function of SIANet including STR follows the equation below:
Ltotal = Lbce(yfinal ,y) +Î±Lbce(yearly,y) (4)
where Î±is the weighting factor and is set to 0.2 in all our experiments.
4
3 Experiments
We used the W4Câ€™22stage1 benchmark dataset and the W4Câ€™22stage2 benchmark dataset to evaluate
SIANet. Also, our SIANet was evaluated on the W4Câ€™22 core transfer benchmark dataset and
achieved stage-of-the-art on the test set and held-out set. However, these results are not covered in
this paper, but in our core transfer paper.
In this section, the experimental results of the W4Câ€™22 leaderboard and the ablation study experimen-
tal results of LCAM and Reï¬ne module are described in detail. The performance evaluation metric of
all experiments was selected as mean intersection over union (mIoU). Note that all ablation studies
were performed on the stage 1 testset. The code is available on 3
3.1 Experimental Setting
W4Câ€™22 stage1Datasets consist of three European regions selected based on precipitation charac-
teristics. The task of the dataset is to receive four satellite images at 15-minute intervals and predict
32 rain events at 15-minute intervals at the pixel level. Therefore, it is a binary classiï¬cation task
that predicts rain/no-rain at the pixel-level. OPERA data was separated into rain/no-rain pixels by a
threshold of 0.001 mm/hr on ground radar, and the stage1 dataset covered February to December
2019.
W4Câ€™22 stage2Datasets consists of two years, 2019 and 2020, and seven regions of Europe (R15,
R34, R76, R04, R05, R06, R07). Unlike the stage1 dataset, the rain/no-rain threshold is 0.2 mm/hr,
and latitude, longitude, and topological height are provided. Note that in all our experiments, only
satellite images are used among them.
Implementation detail To train SIANet, the batch size of one GPU was set to 16, and FP16
training was used. In addition, 90 epochs and 4 positive weights of binary cross entropy were used.
The initial learning rate was set to 1e-4, weight decay to 0.1, and dropout rate to 0.4. AdamW was
used as the optimizer, and the learning rate was reduced by 0.9 when the loss was higher than the
validation loss at the previous epoch. All experiments were performed on Nvidia A100 Ã—8 GPUs.
Table 1: W4Câ€™22 Stage1 test leaderboardexperiment results. Note that bold indicates the highest
performance.
W4Câ€™22 Stage 1 Test Leaderboard
Method Boxi15 Boxi34 Boxi76
Precision Recall F1 IoU Precision Recall F1 IoU Precision Recall F1 IoU
SIANet 0.617 0.751 0.678 0.512 0.535 0.760 0.628 0.458 0.649 0.550 0.595 0.424
FIT-CTU 0.629 0.723 0.673 0.507 0.555 0.705 0.621 0.450 0.653 0.489 0.559 0.388
MS-nowcasting0.647 0.670 0.658 0.490 0.537 0.665 0.594 0.423 0.554 0.572 0.563 0.392
meteoai 0.647 0.669 0.658 0.490 0.517 0.684 0.589 0.417 0.551 0.576 0.563 0.392
KAIST AI 0.587 0.707 0.641 0.472 0.506 0.749 0.604 0.433 0.556 0.575 0.565 0.394
antfugue 0.577 0.707 0.636 0.466 0.547 0.706 0.616 0.446 0.665 0.479 0.557 0.386
3.2 Stage1 Results
Results Table 1 is the result of evaluating SIANet on the stage1 test set. As shown in the table, our
SIANet achieved state-of-the-art performance in all regions of R15, R34, and R76. We achieved these
experimental results without additional deep learning model training processes such as multi-model
ensemble and ï¬ne-tuning, or complex model structures such as ConvLSTM, hybrid, and lead time
embedding.
Discussion SIANet is an end-to-end model composed of only 3D-CNN. In this paper, we did not
use tricks such as ï¬ne-tuning and multi-model ensemble to experimentally prove that SIANet has a
simple structure but a strong performance. However, we believe that SIANet will also beneï¬t from
additional performance improvements by applying a ï¬ne-tuning, multi-model ensemble.
3https://github.com/seominseok0429/W4C22-Simple-Baseline-for-Weather-Forecasting-Using-Spatiotemporal-Context-Aggregation-Network
5
Table 2: W4Câ€™22 Stage2 test leaderboardexperiment results. Note that bold indicates the highest
performance.
W4C 22 Test Leaderboard
Method 2019 2020 mIoU
R15 R34 R76 R04 R05 R06 R07 R15 R34 R76 R04 R05 R06 R07
SIANet0.305 0.1770.360 0.260 0.317 0.4220.3690.2700.3920.1280.361 0.277 0.345 0.2440.302
meteoai0.3100.180 0.323 0.244 0.309 0.4180.3750.220 0.3840.1630.343 0.269 0.285 0.2070.288
FIT-CTU0.308 0.194 0.350 0.253 0.308 0.411 0.3330.2760.388 0.106 0.297 0.249 0.283 0.1990.283
KAIST-AI0.2690.2000.293 0.233 0.280 0.383 0.3610.237 0.362 0.104 0.320 0.271 0.335 0.1330.270
Table 3: W4Câ€™22 Stage2 Heldout leaderboardexperiment results. Note that bold indicates the highest
performance.
W4C 22 Heldout Leaderboard
Method 2019 2020 mIoU
R15 R34 R76 R04 R05 R06 R07 R15 R34 R76 R04 R05 R06 R07
FIT-CTU0.3820.2900.2090.3100.3650.419 0.2440.2680.3160.4200.360 0.3580.436 0.0430.316
meteoai0.334 0.295 0.198 0.313 0.318 0.4100.2870.259 0.2600.4480.348 0.343 0.4390.0470.307
SIANet0.3430.3000.2060.3200.3500.4340.2160.249 0.280 0.417 0.350 0.328 0.446 0.0200.304
team-name0.321 0.278 0.195 0.305 0.341 0.395 0.2360.2700.306 0.402 0.330 0.3390.4700.0010.299
3.3 Stage2 Results
Test Leaderboard Results Table 2 is the result of evaluating SIANet on the stage2 test set. SIANet
achieved stage-of-the-art performance in regions R34, R76, R05, R06, and R07 in 2019 and regions
R34, R04, R05, R06, and R07 in 2020. In this experiment, SIANet did not use the longitude, latitude,
and topological height information provided by the W4Câ€™22 stage2 dataset, but only satellite images.
These experimental results indicate that SIANet can achieve high performance using only satellite
images and a simple model structure.
Heldout Leaderboard Results Table 3 is the result of evaluating SIANet on the stage2 heldout
set. SIANet achieved third place performance with 0.304 mIoU performance. This result is 0.012
lower than the 0.316 scores of the ï¬rst place. In addition, SIANet scored0.302 âˆ¼0.309 in the test set,
validation set, and held-out set. These experimental results indicate that SIANet is not a model whose
performance varies greatly depending on the test set, but a solid baseline with consistent performance
in various test sets.
Discussion Longitude, latitude, and topological height are important properties that can reï¬‚ect
regional features in the model. Many deep learning-based weather forecasting works have improved
performance by utilizing static information. In this paper, we did not experiment using static data
in SIANet for simplicity, but we believe that SIANet can easily add static data because it is easy to
modify and has a simple structure, and can improve performance.
3.4 Ablation Study
Table 4: Efï¬ciency comparison experiment between SIANet and UNet3D according to init ï¬lter size
Models Init Filter Size FLOPs â†“ Parameters â†“ mIoU â†‘
U-Net3D 32 39.52 5.7M 0.332
SIANet 32 54.30 4.6M 0.447
U-Net3D 64 152.22 22.6M 0.338
SIANet 64 137.76 17.9M 0.465
Efï¬ciency Experiment Table 4 is an experiment table comparing the efï¬ciency of UNet3D
and SIANet according to the init ï¬lter size. As shown in the table, SIANet not only has higher
6
Ground truthProbabilityPrediction
ğ‘‡! ğ‘‡" ğ‘‡# ğ‘‡$
Figure 4: Predictions on the validation set using SIANet. The green area of ground trues has a rain
threshold of 0, and the yellow area has a rain threshold of 0.2. SIANetâ€™s prediction result probability
value for each pixel is closer to 1 as it is closer to yellow.
performance, but also has fewer parameters and FLOPs than UNet3D. These experimental results are
because SIANet applied decomposition method in all parts of spatial, temporal, and channels (split).
Efï¬ciency Discussion Since satellite data is accumulated on a daily basis, the quantity is very
large. Therefore, a very large-scale GPU environment is essential to use all weather data for training.
Recently proposed transformer-based weather prediction models show promising performance, but
FLOPs are very large. (Transformer-based methods have large FLOPs even when parameters are
small.) However, methods requiring large FLOPs require more GPU environments to train large
amounts of weather data. The matrix decomposition technology is a method that can achieve high
performance while reducing the amount of computation. Also, SIANet experimentally showed that
the matrix decomposition technique is effective for weather data. Therefore, we hope that the matrix
decomposition technique will be widely used in the ï¬eld of weather, which requires training on large
data.
Component Ablation Study Table 5 is the result of the ablation study for each component of
SIANet. The thresholds in the table are experimental results adjusted from 0.5 to 0.6, and different
thresholds for each evaluation area were not used. Experimentally, using a different threshold for each
region improved performance slightly, but it was not used because of the possibility of overï¬tting to
the testset. In addition, CenterCrop, STM, and LCAM all showed performance improvements, and
these experimental results indicate that each component has a complementary relationship with each
other. Strategies are the training strategies of domain generalization [12].
Table 5: Results of ablation study of each component of SIANet. Note that init ï¬lter size is 64
Models Componets mIoU Gain
Threshold CenterCrop STM LCAM Strategies
SIANet (baseline) - - - - - 0.338 +0
SIANet âœ“ - - - - 0.359 +2.1
SIANet âœ“ âœ“ - - - 0.381 +4.3
SIANet âœ“ âœ“ âœ“ - - 0.418 +8.0
SIANet âœ“ âœ“ âœ“ âœ“ - 0.443 +10.5
SIANet âœ“ âœ“ âœ“ âœ“ âœ“ 0.465 +12.7
7
Ground truthProbabilityPrediction
ğ‘‡! ğ‘‡" ğ‘‡# ğ‘‡$
Figure 5: An example of SIANetâ€™s ability to capture the moving direction of clouds.
3.5 Qualitative results
Results Figure 4 and Figure 5 are the prediction results of SIANet. T1 represents +1 hour, T3
represents +3, T5 represents +5, and T8 represents +8 hours. As shown in Fig. 4, it can be seen that
SIANet makes very different predictions for each timestamp. These experimental results indicate
that SIANetâ€™sT1 to T8 prediction values make different predictions for each timestamp, rather than
predicting one average value.
Figure 5 is an example of SIANetâ€™s ability to capture the direction of cloud movement. As shown in
the ï¬gure, it was conï¬rmed that the prediction of SIANet changes depending on the moving direction
of rain clouds from left to right by time period. These experimental results indicate that SIANet made
predictions by capturing the moving direction of clouds.
Discussion According to the qualitative experimental results, SIANet predicts quite well when the
rain rate threshold is zero. However, when the rain rate was 0.2, there was a problem of predicting
that too many areas would rain. These experimental results are because SIANet was trained in the
same rain class when the rain rate was over 0.2 without considering the rain rate. In other words, since
it has not been trained by classifying rain rates (e.g. light rain, medium rain, strong rain), SIANet has
no ability to classify them. In our future work, we plan to improve the performance by adding rain
rate estimation loss to SIANet, allowing SIANet to discriminate between light, medium and strong
rain.
4 Conclusions
In this paper, we proposed SIANet, a simple but efï¬cient architecture. We point out that existing
weather forecasting models have increasingly complicated training processes, such as multi-model
ensemble, ï¬ne-tuning, and auxiliary inputs. We proposed a simple but efï¬cient structure requiring
fewer parameters, Flops, and input data, SIANet. In addition, we propose a large spatial context
aggregation module as a basic block of SIANet, composed of decomposition technology, a recent
learning approach, and pure 3D-CNN. Finally, we proposed a novel reï¬nement module inspired
by the Markov chain. We show that SIANet can achieve state-of-the-art on various benchmark test
sets using only satellite images and without multi-model ensemble and ï¬ne-tuning. We believe
that SIANet will become a solid baseline that solves the problem that existing deep learning-based
weather forecasting models are not utilized due to their complexity.
8
References
[1] Georgy Ayzel, Tobias Scheffer, and Maik Heistermann. Rainnet v1. 0: a convolutional neural network for
radar-based precipitation nowcasting. Geoscientiï¬c Model Development, 13(6):2631â€“2644, 2020.
[2] Lasse Espeholt, Shreya Agrawal, Casper SÃ¸nderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk
Gazen, Rob Carver, Marcin Andrychowicz, Jason Hickey, et al. Deep learning for twelve hour precipitation
forecasts. Nature communications, 13(1):1â€“10, 2022.
[3] Jaroslav Frnda, Marek Durica, Jan Rozhon, Maria V ojtekova, Jan Nedoma, and Radek Martinek. Ecmwf
short-term prediction accuracy improvement by deep learning. Scientiï¬c Reports, 12(1):1â€“11, 2022.
[4] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Yuyang Wang, Mu Li, and Dit-Yan Yeung. Earthformer:
exploring space-time transformers for earth system forecasting. arXiv preprint arXiv:2207.05833, 2022.
[5] Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is attention better
than matrix decomposition? arXiv preprint arXiv:2109.04553, 2021.
[6] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext:
Rethinking convolutional attention design for semantic segmentation. arXiv preprint arXiv:2209.08575,
2022.
[7] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, and Shi-Min Hu. Visual attention
network. arXiv preprint arXiv:2202.09741, 2022.
[8] Chuong Huynh, Anh Tuan Tran, Khoa Luu, and Minh Hoai. Progressive semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16755â€“
16764, 2021.
[9] Jussi Leinonen, Ulrich Hamann, Urs Germann, and John R Mecikalski. Nowcasting thunderstorm hazards
using machine learning: the impact of data sources on performance. Natural Hazards and Earth System
Sciences, 22(2):577â€“597, 2022.
[10] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan
Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting
using deep generative models of radar. Nature, 597(7878):672â€“677, 2021.
[11] Adrian Rojas-Campos, Michael Langguth, Martin Wittenbrink, and Gordon Pipa. Deep learning models
for generation of precipitation maps based on numerical weather prediction. EGUsphere, pages 1â€“20,
2022.
[12] Minseok Seo, Doyi Kim, Seungheon Shin, Eunbin Kim, Sewoong Ahn, and Yeji Choi. Domain gener-
alization strategy to train classiï¬ers robust to spatial-temporal shift. arXiv preprint arXiv:2212.02968,
2022.
[13] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Con-
volutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural
information processing systems, 28, 2015.
[14] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun
Woo. Deep learning for precipitation nowcasting: A benchmark and a new model. Advances in neural
information processing systems, 30, 2017.
[15] Casper Kaae SÃ¸nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans,
Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model for precipitation
forecasting. arXiv preprint arXiv:2003.12140, 2020.
[16] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representa-
tions using lstms. In International conference on machine learning, pages 843â€“852. PMLR, 2015.
9
--- END OF 2212.02952v2.pdf ---

--- START OF Deep_Learning-Based_Satellite_Image_Analysis_for_P.pdf ---

Peer review status:
This is a non-peer-reviewed preprint submitted to EarthArXiv.

Deep Leaning Based Prediction of Land Surface Temperature and 
Emissivity from Multi-Region Landsat 8 OLIC/TIRS Imagery 
 
Ankur Garg, Space Applications Centre (Corresponding Author),  
Contact : agarg@sac.isro.gov.in 
 
Aatmaj Amol Salunke, Manipal University Jaipur 
Contact:aatmajsalunke@yahoo.com 
 
Declarations 
Conflict of Interest The authors declared that they have no conflict of interest. 
 
Funding N/A 
 
Acknowledgment 
The authors thankfully acknowledges the understanding, encouragement and support 
received from Director, Space Applications Centre, ISRO. The authors would also like 
to thank other Signal and Image Processing Area (SIPA) members who have given their 
support from time to time.   
 
Deep Learning-Based Satellite Image Analysis for
Predicting Land Surface Temperature and Emissivity
from Multi-Region Landsat 8 OLI/TIRS Imagery
Aatmaj Amol Salunke
School of Computer Science and Engineering
Manipal University Jaipur
Jaipur, RJ
aatmajsalunke@yahoo.com
Ankur Garg
Space Applications Centre
Indian Space Research Organization (ISRO)
Ahmedabad, GJ
agarg@sac.isro.gov.in
Abstractâ€”This research investigates the efï¬cacy of deep learn-
ing techniques in estimating Land Surface Temperature (LST)
and Emissivity from Landsat satellite imagery across seven dis-
tinct geographical regions. Utilizing the Single Channel Method
for LST estimation and an NDVI-based approach for Emissivity
estimation, our study spans the years 2018 to 2023, ensuring data
integrity with cloud cover below 10%. We meticulously calibrated
radiometric values and curated combined datasets for training
Pix2Pix models, subsequently evaluating their performance using
robust metrics. Our ï¬ndings demonstrate the effectiveness of
this approach in accurately predicting LST and Emissivity, even
on unseen data, with adept handling of boundary null values
during image stitching. The results showcase the potential of
deep learning models in remote sensing applications, contributing
to improved land surface monitoring and environmental assess-
ments. This research underscores the importance of integrating
advanced computational techniques with earth observation data
for enhanced insights into climate dynamics and land surface
processes.
Index Termsâ€”Deep Learning, Pix2Pix, Landsat 8, Land Sur-
face Temperature, Emissivity, Geospatial Analysis, Single Chan-
nel Method, Radiometric Calibration
INTRODUCTION
Satellite-based remote sensing has become indispensable in
environmental monitoring, offering extensive spatial coverage
and temporal continuity for studying Earthâ€™s surface dynamics.
Among the critical parameters derived from satellite imagery,
land surface temperature (LST) and emissivity are fundamental
in understanding various environmental processes, including
energy balance, vegetation health, and climate change impacts.
Landsat 8, equipped with the Operational Land Imager (OLI)
and Thermal Infrared Sensor (TIRS), provides valuable multi-
spectral and thermal data at a spatial resolution of 30 meters,
making it a cornerstone for land surface studies.
Deep learning has emerged as a powerful tool in image anal-
ysis, capable of learning complex patterns and relationships
directly from data. Pix2Pix, a deep learning framework based
on generative adversarial networks (GANs), excels in image-
to-image translation tasks by learning the mapping between
input and output images. This study harnesses the potential
of deep learning, speciï¬cally Pix2Pix, to predict LST and
emissivity from Landsat 8 OLI/TIRS imagery, enhancing the
accuracy and efï¬ciency of land surface parameter estimation.
The study focuses on seven distinct geographic regions, each
characterized by unique land cover types and environmental
conditions, including forested areas, deserts, snow-covered
regions, and urban landscapes. These regions span diverse
climates and ecosystems, providing a comprehensive dataset
for training and evaluating the deep learning models.
Rigorous preprocessing steps are applied to the Landsat 8
imagery to ensure data quality and consistency. Radiometric
calibration is performed to convert digital numbers (DN)
to physical units, facilitating accurate quantitative analysis.
Additionally, cloud cover mitigation techniques are employed
to minimize data loss and ensure the inclusion of cloud-free
pixels in the analysis, crucial for reliable LST and emissivity
estimation.
The research approach involves the creation of combined
datasets for LST and emissivity modeling, incorporating
spectral bands from Landsat 8 OLI/TIRS imagery. Separate
Pix2Pix models are trained and evaluated for LST and emissiv-
ity prediction, utilizing a combination of supervised learning
techniques and deep neural networks.
The outcomes of this study are expected to contribute
signiï¬cantly to the ï¬eld of satellite image analysis for land
surface characterization and environmental monitoring. Accu-
rate and high-resolution predictions of LST and emissivity will
aid in understanding ecosystem dynamics, assessing land use
changes, and informing decision-making processes related to
land management and environmental conservation.
LITERATURE REVIEW
A. Related Works
Chauhan et al. in [1] address global climate challenges and
the need for sustainable urban thermal harmony. Their study
focuses on remotely sensed land surface temperature predic-
tion in Ajmer, India, using machine learning with MODIS
satellite data from 2003â€“2021. Evaluation metrics like SSIM
and RMSE assess prediction accuracy, valuable for diverse
climate and earth science applications.
Yuan et al. in [2] explored the evolving role of machine
learning (ML) methods in environmental remote sensing,
emphasizing deep learningâ€™s (DL) superiority over traditional
models. They discussed DLâ€™s applications across various earth
science domains and analyzed future challenges and prospects.
Mansourmoghaddam et al. in [3] analyzed LST changes
in Yazd, Iran, using Landsat-8 and machine learning. GBM
achieved highest accuracy (RMSE X, MAE Y , RMSLE Z),
with albedo (80.3% summer, 72.74% winter) and NDVI
(11.27% summer, 17.21% winter) impacting LST signiï¬cantly.
Their ï¬ndings validate GBMâ€™s effectiveness in predicting LST
changes, aiding in urban heat island understanding and climate
adaptation.
Amato et al. in [4] developed an automatic detection al-
gorithm using a deep convolutional neural network (CNN)
on infrared satellite data to identify volcanic thermal activity.
Their model achieved an overall accuracy of 98.3%, distin-
guishing eruptive and non-eruptive volcanic scenes with high
precision (Precision 100.0%), recall (Recall 95.7%), and F1
score (F1 score 97.8%). The ensemble SqueezeNet model
demonstrated robustness and generalizability across various
volcanic domains.
Oliveira et al. in [5] investigated the impact of heatwaves on
Southern European functional urban areas (FUAs), highlight-
ing the Urban Heat Island (UHI) effect. Their energy balance-
based machine learning approach, leveraging Local Climate
Zones (LCZ), achieved a signiï¬cant accuracy improvement of
over 80% in predicting nocturnal UHI during heatwaves.
Yu et al. in [6] investigated the impact of vertical urban
growth on the Urban Heat Island (UHI) effect in Shanghai
City. Their study using 3D landscape metrics revealed insights
into the cooling effect of vegetation, the inï¬‚uence of building
heights on temperatures, and improved understanding of UHI
dynamics.
Wahla et al. in [7] explored drought prediction using the
Standardized Precipitation Evapotranspiration Index (SPEI) in
Cholistan, Punjab, Pakistan. Their study, spanning 1980 to
2020, utilized monthly SPEIâ€“1 and SPEIâ€“3 indices, trained
with random forest (RF) algorithms on climate variables,
achieving satisfactory R2 values of 0.80 and 0.78.
Suvon et al. in [8] explored satellite image analysis for
business recommendation applications. Their study focused
on analyzing satellite image similarity between business cate-
gories using deep learning. Despite high structural similarity,
deep learning models achieved only 60% accuracy due to
biased feature learning, highlighting the need for advanced
strategies in non-obvious visual classiï¬cations.
Zerrouki et al. in [9] proposed a GAN-based approach for
detecting desertiï¬cation changes in Landsat imagery without
image segmentation preprocessing. The method achieved an
impressive 99.3% accuracy, surpassing other state-of-the-art
methods like DBN, DBM, CNN, RF, and AdaBoost, making
it a promising tool for desertiï¬cation detection applications.
Reis et al. in [10] investigated the feasibility of using deep
learning, speciï¬cally a U-Net CNN, for active ï¬re detection
in Sentinel-2 and Landsat-8 satellite images. They achieved
an accuracy of 97.98% on the validation set and 90.22% on
Landsat-8 images, highlighting the potential for environmental
conservation efforts.
Nikolaevich et al. in [11] investigated deep learning models
trained on in situ data and satellite image indices for atmo-
spheric heavy metal (HM) contamination prediction. Results
showed promising accuracies exceeding 89% for selected HMs
in Central Russia.
Elmes et al. in [12] investigated the impact of training
data errors on machine learning models for Earth Observation
applications, emphasizing the need for error quantiï¬cation
and mitigation strategies to improve map accuracy and model
outcomes.
Chen et al. in [13] utilized Random Forest models to assess
urban morphologyâ€™s impact on land surface temperature (LST)
in Shenzhen. Notably, they found that BVF and GVI were
crucial, explaining 79.56%, 79.07%, 76.42%, and 64.74% of
LST variations across spring, summer, autumn, and winter,
offering insights for urban thermal management strategies.
Maheswari et al. in [14] explored Deep Learning (DL)
for intelligent fruit yield estimation in smart farming, ad-
dressing challenges like labor-intensity and imprecision. Their
review of DL-based semantic segmentation techniques for
fruit detection showed improved performance using metrics
like RMSE, R2, pixel accuracy, recall, precision, F1 score,
and IoU, commonly used in evaluating semantic segmentation
architectures.
GÂ´omez et al. in [15] addressed desert locust plagues in
northern Africa, improving early warning systems with ad-
vanced modeling techniques and Earth observation data. Their
study achieved high model performance (KAPPA TSS=0.901,
ROC=0.986) using SMAP satellite variables, highlighting the
potential of machine learning for locust presence detection.
Stampoulis et al. in [16] investigated the impact of chang-
ing precipitation dynamics on East Africaâ€™s hydrology and
vegetation. Using machine learning models, they predicted
precipitation trends with less than 27% error, classifying
vegetation types with at least 81% accuracy across different
precipitation intensity levels.
Zhang et al. in [17] addressed the challenge of estimating
winter wheat yields in the Guanzhong Plain, PR China, with
limited data. They combined GANs and CNNs to augment
training samples, improving yield estimation accuracy signiï¬-
cantly (R2 = 0.50, RMSE = 591.46 kg/ha) compared to using
original samples alone.
Baqa et al. in [18] examined Karachiâ€™s land use changes
(2000â€“2020), noting a 173% rise in built-up areas. Built-up
and bare land had the highest land surface temperature (LST).
The ï¬ndings emphasize the importance of adaptive urban
planning to mitigate urban heat island effects sustainably,
especially as built-up areas expand signiï¬cantly, impacting
thermal environments.
Goel et al. in [19] explored using multi-sensor spectral
data from satellites like LANDSAT and ECOSTRESS to
infer canopy height from sparsely observed Lidar waveforms.
Their deep learning model achieved an RMSE of 4.604m
for Tippecanoe County and 5.479m for Monroe County, im-
proving with additional ECOSTRESS features in Tippecanoe
County but not in Monroe County.
Amani et al. in [20] address challenges in sea ice (SI)
monitoring using remote sensing, emphasizing the need for
higher temporal resolution data. They propose multi-sensor
approaches, solutions for SI thickness measurements, and ad-
vocate for standardized SI measurement formats and advanced
ML models.
Gumma et al. in [21] analyze crop yield estimation using re-
mote sensing and crop modeling in Indian states of Telangana,
Andhra Pradesh, and Odisha. They assess yields at the village
level (5 km2), using Sentinel-2 and Landsat 8 data, achieving
over 80% accuracy in crop type classiï¬cation.
Qin et al. in [ 22] proposed MUSTFN, a Convolutional
Neural Network (CNN)-based algorithm for spatiotemporal
fusion of remote sensing images. MUSTFN demonstrated
superior performance, achieving an average relative Mean Ab-
solute Error (rMAE) of 6.8% in fusing Landsat-7 and MODIS
images, surpassing ESTARFM (14.1%), FSDAF (12.8%), ES-
RCNN (8.4%), and STFGAN (8.1%). It excelled in regions
with varying resolutions, ensuring high spectral reï¬‚ectance
accuracy.
In their study, Larosa et al. in [23] introduced an algorithm
utilizing a neural network (NN) for cloud detection based on
spectral observations from spaceborne microwave radiometers.
The NN algorithm effectively distinguishes clear sky, ice, and
liquid clouds from microwave sounder (MWS) observations
over both ocean and land. Evaluation metrics indicate high
accuracy, with precision indexes ranging from 0.83 to 0.95
(MWS) and 0.85 to 0.91 (AMSU-A/MHS) over the ocean,
recall indexes between 0.79 and 0.96 (MWS) and 0.82 and
0.93 (AMSU-A/MHS) over the ocean, and F1 scores ranging
from 0.81 to 0.96 (MWS) and 0.83 to 0.90 (AMSU-A/MHS)
over the ocean.
Al-Ruzouq et al. in [24] examined the fusion of remote
sensing and machine learning (ML) for oil spill detection.
Their review of over 100 recent publications emphasizes
data preprocessing, feature extraction, and the application of
traditional ML techniques such as artiï¬cial neural networks
(ANN), support vector machines (SVM), and decision trees
(DT). The study outlines challenges and future prospects in
oil spill classiï¬cation using ML and remote sensing.
Oâ€™carroll et al. in [25] conducted a comprehensive study on
the role of sea surface temperature (SST) in understanding
ocean-atmosphere interactions, climate patterns, and global
heat distribution. Their research focused on the efforts of
GHRSST and CEOS SST-VC in providing daily global SST
maps, utilizing satellite and in situ data.
Li et al. in [26] explored AIâ€™s impact on Arctic science,
focusing on deep learning in sea ice remote sensing. They
reviewed applications like lead detection, thickness estimation,
and uncertainty quantiï¬cation, emphasizing AIâ€™s potential and
the need for integrated physics-based models for sea ice
research advancement.
Janga et al. in [27] reviewed the integration of AI with
remote sensing, analyzing methodologies, outcomes, and chal-
lenges. They explored AIâ€™s applications such as image classiï¬-
cation, land cover mapping, and object detection, emphasizing
data quality, model interpretability, and future directions for
researchers and practitioners in Earth sciences.
Chauhan et al. in [28] analyzed land surface temperature
(LST) and Normalized Difference Vegetation Index (NDVI)
from 2015 to 2022 in Ajmer City, India. They found that Urban
LST decreased from 312.07 K in 2019 to 308.228 K in 2020
due to Covid-19 lockdowns, then increased to 308.27 K in
2021 and further to 311.71 K in 2022, highlighting the impact
of human activities on climate variables in urban areas.
Orusa et al. in [29] evaluated the feasibility of Sen4MUN,
a service leveraging Copernicusâ€™ Sentinel missions for mu-
nicipal contributions in Aosta Valley, Italy. Sen4MUN, based
on geospatial deep learning, achieved MAEs of 0.16 km!
for urban areas, 0.81 km for road length, and 11 units for
real estate, demonstrating its effectiveness compared to the
Ordinary Workï¬‚ow.
Shao et al. in [30] developed a novel ensemble model for
urban land surface temperature (LST) prediction, achieving
RMSE of 1.5Â°C and SSIM of 0.85. Their method addressed
urban heat island challenges, using MODIS Aqua/Terra data
from 2003 to 2021. Their evaluation showed notable accuracy
in LST prediction, showcasing potential in complex urban
environments.
Kim et al. in [31] proposed a DNN-based method using 148
Landsat 8 images for LST retrieval in South Korea. They inte-
grated multiple land surface variables and in-situ observations
to optimize seasonal DNN models, achieving CC=0.910 0.917
and RMSE=3.245 3.365Â°C, especially successful in spring and
fall, suggesting future big data incorporation for enhanced LST
retrieval.
Pande et al. in [ 32] proposed an ensemble model for
accurate Land Surface Temperature (LST) prediction using
Landsat-8 satellite data. They achieved signiï¬cant improve-
ments in accuracy, with the Bagging ensemble model outper-
forming standalone models (R2 = 0.75). The study correlates
NDVI (R2 = 0.31), rainfall (R2 = 0.47), and ET (R2 = 0.95)
with LST, contributing to sustainable decision-making.
Wang et al. in [33] improved Land Surface Temperature
(LST) retrieval over the Tibetan Plateau using Landsat-7
images. Their random forest regression (RFR) model achieved
an RMSE of 1.890 K, outperforming linear regression (LR)
with 2.767 K. This surpassed MODIS (RMSE 3.625 K) and
the original SC method (RMSE 5.836 K), crucial for climate
change studies on the TP.
Vanhellemont et al. in [34] evaluated LST estimation us-
ing Landsat 8 TIRS data with neural network approaches
based on ECOSTRESS spectra, assessing emissivity () from
ECOSTRESS, an NDVI method, and ASTER GED. LST
retrievals performed well, with RMSE ranging from 0.8 to
3.7 K and unb-RMSD of 2.9 to 3.5 K, beneï¬ting from unity
emissivity.
Sekertekin et al. in [35] modeled diurnal Land Surface
Temperature (LST) using 78 Landsat-8 images in an arid
environment. They found strong correlations (-0.80 and -0.94)
between LST and spectral indexes BAEI and NDBaI. The
ANN model achieved RMSE values of 0.74 K (training) and
2.54 K (testing), demonstrating its accuracy in LST prediction.
Jia et al. [36] proposed a time series framework for urban re-
mote sensing surface temperature prediction under cloud inter-
ference. Using Landsat 7/8 data from 2010 to 2019 in Beijing,
they analyzed temporal trends, achieving 0.2 NDBI indicating
cloud contamination. Their GWR-based model yielded RMSE
0.74 K training, 2.54 K testing, outperforming deep learning
in predicting impervious surfaces and water bodies.
Wang et al. in [37] introduced the MDK-DL method for
land surface temperature (LST) retrieval, achieving a minimum
mean absolute error (MAE) of less than 0.1 K (Â¡7.5Â° viewing)
and less than 0.8 K (Â¡65Â° viewing), with a standard deviation
of 0.04 K and a correlation coefï¬cient of 1.000. Validation
conï¬rmed a minimum MAE of approximately 1 K (RMSE =
1.12 K; R! = 0.902), demonstrating the methodâ€™s accuracy and
reliability.
Ye et al. in [38] proposed an algorithm to estimate land
surface temperature (LST) directly from Landsat-9 TIR data
without external parameters. Their method achieved an RMSE
of 1.496 K, with 72.8% and 88.2% of residuals within Â±1
K and Â±2 K, respectively. Validation results conï¬rmed its
accuracy and consistency.
BACKGROUND
B. Pix2Pix model architecture
The Pix2Pix image-to-image translation model is a powerful
framework for image-to-image translation tasks, characterized
by its generator-discriminator structure as depicted inï¬g. 1.
This architecture has revolutionized the ï¬eld of computer
vision and deep learning by enabling the generation of highly
realistic images from input data.
Fig. 1: Combined GAN Model Architecture
The discriminator model in Pix2Pix plays a crucial role
in distinguishing between real and generated images. It is
designed to evaluate the realism of the generated images
compared to the actual (target) images. As shown inï¬g. 2,
the discriminator comprises several convolutional layers, each
followed by batch normalization and LeakyReLU activation
to ensure stable and effective training. The model takes input
images from both the source domain (src images) and the
target domain (tar images), concatenates them channel-wise,
and processes them through a series of convolutional layers
with increasing depths (C64, C128, C256, C512). The use of
convolutional layers with a kernel size of 4x4 and a stride of
2x2 allows for downsampling, capturing hierarchical features
at different spatial resolutions. Batch normalization is applied
after each convolutional layer to stabilize the training process
and improve model convergence. The LeakyReLU activation
function introduces non-linearity and helps prevent the issue
of vanishing gradients during training.
Fig. 2: Discriminator Model Architecture
The discriminator model culminates in a patch output layer
(patch out) with a sigmoid activation function, producing a
binary classiï¬cation output indicating whether the input image
patch is real or fake. The model is trained using the binary
cross-entropy loss function and optimized using the Adam
optimizer with a learning rate of 0.0002 and a beta parameter
of 0.5.
The generator in Pix2Pix is responsible for producing
realistic images from input data in the source domain. The
information presented in ï¬g. 3 visualizes the encoder and
decoder blocks connected in a U-Net-like architecture. The
encoder blocks progressively downsample the input image to
extract low-level and high-level features, while the decoder
blocks upsample the features to generate the output image.
The use of skip connections between corresponding encoder
and decoder blocks helps preserve spatial information and
enhances the quality of the generated images.
Fig. 3: Generator Model Architecture
Overall, the Pix2Pix model architecture combines the
strength of adversarial training with convolutional neural net-
works, making it a versatile and effective framework for a wide
range of image translation tasks, including but not limited to
style transfer, image colorization, and semantic segmentation.
C. Masking and Handling Outliers
During image preprocessing and prediction on test images,
interpolation and masking play crucial roles in handling out-
liers and vague values for accurate predictions. Interpolation
techniques are used to estimate pixel values in areas with
missing or undeï¬ned data, such as black (0.0 value) pixels
at boundaries or NaN value pixels in the image. This ensures
the continuity and smoothness of the image, reducing artifacts
and enhancing the quality of predictions.
Masking the black pixels from boundaries is essential to
exclude erroneous data that may distort predictions. By mask-
ing off these areas, the model focuses on valid image regions,
improving the reliability and accuracy of predictions. Handling
outliers and vague values is vital as they can introduce
noise and bias into the model, leading to inaccurate results.
Proper preprocessing techniques, including interpolation and
masking, mitigate these issues, enabling more robust and
trustworthy predictions in image-to-image translation tasks.
D. Thermography and Surface Properties
Thermography, also known as thermal imaging, is a tech-
nique used to capture and visualize thermal radiation emitted
by objects in the thermal infrared portion of the electromag-
netic spectrum (typically 3 to 14 micrometers). This technique
is widely applied in various ï¬elds such as industry, medicine,
and environmental monitoring. The mathematical expression
in eq. (1) represents the fundamental principle underlying
thermography, i.e., the Stefan-Boltzmann law, which describes
the relationship between the emitted thermal radiation, surface
temperature, and emissivity of an object.
!= Ï‰ÎµT 4 (1)
Where:
â€¢ ! represents the radiated power per unit area (W/m!).
â€¢ Ï‰ is the emissivity of the surface, which signiï¬es the
efï¬ciency of the surface in emitting thermal radiation.
â€¢ Îµ is the Stefan-Boltzmann constant ( 5.67 â†’ 10â†’8
W/m!KË†4).
â€¢ T denotes the absolute temperature of the surface (K).
Emissivity (Ï‰), as derived ineq. (2), is a crucial parameter
in thermography as it quantiï¬es the ability of a surface to
emit thermal radiation relative to an ideal blackbody radiator
(emissivity of 1). Emissivity values range from 0 to 1, where
higher values indicate surfaces that emit radiation more ef-
fectively. Emissivity is inï¬‚uenced by factors such as material
composition, surface roughness, and wavelength.
Ï‰ = !
ÎµT 4 (2)
Land Surface Temperature (LST) refers to the temperature
of the Earthâ€™s surface as measured by remote sensing instru-
ments like satellites or airborne sensors. LST estimation is vital
for climate studies, agricultural monitoring, and urban heat
island assessments. Below, eq. (3) outlines the relationship
between emitted thermal radiation and LST
TLST =
( !
Ï‰Îµ
) 1
4
(3)
Where TLST denotes the Land Surface Temperature (K).
In remote sensing applications, LST estimation involves
converting thermal infrared radiance (brightness temperature)
observed by sensors to actual surface temperatures. This
process integrates atmospheric correction algorithms to mit-
igate atmospheric effects on thermal radiation and emissivity
corrections to account for surface emissivity variations.
Accurate LST estimation is crucial for various applications,
including agriculture, where it aids in monitoring crop health,
detecting water stress, and optimizing irrigation strategies.
Additionally, LST data are utilized in climate models to study
heat ï¬‚uxes, land-atmosphere interactions, and global climate
change impacts.
The integration of Thermography, Emissivity, and Land
Surface Temperature concepts with advanced remote sensing
technologies and analytical methods has signiï¬cantly con-
tributed to scientiï¬c research and practical applications in
environmental monitoring, resource management, and climate
studies. These concepts form the basis for extracting valuable
information from thermal infrared imagery, enhancing our
understanding of Earthâ€™s surface dynamics and environmental
processes.
E. Landsat 8 OLI/TIRS Band Information - TOA and SR
The bands used for geospatial analysis and dataset
creation for model training consist of the LAND-
SAT/LC08/C02/T1 TOA and LANDSAT/LC08/C02/T1 L2
satellite data. These bands play a critical role in remote
sensing applications, providing essential information for
environmental monitoring and land surface analysis.
ST B10 represents Band 10 surface temperature, measured
in Kelvin (K), with a scale of 0.00341802 and an offset of
149. The valid range for surface temperature values is from 0
to 65535. This band captures thermal infrared radiation in the
wavelength range of 10.60 to 11.19 m, providing insights into
surface temperature variations.
ST EMIS denotes the emissivity of Band 10, which is
dimensionless and has a valid range from 0 to 10000, with a
scale of 0.0001. Emissivity characterizes the surfaceâ€™s ability
to emit thermal radiation and is crucial for accurate tempera-
ture retrievals.
B4 (Red) and B5 (Near Infrared) have a resolution of 30
meters and capture electromagnetic radiation in the wavelength
ranges of 0.64 to 0.67 m and 0.85 to 0.88 m, respectively.
These bands are valuable for vegetation analysis, land cover
classiï¬cation, and ecosystem monitoring.
B10 (Thermal Infrared 1) also has a resolution of 30 meters
and captures thermal infrared radiation in the wavelength range
of 10.60 to 11.19 m. This band provides information about
land surface temperature, aiding in applications related to
agriculture, water resource management, and urban heat island
studies.
The resampling from 100m to 30m in Thermal Infrared
1 (B10) enhances spatial resolution, enabling more detailed
analysis of thermal features and surface temperature variations
across landscapes. Overall, these bands play a crucial role
in remote sensing studies, facilitating the analysis of Earthâ€™s
surface properties, environmental changes, and ecosystem dy-
namics.
F . LST and Emissivity Calculation - Radiative Transfer Equa-
tions
The process of calculating Land Surface Temperature (LST)
and Emissivity using Landsat 8 satellite images involves
several steps and equations as shown below:
1) Top of Atmosphere Radiance (TOA):Calculate TOA
radiance (L) using the equation:
TOA (L)= ML â†’ Qcal + AL (4)
where ML and AL are sensor-speciï¬c constants, and
Qcal is the digital number (DN) from the satellite sensor.
2) Brightness Temperature (BT):Compute the brightness
temperature (BT) using the equation:
BT =
(
K2
ln
(K1
L
)
+1
)
â†‘ 273.15 (5)
where K1 and K2 are sensor-speciï¬c constants.
3) Normalized Difference Vegetation Index (NDVI):Cal-
culate NDVI using the formula:
NDVI = Band 5â†‘ Band 4
Band 5+ Band 4 (6)
where Band 4 is the Red band and Band 5 is the Near
Infrared (NIR) band.
4) Vegetation Fraction (Pv): Determine the vegetation
fraction (Pv) using:
Pv =
( NDVI â†‘ NDVImin
NDVImax â†‘ NDVImin
)2
(7)
where NDVImin and NDVImax are the minimum and
maximum NDVI values, respectively.
5) Surface Emissivity (Ï‰): Calculate surface emissivity (Ï‰)
using:
Ï‰ =0 .004 â†’ Pv +0 .986 (8)
6) Land Surface Temperature (LST):Finally, compute
the Land Surface Temperature (LST) using:
LST = BT
1+
(
0.00115 â†’ BT
1.4388
)
â†’ ln(Ï‰) â†‘ 273.15 (9)
where Ln is the natural logarithm.
These equations (eq. (4), eq. (5), eq. (6), eq. (7) eq. (8)
eq. (9)) are integral to the process of deriving accurate Land
Surface Temperature and Emissivity values from Landsat 8
satellite imagery. They take into account atmospheric condi-
tions, sensor characteristics, and vegetation cover to provide
meaningful temperature estimates for environmental monitor-
ing and analysis purposes.
DATASET DESCRIPTION
The Landsat 8 satellite imagery used in this study was
sourced from the USGS Earth Explorer platform. Numerous
research zones were covered by the imaging, including the
snow region in northeastern USA, the forest region of Brazil,
the desert region of the United Arab Emirates, and the re-
gions of northern, central, and southern India as described in
Table III. Our data collecting period, which ran from 2018 to
2023, allowed us to record both long-term trends and seasonal
ï¬‚uctuations in land surface features.
The dataset utilized in this research is fundamental to the
accuracy and reliability of the land surface temperature (LST)
and emissivity prediction models. As shown inTable I, the
dataset consists of Landsat 8 Operational Land Imager (OLI)
and Thermal Infrared Sensor (TIRS) imagery, speciï¬cally the
LANDSAT/LC08/C02/T1 TOA images for bands B4 (Red),
B5 (Near Infrared), and B10 (Thermal Infrared), all with
a resolution of 30 meters, serving as input images for the
deep learning models. These bands are chosen due to their
relevance in capturing crucial information about land surface
properties, including vegetation health, moisture content, and
thermal characteristics.
TABLE I: USGS Landsat 8 Level 2, Collection 2, Tier 1
(LANDSAT/LC08/C02/T1 L2)
Band NameResolution Wavelength Description
B4 30 meters 0.64 - 0.67 m Red
B5 30 meters 0.85 - 0.88 m Near infrared
B10 30 meters10.60 - 11.19 mThermal infrared 1, resampled from 100m to 30m
The information presented in Table II demonstrates the
target image bands for prediction that are derived from LAND-
SAT/LC08/C02/T1 L2 data and include ST B10 (Band 10
surface temperature) and STEMIS (Emissivity of Band 10)
images. These target images are essential for understanding
the thermal behavior and emissivity of the land surface, pro-
viding insights into surface energy balance and environmental
processes.
TABLE II: USGS Landsat 8 Level 2, Collection 2, Tier 1
(LANDSAT/LC08/C02/T1 L2)
Band NameUnits Min Max Scale Offset Description
STB10 K 0 65535 0.00341802 149 Band 10 surface temperature
STEMIS 0 10000 0.0001 Emissivity of Band 10
The dataset curation process involved meticulous selection
criteria to ensure the inclusion of high-quality, cloud-free
images. A cloud cover threshold was set at <=10% for
certain regions and 0% for others, reï¬‚ecting the varying
atmospheric conditions and cloud cover prevalence across
different geographic areas. This approach ensured that the
dataset represents a diverse range of land cover types and
environmental conditions, enhancing the generalizability and
robustness of the deep learning models.
Furthermore, radiometric calibration was applied to the
Landsat 8 imagery to convert digital numbers (DN) to physical
units, facilitating quantitative analysis and model training. Pre-
processing steps involve meticulous handling of null and NaN
TABLE III: Regions for Image Collection
Region Country
Forest Northern Brazil
Desert United Arab Emirates
Snow Northeastern United States
Northern India India
Central India India
Southern India India
Northeastern India India
values to ensure data completeness. Subsequently, geometric
correction rectiï¬es sensor distortions, while atmospheric cor-
rection compensates for atmospheric interferences, enhancing
data accuracy. Additionally, patching techniques segment the
imagery, optimizing data organization for subsequent analyses
and model training.
The curated dataset provides a comprehensive and reliable
foundation for training and evaluating the Pix2Pix models
for LST and emissivity estimation using the single-channel
method and NDVI-based approach respectively. The inclu-
sion of diverse geographic regions, stringent quality control
measures, and accurate radiometric calibration contribute to
the accuracy and applicability of the models in real-world
environmental monitoring and decision-making contexts.
METHODOLOGY
G. Data Acquisition and Preprocessing
The ï¬rst phase of our research involved acquiring and
preprocessing Landsat 8 OLI/TIRS imagery to prepare it for
land surface temperature (LST) and emissivity prediction. This
section outlines the data collection process and the various
preprocessing steps undertaken to ensure data quality.
1) Data Collection: For this study, Landsat 8 satellite
imagery was obtained from the USGS Earth Explorer platform.
The imagery covered a range of study regions, including
The Forest Region of Brazil, The desert region of UAE, the
Snow region of Northeastern USA, Northern India, Central
India, Southern India, and Northeastern India. The timeframe
for data collection spanned from 2018 to 2023, allowing us
to capture seasonal variations and long-term trends in land
surface characteristics.
2) Preprocessing: The acquired Landsat 8 imagery under-
went several preprocessing steps to enhance its suitability for
further analysis:
â€¢ Radiometric Calibration: This process involved con-
verting the digital numbers (DN) of satellite imagery into
reï¬‚ectance values and brightness temperature, ensuring
accuracy and consistency in spectral information capture.
Top of Atmosphere (TOA) Radiance (L)is computed
using the eq. (4):
TOA (L)= ML â†’ Qcal + AL
where ML and AL represent sensor-speciï¬c multiplica-
tive and additive constants, and Qcal is the digital number
(DN) from the satellite sensor.
The Brightness Temperature (BT)is determined by the
eq. (5):
BT =
(
K2
ln
(K1
L
)
+1
)
â†‘ 273.15
where K1 and K2 are sensor-speciï¬c constants. For
Landsat 8 satellite images, the constants are as follows:
K1 constant band 10 = 774.8853
K2 constant band 10 = 1321.0789
ML band 10 =0 .00038
AL band 10 =0 .10000
Radiometric calibration is a crucial step in satellite image
processing, ensuring the reliability and accuracy of de-
rived information for scientiï¬c analysis and applications.
â€¢ Cloud Masking: Clouds can signiï¬cantly impact the
quality of satellite imagery and introduce errors in subse-
quent analyses. To mitigate this issue, we applied cloud
masking techniques to identify and remove cloud-affected
pixels from the imagery. This process involved the use
of spectral thresholds and cloud detection algorithms to
accurately mask out cloud-covered areas.
H. Dataset Creation
Following data acquisition and preprocessing, we proceeded
to create the datasets required for training and testing our
deep learning models for LST and emissivity prediction. This
section details the steps involved in dataset creation, including
image preprocessing, data fusion and patch extraction tech-
niques.
1) Preprocessing of Landsat Images: During the initial
stages of dataset preparation, it was observed that the Landsat
raw images exhibited rotation and were enclosed within a
black bounding box. To rectify this, a preprocessing step
was introduced. A central square of dimension 4864x4864
was cropped from each image, ensuring that the essential
geographical features were retained. Subsequently, the cropped
images were segmented into patches of 256x256 dimensions.
The Normalized Difference Vegetation Index (NDVI)
which was required as an input to the Emissivity Estimation
model was calculated using theeq. (6):
NDVI = NIR â†‘ Red
NIR + Red
where NIR represents the Near-Infrared band (B5 in this
study) and Red represents the Red band (B4 in this study).
In the context of remote sensing, NDVI is a widely used
metric for assessing vegetation health and density. The formula
ensures that NDVI values range from -1 to 1, with higher
values indicating healthier vegetation.
In our research, we computed NDVI for each pixel in the
image patches using the following procedure:
1) Convert the Red (B4) and Near-Infrared (B5) bands to
ï¬‚oating-point values.
2) Calculate the denominator as the sum of the Near-
Infrared and Red band values (NIR + Red).
3) Identify and handle invalid values such as NaN (Not a
Number) or zero denominators.
4) Compute the NDVI using the formula, replacing invalid
values with NaN to maintain data integrity.
This approach ensures accurate NDVI computation while
handling data anomalies gracefully. The resulting NDVI im-
ages provide valuable insights into vegetation distribution and
health, crucial for environmental monitoring and land use
studies.
Further preprocessing steps were then applied to normalize
the pixel values and align them with the required input format
for our deep learning models. This involved scaling and offset
adjustments to ensure accurate representation of the image
data.
1) Preprocessing for LST Estimation Model:The scaling
factor, denoted as scale, was set to 0.0038, and the
offset value, denoted asoffset , was set to 149. These
parameters were applied to the STB10 target images
using the eq. (10):
scaled offset tar images =( tar images.astype(np.float 32)
â†’ scale)+ offset +( â†‘273.15)
(10)
Here, tar images represents the initial target images
after preprocessing, and the addition of(â†‘273.15) is to
convert the temperature from Kelvin to Celsius.
2) Preprocessing for Emissivity Estimation Model:The
target data for the Emissivity Estimation Model under-
went scaling to align with the modelâ€™s input require-
ments. The scaling factor, denoted astarget scale, was
set to 0.0001, and it was applied to the initial target
images (tar images) using theeq. (11):
scaled tar images = tar images.astype(np.ï¬‚oat32)
â†’ target scale
(11)
Additionally, the pixel values were clipped to ensure
they remain within the valid range of 0 to 1, as Emis-
sivity values are constrained within this range. The
formula in eq. (12) shows the clipping operation being
performed.
tar images = np.clip(scaled target images, 0, 1)
(12)
This step ensures the ï¬delity of the input data for
accurate modeling of Emissivity.
Finally, the processed data was saved as .npz ï¬les, which
served as inputs for training both the LST and emissivity
prediction models.
2) Patch Extraction: To facilitate the training of our deep
learning models, we implemented patch extraction techniques
Fig. 4: LST Model Training and Prediction Flow
Fig. 5: Emissivity Model Training and Prediction Flow
to generate training samples from the combined dataset. Im-
age patches of size 256x256 pixels were extracted from the
multispectral imagery, ensuring adequate spatial coverage and
variability in the training data. Random sampling techniques
were employed to select patches from different geographic
locations and land cover types, reducing bias in the training
process.
3) Combined Dataset: The creation of the combined
dataset involved merging Landsat 8 OLI/TIRS bands B4
(Red), B5 (Near Infrared) for Emissivity estimation, and B10
(Thermal Infrared) for LST estimation to form a multispectral
input feature set. Additionally, target images for LST pre-
diction (ST B10) and emissivity prediction (STEMIS) were
generated from the preprocessed Landsat 8 imagery. These
target images were derived using established algorithms and
techniques that apply the set scale and offset to the image
pixel values to convert the raw surface reï¬‚ectance images into
land surface temperature and emissivity values.
I. Deep Learning Model Training
The core of our methodology is grounded in the utilization
of deep learning techniques, speciï¬cally the Pix2Pix model,
for conducting image-to-image translation tasks. Our objective
is to accurately predict both Land Surface Temperature (LST)
and emissivity values from input imagery. This section delves
into the intricate details of our approach, encompassing the ar-
chitectural intricacies of the deep learning models, the system-
atic training process employed, and the strategic optimizations
implemented to enhance model performance signiï¬cantly.
Algorithm 1 LST Estimation Model Algorithm
Input: B10 images after pre-processing (cropping, radio-
metric calibration, masking, and patching)
Output: Map of ST B10 (Land Surface Temperature of
Band 10) corresponding to the given input B10 map after
applying the set Scale and Offset.
scaled offset target images = (target images * scale)
+ offset + (-273.15)
0: Normalize the input and target images to the model.
0: Train the Pix2Pix image-to-image translation model.
0: Perform predictions on the test set. =0
At the heart of our methodology lies the Pix2Pix model,
renowned for its efï¬cacy in image translation tasks. By lever-
aging this model, we aimed to bridge the gap between input
imagery and the desired outputs of LST and emissivity maps.
The architecture of our deep learning models is meticulously
designed to capture the complex relationships and patterns
present in the input data, ensuring robust and accurate pre-
dictions.
The ï¬g. 4 serves as a visual representation of the training
and prediction ï¬‚ow for the LST estimation model. This ï¬gure
delineates the sequential steps involved in processing input
images, extracting relevant features, and generating precise
LST maps. The depiction inï¬g. 4 elucidates the intricate trans-
formations that occur within the deep learning model to derive
meaningful insights regarding land surface temperatures.
Algorithm 2 Emissivity Estimation Model Algorithm
Input: NDVI images after pre-processing (cropping,
NDVI Calculation from B4 and B5 images, masking, and
patching)
NDVI = (Band 5 â€“ Band 4) / (Band 5 + Band 4)
Output: Map of ST EMIS (Emissivity of Band 10)
corresponding to the given input NDVI map after applying
the set Scale.
scaled target images = (targetimages * scale)
0: Normalize the input and target images to the model.
0: Train the Pix2Pix image-to-image translation model.
0: Perform predictions on the test set. =0
Complementing this, ï¬g. 5 elucidates the training and pre-
diction ï¬‚ow for the Emissivity estimation model. This ï¬gure
provides a comprehensive overview of the process involved in
estimating surface emissivity values from input imagery (B4
and B5 band images inTable I). The visualization inï¬g. 5
captures the essence of how the Pix2Pix architecture deciphers
the input data to derive emissivity values, contributing signif-
icantly to our understanding of surface characteristics.
1) Model Architecture: The Pix2Pix architecture was cho-
sen for its effectiveness in image translation tasks and its
ability to generate high-quality output images. The model
architecture comprised a generator network responsible for
producing predicted LST and emissivity images from input
multispectral imagery, and a discriminator network trained
to distinguish between real and generated images. Modiï¬-
cations were made to the standard Pix2Pix architecture to
accommodate the speciï¬c requirements of LST and emissivity
prediction.
2) Training Process: The training process involved several
key steps to ensure the robustness and accuracy of the deep
learning models. Speciï¬cally,algorithm 1 was employed for
the LST Estimation Model, incorporating a deep learning
approach to predict land surface temperatures. Similarly,al-
gorithm 2, designed speciï¬cally for the Emissivity Estimation
Model, utilized advanced neural network architectures to esti-
mate surface emissivity values.
â€¢ Dataset Partitioning: The combined dataset was par-
titioned into training, validation, and testing sets. The
training set was used to optimize model parameters,
while the validation set facilitated model selection and
hyperparameter tuning. The testing set was kept separate
for evaluating the ï¬nal model performance.
â€¢ Optimization Algorithm: We employed the Adam op-
timizer with a learning rate of 0.0002 to train the deep
learning models. Adam is well-suited for training deep
neural networks and offers efï¬cient convergence and
gradient descent optimization.
â€¢ Loss Functions: The discriminator in the GAN model
is compiled using the binary cross-entropy loss function.
This loss function is effective for binary classiï¬cation
tasks and helps in training the discriminator to distinguish
between real and generated images. Additionally, for
the combined GAN model, the use of â€™maeâ€™ (mean
absolute error) in conjunction with â€™binarycrossentropyâ€™
contributes to enhancing the modelâ€™s ability to generate
realistic images while maintaining ï¬delity to the ground
truth data.
â€¢ Batch Processing and Summary:Training utilized mini-
batch stochastic gradient descent with a batch size of 2.
Mini-batch processing optimizes memory utilization and
accelerates model training by updating parameters based
on smaller subsets of the training data. Furthermore, a
summary of the training progress is generated after every
10 epochs. This summary includes predictions made by
the model, utilizing the checkpointed model at that epoch,
on three randomly selected images from the dataset. Ad-
ditionally, both the Generator and Discriminator models
are saved at these checkpoints to preserve the training
progress.
â€¢ Epochs and Early Stopping:The models were trained
over multiple epochs, with early stopping criteria based
on validation loss implemented to prevent overï¬tting.
Early stopping allowed us to monitor model performance
during training and terminate training when validation
performance no longer improved.
J. Model Evaluation
Following model training, a comprehensive evaluation of
the deep learning models was conducted to assess their perfor-
mance in predicting LST and emissivity from input imagery.
This section elaborates on the evaluation metrics used and the
validation strategies.
1) Evaluation Metrics: The accuracy of LST and emissivity
predictions was quantitatively evaluated using standard evalu-
ation metrics:
â€¢ Root Mean Squared Error (RMSE): RMSE mea-
sures the average deviation between predicted and ac-
tual LST/emissivity values, providing insights into the
modelâ€™s predictive accuracy.
â€¢ Mean Absolute Error (MAE): MAE calculates the
average absolute difference between predicted and actual
values, offering a robust measure of prediction error.
â€¢ Peak Signal-to-Noise Ratio (PSNR): PSNR measures
the quality of the reconstructed images by comparing
them to the original images. It quantiï¬es the level of noise
present in the images and is often used as a metric for
image quality assessment in image processing tasks.
â€¢ Structural Similarity Index (SSIM): SSIM evaluates
the similarity between two images based on their lu-
minance, contrast, and structure. It provides a measure
of how well the predicted images match the ground
truth images in terms of visual perception and structural
similarity.
â€¢ Perceptual Metrics (e.g., Perceptual Loss):Perceptual
metrics, such as perceptual loss, assess image quality
based on high-level features and spatial coherence, con-
sidering perceptual similarity alongside pixel-wise differ-
ences. These metrics provide insights into the perceptual
ï¬delity and visual quality of image-to-image translation
models.
These metrics were used to comprehensively evaluate the
accuracy and consistency of the deep learning modelsâ€™ pre-
dictions.
EXPERIMENTAL RESULTS
The experimental results as shown inTable IV demonstrate
the efï¬cacy of our proposed methodology in accurately esti-
mating Land Surface Temperature (LST) and emissivity from
Landsat 8 satellite images. Our model achieved signiï¬cant
success in predicting LST values with a high degree of
precision, as evidenced by low Root Mean Squared Error
(RMSE) and Mean Absolute Error (MAE) values (in ï¬gure
..). The correlation between predicted and actual LST values
was strong, indicating the robustness of our approach.
Furthermore, the emissivity estimation results were also
promising, with the model accurately capturing variations in
surface emissivity across different land cover types and envi-
ronmental conditions. The predicted emissivity values closely
aligned with ground truth data, demonstrating the reliability
and accuracy of our model.
K. Evaluation Metrics
The information presented in Table IV for the LST and
Emissivity estimation models showcase promising perfor-
mance in both seen and unseen data scenarios, indicating the
effectiveness of the developed models.
For the LST model on seen data, the Mean Absolute
Error (MAE) is reported at a commendably low value of
0.0157, demonstrating accurate predictions of Land Surface
Temperature. The Mean Squared Error (MSE) of 0.00048 fur-
ther conï¬rms the modelâ€™s precision in capturing temperature
variations. Moreover, the Peak Signal-to-Noise Ratio (PSNR)
of 33.7828 signiï¬es high-quality temperature reconstructions,
aligning well with ground truth data. The Structural Similarity
Index (SSIM) of 0.9662 emphasizes the modelâ€™s ability to
preserve structural details in temperature images, enhancing
its reliability.
In the case of unseen data, the LST estimation model
continues to exhibit strong performance, with an even lower
MAE of 0.00483 and MSE of 0.000359. The signiï¬cantly
higher PSNR of 44.412 and SSIM of 0.9453 underscore the
modelâ€™s robustness in handling novel data instances, reï¬‚ecting
its generalization capability.
Moving to the Emissivity estimation model, similar positive
trends are observed. On seen data, the model achieves an
impressive MAE of 0.00543 and MSE of 0.000416, indicating
accurate estimations of surface emissivity. The high PSNR
of 43.882 and SSIM of 0.9718 further validate the modelâ€™s
ï¬delity in preserving emissivity patterns.
Even on unseen data, the Emissivity model maintains its
effectiveness, with an MAE of 0.0136 and MSE of 0.00195.
Although slightly higher than the seen data metrics, the model
still performs admirably well, as evidenced by the PSNR of
37.0112 and SSIM of 0.9171.
These results in Table IV collectively demonstrate the ro-
bustness, accuracy, and generalization ability of the developed
LST and Emissivity models, showcasing their potential for
various remote sensing and environmental monitoring appli-
cations.
TABLE IV: Model Evaluation Metrics
LST Model Emissivity Model
Seen Data Unseen Data Seen Data Unseen Data
MAE 0.0157 0.00483 0.00543 0.0136
MSE 0.00048 0.000359 0.000416 0.00195
PSNR 33.7828 44.412 43.882 37.0112
SSIM 0.9622 0.9453 0.9718 0.9171
L. Predicted Images on the Test Set
The predicted images generated using the trained models
for both the LST and Emissivity datasets exhibit remarkable
proximity to the target (ground truth) images. As depicted
in ï¬g. 6, ï¬g. 8, ï¬g. 7, and ï¬g. 9, this closeness afï¬rms the
efï¬cacy and accuracy of the developed models in capturing the
underlying patterns and features within the input data. Through
rigorous training and optimization, the models have learned to
discern subtle variations and nuances in the input imagery,
resulting in highly accurate predictions. The close resem-
blance between the predicted and actual images demonstrates
the robustness and reliability of the developed algorithms
in capturing complex spatial and spectral information. This
successful replication of the ground truth images validates the
modelsâ€™ ability to generalize well to unseen data and highlights
their potential for practical applications in remote sensing,
environmental monitoring, and geospatial analysis.
The evaluation metrics further substantiate the exceptional
predictive capabilities of both models. The Structural Simi-
larity Index (SSIM) and perceptual scores, key indicators of
image ï¬delity and similarity to ground truth, showcase the
modelsâ€™ proï¬ciency in generating realistic and accurate predic-
tions. High SSIM scores signify the modelsâ€™ ability to preserve
Fig. 6: LST Model Prediction - 256 x 256 Patch
Fig. 7: Emissivity Model Prediction - 256 x 256 Patch
structural information and spatial coherence, reï¬‚ecting their
close alignment with actual ground truth images. Similarly, the
perceptual scores, which gauge the perceptual similarity be-
tween predicted and target images based on human perception,
demonstrate the modelsâ€™ capacity to capture intricate details
and nuances.
The robust RMSE, MAE and PSNR scores as summarized
in Table IV, highlight the modelsâ€™ superior performance in
producing outputs that closely resemble real-world imagery in
terms of both perception and pixel-wise accuracy. This vali-
dation through objective evaluation metrics further solidiï¬es
the modelsâ€™ reliability and applicability in diverse domains
such as environmental monitoring, land surface temperature
estimation, and emissivity mapping.
DISCUSSION
The results obtained from the predictive modeling for land
surface temperature (LST) and emissivity using remote sensing
data showcase the robustness and accuracy of the developed
models. The predictions generated by both the LST and
Emissivity models demonstrate a high level of correspondence
and proximity to the ground truth data, indicating the modelsâ€™
capability to capture complex spatial and spectral information
effectively.
In the case of the LST model, the training process yielded
impressive results with good visual and pixel-wise accuracy
achieved relatively early in the training phase as shown in
ï¬g. 6 and ï¬g. 8. This ease of training can be attributed to
the modelâ€™s ability to generalize well to unseen data, thanks
to its exposure to diverse regions during the training phase.
The LST model exhibits a strong capability to estimate land
surface temperatures accurately, making it a valuable tool for
climate studies, agricultural monitoring, and urban heat island
assessments.
The graph in theï¬g. 10 provides an overview of the training
of the Pix2Pix model on the Land Surface Temperature (LST)
dataset, in which the Generator Loss exhibited an initial value
of 78.127, which steadily decreased to 8.372 by the 50000th
step. Following this decline, the Generator Loss stabilized
Fig. 8: LST Model Prediction - 1024 x 1024 Patch (Larger Area Analysis)
Fig. 9: Emissivity Model Prediction - 1024 x 1024 Patch (Larger Area Analysis)
within the range of 5.0 to 15.0 until the ï¬nal step at 435000.
In contrast, the Discriminator Loss began at approximately
0.992 and exhibited ï¬‚uctuations within the range of 0.0 to 0.8,
characterized by sporadic extreme ï¬‚uctuations throughout the
training process. These ï¬‚uctuations in the Discriminator Loss
indicate the ongoing adversarial dynamics between the gener-
ator and discriminator networks, reï¬‚ecting the complexity and
evolution of the training process. These observations as seen
in ï¬g. 10 accentuate the intricate interplay and convergence
of the generator and discriminator networks in optimizing the
Pix2Pix model for the LST dataset.
On the other hand, the training of the Emissivity model
presented some challenges, requiring multiple iterations and
experimentation to achieve the desired results as depicted in
ï¬g. 7 and ï¬g. 9. The complexity of emissivity estimation,
inï¬‚uenced by factors such as surface composition, vegetation
cover, and atmospheric conditions, contributed to the modelâ€™s
learning curve. However, through rigorous trial and error, the
Emissivity model eventually attained a satisfactory level of
accuracy, showcasing its potential for applications in land
cover classiï¬cation, soil moisture estimation, and thermal
anomaly detection.
Throughout the training regimen applied to the Emissivity
dataset, as illustrated in ï¬g. 11, the Generator Loss com-
menced at an initial value of 104.173, progressively dimin-
ishing to 18.3713 by the 20,000th iteration. Following this
phase, the Generator Loss stabilized within the interval of 8.0
to 45.0 until the culmination of the training regimen at 435,000
iterations. Conversely, the Discriminator Loss embarked upon
its trajectory near 3.2714 and proceeded to oscillate within the
conï¬nes of 0.0 to 4.0, displaying irregular ï¬‚uctuations marked
by intermittent peaks and troughs throughout the training
process. This delineation observed inï¬g. 11 indicates a notable
pattern wherein the Generatorâ€™s performance exhibited an early
stage of reï¬nement succeeded by a phase of relative stability,
while the Discriminatorâ€™s behavior remained characterized by
variability.
Fig. 10: LST Model Training Losses Vs Steps
Fig. 11: Emissivity Model Training Losses Vs Steps
One notable aspect of both models is their adaptability
and scalability. Trained on datasets encompassing diverse
geographical regions, including forested areas, deserts, snow-
covered regions, and urban environments, the models have
learned to capture the nuanced characteristics of different land
types. This versatility enables the models to estimate land
surface temperature and emissivity accurately for any given
location on Earth, regardless of its environmental attributes.
The resultant images presented inï¬g. 6, ï¬g. 8, ï¬g. 7, and
ï¬g. 9 afï¬rm the potency of the trained models in predicting
land surface temperature and emissivity with high accuracy
and reliability. These models hold promise for various environ-
mental monitoring and geospatial analysis tasks, contributing
to a deeper understanding of Earthâ€™s surface dynamics and
supporting informed decision-making in diverse domains.
CONCLUSION
In our study, weâ€™ve developed and tested models for esti-
mating land surface temperature (LST) and emissivity using
Landsat 8 data. The results presented inTable IV show their
accuracy and effectiveness in estimating crucial environmental
parameters for climate studies, agriculture, and land manage-
ment. The LST model performed exceptionally well early
on, showing high accuracy visually and pixel-wise across
diverse environments. Its reliability in different regions makes
it valuable for thermal monitoring. The Emissivity model,
after initial training challenges, also provided satisfactory
results adaptable to various conditions, useful for applications
like land cover classiï¬cation and thermal anomaly detection.
These modelsâ€™ simplicity, needing only three input images, as
presented inï¬g. 4 and ï¬g. 5, enables quick estimation globally,
enhancing operational efï¬ciency and decision-making. They
signiï¬cantly contribute to remote sensing and environmental
monitoring, with potential for further reï¬nement and broader
applications in climate change impact assessment and resource
management.
REFERENCES
[1] Chauhan, S., Jethoo, A. S., Mishra, A., & Varshney, V . (2023). Duo
satellite-based remotely sensed land surface temperature prediction by
various methods of machine learning. International Journal of Data
Science and Analytics, 1-19.
[2] Yuan, Q., Shen, H., Li, T., Li, Z., Li, S., Jiang, Y ., ... & Zhang, L.
(2020). Deep learning in environmental remote sensing: Achievements
and challenges. Remote Sensing of Environment, 241, 111716.
[3] Mansourmoghaddam, M., Rousta, I., Ghafarian Malamiri, H., Sadegh-
nejad, M., Krzyszczak, J., & Ferreira, C. S. S. (2024). Modeling and
Estimating the Land Surface Temperature (LST) Using Remote Sensing
and Machine Learning (Case Study: Yazd, Iran). Remote Sensing, 16(3),
454.
[4] Amato, E., Corradino, C., Torrisi, F., & Del Negro, C. (2023). A Deep
convolutional neural network for detecting volcanic thermal anomalies
from satellite images. Remote Sensing, 15(15), 3718.
[5] Oliveira, A., Lopes, A., Niza, S., & Soares, A. (2022). An urban energy
balance-guided machine learning approach for synthetic nocturnal sur-
face Urban Heat Island prediction: A heatwave event in Naples. Science
of the total environment, 805, 150130.
[6] Yu, S., Chen, Z., Yu, B., Wang, L., Wu, B., Wu, J., & Zhao, F. (2020).
Exploring the relationship between 2D/3D landscape pattern and land
surface temperature based on explainable eXtreme Gradient Boosting
tree: A case study of Shanghai, China. Science of the total environment,
725, 138229.
[7] Wahla, S. S., Kazmi, J. H., Shariï¬, A., Shirazi, S. A., Tariq, A., & Joyell
Smith, H. (2022). Assessing spatio-temporal mapping and monitoring
of climatic variability using SPEI and RF machine learning models.
Geocarto International, 37(27), 14963-14982.
[8] Suvon, I. H., Loh, Y . P., Hashim, N., Mohd-Isa, W. N., Ting, C.
Y ., Ghauth, K. I., ... & Matsah, W. R. (2023). Business Category
Classiï¬cation via Indistinctive Satellite Image Analysis Using Deep
Learning. International Journal on Advanced Science, Engineering &
Information Technology, 13(6).
[9] Zerrouki, N., Dairi, A., Harrou, F., Zerrouki, Y ., & Sun, Y . (2022).
Efï¬cient land desertiï¬cation detection using a deep learning-driven
generative adversarial network approach: A case study. Concurrency and
Computation: Practice and Experience, 34(4), e6604.
[10] Reis, C. E. P., dos Santos, L. B. R., Morelli, F., & Vijaykumar, N. L.
Deep Learning-Based Active Fire Detection Using Satellite Imagery.
[11] Nikolaevich, V . K. (2023). Central Russia heavy metal contamination
model based on satellite imagery and machine learning, 47(1), 137-151.
[12] Elmes, A., Alemohammad, H., Avery, R., Caylor, K., Eastman, J. R.,
Fishgold, L., ... & Estes, L. (2020). Accounting for training data error in
machine learning applied to earth observations. Remote Sensing, 12(6),
1034.
[13] Chen, K., Tian, M., Zhang, J., Xu, X., & Yuan, L. (2023). Evaluating
the seasonal effects of building form and street view indicators on street-
level land surface temperature using random forest regression. Building
and Environment, 245, 110884.
[14] Maheswari, P., Raja, P., Apolo-Apolo, O. E., & PÂ´erez-Ruiz, M. (2021).
Intelligent fruit yield estimation for orchards using deep learning based
semantic segmentation techniquesâ€”a review. Frontiers in plant science,
12, 684328.
[15] GÂ´omez, D., Salvador, P., Sanz, J., Casanova, C., Taratiel, D., &
Casanova, J. L. (2019). Desert locust detection using Earth observation
satellite data in Mauritania. Journal of arid environments, 164, 29-37.
[16] Stampoulis, D., Damavandi, H. G., Boscovic, D., & Sabo, J. (2021).
Using satellite remote sensing and machine learning techniques towards
precipitation prediction and vegetation classiï¬cation. Journal of Envi-
ronmental Informatics, 37(1), 1-15.
[17] Zhang, J., Tian, H., Wang, P., Tansey, K., Zhang, S., & Li, H. (2022).
Improving wheat yield estimates using data augmentation models and
remotely sensed biophysical indices within deep neural networks in the
Guanzhong Plain, PR China. Computers and Electronics in Agriculture,
192, 106616.
[18] Baqa, M. F., Lu, L., Chen, F., Nawaz-ul-Huda, S., Pan, L., Tariq, A., ...
& Li, Q. (2022). Characterizing spatiotemporal variations in the urban
thermal environment related to land cover changes in Karachi, Pakistan,
from 2000 to 2020. Remote Sensing, 14(9), 2164.
[19] Goel, A. (2024). INFERRING STRUCTURAL INFORMATION FROM
MULTI-SENSOR SATELLITE DATA FOR A LOCALIZED SITE
(Doctoral dissertation, Purdue University Graduate School).
[20] Amani, M., Mehravar, S., Asiyabi, R. M., Moghimi, A., Ghorbanian, A.,
Ahmadi, S. A., ... & Jin, S. (2022). Ocean remote sensing techniques
and applications: A review (part ii). Water, 14(21), 3401.
[21] Gumma, M. K., Kadiyala, M. D. M., Panjala, P., Ray, S. S., Akuraju,
V . R., Dubey, S., ... & Whitbread, A. M. (2022). Assimilation of remote
sensing data into crop growth model for yield estimation: A case study
from India. Journal of the Indian Society of Remote Sensing, 50(2),
257-270.
[22] Qin, P., Huang, H., Tang, H., Wang, J., & Liu, C. (2022). MUSTFN: A
spatiotemporal fusion method for multi-scale and multi-sensor remote
sensing images based on a convolutional neural network. International
Journal of Applied Earth Observation and Geoinformation, 115, 103113.
[23] Larosa, S., Cimini, D., Gallucci, D., Di Paola, F., Nilo, S. T., Ricciardelli,
E., ... & Romano, F. (2023). A cloud detection neural network approach
for the next generation microwave sounder aboard EPS MetOp-SG A1.
Remote Sensing, 15(7), 1798.
[24] Al-Ruzouq, R., Gibril, M. B. A., Shanableh, A., Kais, A., Hamed,
O., Al-Mansoori, S., & Khalil, M. A. (2020). Sensors, features, and
machine learning for oil spill detection and monitoring: A review.
Remote Sensing, 12(20), 3338.
[25] Oâ€™carroll, A. G., Armstrong, E. M., Beggs, H. M., Bouali, M., Casey,
K. S., Corlett, G. K., ... & Wimmer, W. (2019). Observational needs of
sea surface temperature. Frontiers in Marine Science, 6, 420.
[26] Li, W., Hsu, C. Y ., & Tedesco, M. (2024). Advancing Arctic sea ice
remote sensing with AI and deep learning: now and future. EGUsphere,
2024, 1-36.
[27] Janga, B., Asamani, G. P., Sun, Z., & Cristea, N. (2023). A Review of
Practical AI for Remote Sensing in Earth Sciences. Remote Sensing,
15(16), 4112.
[28] Chauhan, S., & Jethoo, A. S. (2023). Thermal characterization of Ajmer
city: insights into urban heat dynamics. Materials Today: Proceedings.
[29] Orusa, T., Viani, A., Borgogno-Mondino, E. (2024). Earth Observation
Data and Geospatial Deep Learning AI to Assign Contributions to
European Municipalities Sen4MUN: An Empirical Application in Aosta
Valley (NW Italy). Land, 13(1), 80.
[30] Shao, Z., Cai, J., Fu, P., Hu, L., Liu, T. (2019). Deep learning-based
fusion of Landsat-8 and Sentinel-2 images for a harmonized surface
reï¬‚ectance product. Remote Sensing of Environment, 235, 111425.
[31] Kim, S., Lee, S. J., Lee, Y . W. (2020). Retrieval of land surface
temperature using Landsat 8 images with deep neural networks. Korean
Journal of Remote Sensing, 36(3), 487-501.
[32] Pande, C. B., Egbueri, J. C., Costache, R., Sidek, L. M., Wang, Q.,
Alshehri, F., ... Pal, S. C. (2024). Predictive modeling of land surface
temperature (LST) based on Landsat-8 satellite data and machine learn-
ing models for sustainable development. Journal of Cleaner Production,
444, 141035.
[33] Wang, X., Zhong, L., Ma, Y . (2022). Estimation of 30 m land surface
temperatures over the entire Tibetan Plateau based on Landsat-7 ETM+
data and machine learning methods. International Journal of Digital
Earth, 15(1), 1038-1055.
[34] Vanhellemont, Q. (2020). Combined land surface emissivity and tem-
perature estimation from Landsat 8 OLI and TIRS. ISPRS Journal of
Photogrammetry and Remote Sensing, 166, 390-402.
[35] Sekertekin, A., Arslan, N., Bilgili, M. (2020). Modeling diurnal Land
Surface Temperature on a local scale of an arid environment using
artiï¬cial Neural Network (ANN) and time series of Landsat-8 derived
spectral indexes. Journal of Atmospheric and Solar-Terrestrial Physics,
206, 105328.
[36] Jia, H., Yang, D., Deng, W., Wei, Q., Jiang, W. (2021). Predicting land
surface temperature with geographically weighed regression and deep
learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery, 11(1), e1396.
[37] Wang, H., Mao, K., Yuan, Z., Shi, J., Cao, M., Qin, Z., ... Tang,
B. (2021). A method for land surface temperature retrieval based on
model-data-knowledge-driven and deep learning. Remote sensing of
environment, 265, 112665.
[38] Ye, X., Liu, R., Hui, J., Zhu, J. (2023). Land Surface Temperature
estimation from Landsat-9 thermal infrared data using ensemble learning
method considering the physical radiance transfer process. Land, 12(7),
1287.
--- END OF Deep_Learning-Based_Satellite_Image_Analysis_for_P.pdf ---

--- START OF Multisensor_Machine_Learning_to_Retrieve_High_Spatiotemporal_Resolution_Land_Surface_Temperature.pdf ---
Received 14 July 2022, accepted 3 August 2022, date of publication 16 August 2022, date of current version 29 August 2022.
Digital Object Identifier 10.1 109/ACCESS.2022.3198673
Multisensor Machine Learning to Retrieve High
Spatiotemporal Resolution Land
Surface Temperature
KATE DUFFY
 , THOMAS J. VANDAL, AND RAMAKRISHNA R. NEMANI
BAER Institute, Mountain View, CA 94043, USA
NASA Ames Research Center, Mountain View, CA 94043, USA
Corresponding author: Kate Duffy (duffy.m.kate@gmail.com)
This work was supported by the NASA Earth eXchange (NEX).
ABSTRACT Climate change is making heat waves more frequent, long-lasting, and severe. While multiple
satellite types provide data to monitor surface temperature, geostationary (GEO) sensors provide near-
continuous, continental-scale observations which can better capture the diurnal variability of land surface
temperature (LST) than intermittent observations from low-earth orbit (LEO) sensors. However, standard
products from GEO satellites are available at coarsened spatial and temporal resolutions compared to
the native sensor resolution. Using datasets from the NASA Earth Exchange, we leveraged co-located,
co-temporal observations from LEO and GEO satellites to learn a data-driven mapping using a convolutional
neural network. The resulting NASA Earth eXchange Artiï¬cial Intelligence LST (NEXAI-LST) achieved a
mean absolute error of 1.73 K relative to the target LEO product and improves on both spatial and temporal
resolution [2 km, 10 minute] compared to the GEO full disk standard product [10 km, hourly]. In validation
against measurements from a ground-based sensor network, NEXAI-LST achieves similar or better ï¬t than
both LEO and GEO standard products, while depending none of the prior knowledge of land surface and
atmospheric states required by physical-statistical models. Further, application of the model to unseen LEO
and GEO satellites demonstrates robust generalization of the model across spatial region, time of day, and
sensor. In support of NASAâ€™s open-source science initiative, we make our NEXAI-LST product, model, and
codes available to facilitate data exploration and further studies.
INDEX TERMS Datasets, deep learning, emulation, remote sensing, land surface temperature.
I. INTRODUCTION
As the world warms, patterns of extreme heat events are
intensifying around the globe. In addition to raising aver-
age temperatures, greenhouse gases have been associated
with strengthening the patterns of atmospheric circulation
associated with heat waves [1], [2]. These patterns suggest
potential for increasingly common, severe, and long-lasting
heat events. In the United States, extreme heat is a lead-
ing cause of weather-related deaths [3], with economically
disadvantaged census tracts and people of color bearing
disproportionate exposure to heat in cities [4]. Heat events
exacerbate cardiovascular and respiratory illnesses [5], [6],
The associate editor coordinating the review of this manuscript and
approving it for publication was Stefania Bonafoni
.
impact birth outcomes [7], and burden health and emergency
services. Surface urban heat islands (SUHI) describe the
effect of elevated land surface temperature (LST) in urban
areas compared to surrounding rural areas [8], [9]. While
monitoring capabilities on the ground are limited, satellite
data can be used to identify SUHI with coverage all over the
world. SUHI mapping enables assessment of trends over time
as well as near real-time monitoring of conditions that are
relevant to human health and comfort. Recent heat events in
the Paciï¬c Northwest (2021), Western United States (2020),
and Australia (2019) have drawn attention to the deadly lack
of resilience and the need for data to increase preparedness
for climate extremes.
From a climate perspective, LST is critical for the study
of land surface processes and to constrain the surface energy
VOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 89221
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
FIGURE 1. a, Geostationary (GEO) satellites like GOES-16 and Himawari-8 provide observations with unprecedented spatial and temporal consistency
over their full disk views. Low earth orbiting (LEO) satellites produce a multitude of higher-level products that coincide with GEO data several times per
day at each location. b, Our proposed framework leverages co-located, co-temporal observations from LEO and GEO satellites to build models for
data-driven algorithm emulation. c, We trained a convolutional neural network to predict NEXAI-LST and classify clear sky pixels based on GEO infrared
bands and elevation.
budget and model parameters. LST provides information
about energy partitioning into sensible and latent heat ï¬‚uxes
that makes it a key parameter to study land-atmosphere inter-
actions. LST helps us to understand the effects of climate on
vegetation [10], hydrology [11], [12], and ecosystems [13]
due to its close association with evapotranspiration. As sur-
face temperatures can change over the course of minutes,
land surfaces exhibit diurnal temperature ï¬‚uctuations which
are far more information-rich than individual observations.
Diurnal cycles are primarily a lagged response to incoming
and outgoing radiation, but there is signiï¬cant spatial and
seasonal heterogeneity in the timing of peak temperature and
the range between daily minimum and maximum [9], [14].
The diurnal range of LST is a key, though relatively poorly
observed, indicator of climate change [15], [16].
Various approaches have been used to observe land sur-
face temperature, which remains a challenging task due
to its ill-posed nature and the heterogeneity of land sur-
face types [17]. Ground based observations of LST are
sparse and not generally adequate to study spatial vari-
ation. Observations from low earth orbit (LEO) satellites
such as NASAâ€™s Moderate Resolution Imaging Spectrora-
diometer (MODIS) have accurate and comprehensively vali-
dated products. MODIS data products boast global coverage,
relatively long record lengths, and high spatial resolution.
However, LEO sensors like MODIS have limited temporal
resolution, reï¬‚ecting the intermittent revists permitted by
LEO orbit patterns. MODIS ï¬‚ies onboard Terra and Aqua,
two companion satellites with offset 12-hour revisit times.
The infrequency of observations makes it challenging for
LEO satellites to capture information about diurnal variation.
Traditionally deployed for weather observations, geosta-
tionary (GEO) satellites have emerging applications for sci-
ence due to their spatial and spectral similarities to LEO
sensors, including the requisite thermal bands to retrieve
land surface temperature. By orbiting at a high altitude of
35,785 km, GEO sensors have an identical orbital period to
the Earth and remain in ï¬xed positions as viewed from the
Earthâ€™s surface. As a result, GEO sensors like the Geosta-
tionary Operational Environmental Satellites (GOES)-16 and
-17 can provide full disk observations every 10 minutes or
more. GEO observations resolve diel surface changes and
have increased robustness to cloud cover [18].
Motivated by the complementary attributes of separate
satellite datasets, we introduce a framework for cross-sensor
emulation, in which a model is trained on overlapping LEO-
GEO observations. We propose NEXAI-LST (NASA Earth
eXchnage Artiï¬cial Intelligence Land Surface Temperature),
a convolutional neural network that learns a mapping between
GOES-16â€™s infrared bands and MODIS Terra LST to accu-
rately predict geostationary LST with a built-in clear sky
classiï¬er (Figure 1). Key contributions of this work include:
1) Prediction of a high quality, spatially and temporally
consistent LST at a higher resolution (10 minute/2 km)
than the GOES-16 standard full disk product
(1 hour/10 km) (Figure 2).
2) Elimination of the need for ancillary datasets in our
machine learning-based approach.
89222 VOLUME 10, 2022
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
TABLE 1. Comparison of NEXAI-LST and MODIS LST using metrics of absolute error. NEXAI-LST derived from GOES-16 and Himawari-8 are spatially and
temporally coincident with MODIS Terra and Aqua, resulting in four sensor combinations. While GOES-16/Terra pairs were used for training, all data from
Himawari-8 and Aqua were reserved for validation. Metrics of error on LST prediction include root mean square error (RMSE), mean absolute error (MAE)
and bias. These statistics and clear sky classification accuracy are presented with 95% confidence intervals.
3) Demonstration of the generalizability of the model
across unseen times of day and spatial regions using
holdout LEO and GEO sensors.
4) Greater absolute and relative ï¬t to in-situ measure-
ments from the SURFRAD (SURface RADiation Bud-
get) Network, compared to two standard LST products.
Our experimental results suggest that the learned rep-
resentation is robust in generalization to unseen satellite
sensors. Further, multiple metrics of ï¬t to ground obser-
vations suggest superior performance in comparison to the
National Oceanic and Atmospheric Administration (NOAA)
GOES-16 standard product and the MODIS Daily LST
product. This performance is achieved without prior knowl-
edge of current land and atmosphere states, in contrast with
traditional physical-statistical models. To the best of our
knowledge, there is no other framework to learn data-driven
land surface temperature models that leverages overlap-
ping observations from multiple sensors. To promote future
research and development, we have made the NEXAI-LST
product, model, and codes publicly available for download at
https://data.nas.nasa.gov /geonex/geonexdata/ML/nexai - lst /
and on GitHub at https://github.com/KateDuffy/LEO-GEO-
landsurfacetemp.
II. RELATED WORK
Traditional remote sensing methods for LST retrieval use
thermal infrared measurements supplemented by data from
weather models and other satellite sensors. The thermal
infrared channels of satellite sensors receive a signal that
is determined by the Earthâ€™s surface temperature, surface
emissivity/reï¬‚ectivity, atmospheric effects, and solar radia-
tion. Land surface temperature is extracted from this sig-
nal using methods from two main families: single infrared
channel and split infrared channel. The single channel equa-
tion uses modeling of radiation scattering and absorption
through the atmosphere and requires extensive ancillary
information about current atmospheric proï¬les. Split channel
approaches use differential absorption in two channels to
partially account for atmospheric and surface effects. The
MODIS LST retrieval algorithm is based on the split channel
approach using MODIS bands 31 and 32 (central wave-
lengths = 11 and 12 Âµm) [19]. Coefï¬cients of the generalized
split window algorithm depend on viewing zenith angle,
atmospheric surface temperature and water vapor, and are
estimated from regression analysis of a radiative transfer
model. The MODIS LST algorithm is reduced to a lookup
table in the operational implementation. The lookup table
is organized as a database by water vapor saturation, air
temperature, and surface emissivity.
There are several challenges that make LST retrieval a
difï¬cult problem. Atmospheric water vapor is the main atmo-
spheric contribution to the thermal infrared signal that reaches
satellites. As a result of water vapor, the actual surface
temperature is generally higher than the brightness temper-
ature measured at the satellite. The relationship between
radiance and temperature is nonlinear, making linear mod-
els, like the single and split channel methods, imprecise,
especially for hot and wet atmospheric conditions [20]. This
systematic error increases as a function of column water
vapor, such that including water vapor information improves
LST accuracy [21]. However, the need for prior knowledge
of land and atmospheric states leads to the propagation
of error into LST products. LST products are also subject
to strong directional effects due to structure of surfaces,
including trees, topography, and buildings. Thus, differences
in view angle can introduce signiï¬cant difference between
products.
Deep learning has begun to demonstrate promise in tradi-
tionally challenging problems in the Earth sciences, from pre-
cipitation nowcasting [22] to climate downscaling [23], [24].
Convolutional neural networks (CNN) can automatically
extract spatial features, and it has been suggested that the
ability to apply nonlinear reasoning using spatial context is
behind CNNâ€™s power in the geosciences [25]. The general
function approximation ability of neural networks, including
CNNs, has been applied to the emulation of physical mod-
els in scientiï¬c domains ranging from turbulent ï¬‚ow [26]
to atmospheric radiative transfer modeling [27], [28], astro-
physics, climate modeling, biogeochemistry, high energy
density physics, and more [29]. In the context of remote
sensing, machine learning has been applied to generate spatial
datasets like tree cover maps [30], synthetic sensor spec-
tra [31], and poverty maps [32].
In land surface temperature retrieval, machine learning
has been used to address several longstanding challenges.
A method using knowledge-driven deep learning has been
developed to deduce LST retrieval mechanisms and reduce
the need for acquiring prior knowledge [33]. This paper
VOLUME 10, 2022 89223
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
FIGURE 2. Summer 2021 brought a persistent, unprecedented heatwave to the Pacific Northwest. Land surface temperature across the region was
produced by MODIS Terra (MOD11A1) (a) and predicted from GOES-16 by the ML model (b) at 22:30 PST on June 28, 2021. c, MODIS Terra and Aqua
products, plotted with hollow green and blue markers, exhibit agreement with the model inferences but capture less detail about the temporal profile of
LST. d, 10-minute LST on June 28, 2021 exceeds the June/July average for 2021.
demonstrated that in most cases the thermal infrared bands
and water vapor data available from geostationary sensors
are sufï¬cient for retrieval requirements. Machine learning
has also been used to fuse high-resolution optical imagery
with thermal infrared data to estimate sub-pixel LST [34],
to reconstruct missing data in LST [35], [36] and to retrieve
emissivity and LST with better accuracy than the standard
model [37].
III. MATERIALS AND METHODS
A. STUDY AREA AND DATASETS
A group of geostationary satellites including (GOES)-16
and -17 (NASA/NOAA) and Himawari-8 and -9 (Japanese
Meteorological Agency) provides near-global coverage
at a high temporal resolution. The Advanced Baseline
Imager (ABI) on board GOES-16 and the Advanced
Himawari Imager (AHI) on board Himawari-8 are sim-
ilar sensors positioned on nearly opposite sides of the
earth. At 75.2 â—¦W, GOES-16 observes the Americas, while
Himawari-8, positioned at 140.7 â—¦E, covers east Asia and
the western Paciï¬c Ocean. ABI and AHI each scan the full
disk, an area approximately 120 â—¦ by 120 â—¦, every 10 min-
utes. The sensors have similar spectral range and 2 km
spatial resolution in the infrared bands [38]. The Level 1 -
Geostationary (L1-G) data were generated on a common
grid by the Geostationary-NASA Earth eXchange (GeoNEX)
project [39]. The common geographic projection spans from
60â—¦S to 60 â—¦N and from 180 â—¦W to 180 â—¦E.
MODIS is a ï¬‚agship sensor on board NASAâ€™s Terra and
Aqua satellites. Together, Terra and Aqua observe every loca-
tion on Earth each 1-2 days, making observations in 36 dis-
crete bands at 1 km or better resolution. We obtained MODIS
Land Surface Temperature Daily L3 Global 1km (Terra:
MOD11A and Aqua: MYD11A1) for the years 2019 and
2020.
The SURFRAD network was established in 1993 with
the objective of providing accurate, continuous, long-
term measurements of the surface radiation budget in the
United States [40]. Seven stations covering diverse climates
provide 1-minute measurements, including upwelling and
downwelling infrared radiation. While measurements from
networks like SURFRAD are too sparse to systematically
validate remote sensing products, they provide high-quality,
independent measurements that complement scientiï¬c efforts
at comparing and benchmarking various LST products.
We accessed SURFRAD measurements of upwelling and
downwelling longwave ï¬‚ux corresponding to satellite
observation times for the year 2020 at http://gml.noaa.
gov/grad/surfrad/. SURFRAD LST was retrieved from the
upwelling longwave ï¬‚ux measurement Fâ†‘
LW , downwelling
longwave ï¬‚ux measurement Fâ†“
LW , the Stefan-Boltzmann con-
stant Ïƒ and the broadband longwave surface emissivity Ïµsfc.
89224 VOLUME 10, 2022
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
TABLE 2. Summary statistics of the NEXAI-LST distributions and the coincident MODIS LST distributions. Distribution mean, variance, skew, and kurtosis
are provided with 95% confidence intervals.
We assumed a value of 0.97 for the broadband longwave
surface emissivity following the recommendation of previous
studies [41], [42].
LST =
(Fâ†‘
LW âˆ’(1 âˆ’Ïµsfc)Fâ†“
LW
ÏµsfcÏƒ
)1
4
(1)
1) RATIONALE FOR THE CHOICE OF INPUT FEATURES
The predictors, which consist of 10 GOES-16 ABI infrared
bands and 1 layer of elevation information, were selected
based on prior indications of relevance to the task. Atmo-
spheric water vapor (bands 7-10) is the main contributor to
the attenuated thermal infrared signal that reaches satellite
sensors [20], [21]. The cloud properties wavelength (band 11)
is used in the MODIS LST algorithm, while the accuracy of
retrievals also depends on ozone (band 12) [43]. Band 13 pro-
vides longwave infrared information that aids in identiï¬ca-
tion of clouds and other atmospheric features. Bands 14 and
15 convey information about surface longwave radiation and
are used with the split window technique in the GOES-R
ABI LST standard product. In the case of GOES-16 data,
the 10 infrared bands were also chosen for their availability
at all times of day. The remaining 6 GOES-16 bands are
visible or near-infrared and are available only during daytime.
Elevation was also included as a covariate. Elevation inï¬‚u-
ences the thermal environment by a negative correlation with
LST [44].
2) CONSTRUCTION OF TRAINING DATABASE
We used geolocation information and observation times to
match MODIS Terra/Aqua Daily LST with L1-G products
generated by GeoNEX. MODIS LST was projected to the
2 km L1-G grid in Python. MODIS observation times were
converted from local solar time to the Universal Coordinated
Time system used by L1-G. Then, the nearest 10-minute
geostationary observations were selected from the NEX
database to obtain quasi-simultaneous pairs observed no more
than 5 minutes apart. Both daytime and nighttime observa-
tions were used as infrared observations are available day
and night. Tile matching resulted in over 30,000 MODIS
Terra tiles paired with GOES-16 L1-G data from 2019. For
validation and generalization experiments, pairs were also
constructed for the year 2020 with Terra/GOES-16, Aqua/
GOES-16, Terra/Himawari-8, and Aqua/Himawari-8. Eleva-
tion data was prepared as an ancillary data source from the
Shuttle Radar Topography Mission global digital elevation
model (SRTM30) by projecting elevation to the 2 km L1-G
grid.
B. MODEL ARCHITECTURE AND TRAINING
We present NEXAI-LST, a deep learning model for retrieval
of land surface temperature using remotely sensed infrared
bands and elevation data. We adopt a convolutional neural
network architecture that is suitable to various image pro-
cessing tasks and has been used extensively in modeling land
surface temperature due to its capacity for leveraging spatial
features [35], [36], [45]. The detailed architecture is shown in
Figure 1c.
The convolutional neural network, F(x), maps geostation-
ary data, x, to land surface temperature, y, and cloud mask m.
The 11 input channels are composed of 10 geostationary
infrared bands and 1 layer of elevation information. The
model consists of four convolutional layers, each with 128 ï¬l-
ters of size 3 by 3, enclosed by one skip connection, the
presence of the which reformulates the task of the network as
learning a residual function [46], or the difference between
the output mapping F(x) and input x. The ï¬rst three convo-
lutional layers are followed by Rectiï¬ed Linear Unit (ReLU)
activations, and the last by an identity mapping. The model
excludes pooling layers, which is consistent with the taskâ€™s
dependence on relatively local information and helps to main-
tain temporal consistency in local features in the presence of
global changes in the images. The relatively wide, shallow
network was selected as a result of superior performance
in a grid search evaluating various hyperparameters. While
deeper model designs have become common in recent years,
studies have found deep networks that cannot be realized
by shallow networks [47], as well as wide networks which
cannot be realized by any narrow network [48].
Following a previously published approach to handle
discrete-continuous distributions in geoscience data, the
model was conditioned to predict LST as one channel and
clear sky probability, Ë†p, as a second channel [23]. The corre-
sponding terms of the loss function used least square errors
VOLUME 10, 2022 89225
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
(L2) and binary cross entropy loss to compute the distance
between the model output and the targets. For the network
F(x) with weights W , the loss function was written as follow-
ing for {mi}N
i=1 the classiï¬cation label, {pi}N
i=1 the predicted
clear sky probability, {yi}N
i=1 land surface temperature label,
{Ë†yi}N
i=1 the predicted land surface temperature, N the number
of pixels in the patch, and D the number of clear sky pixels in
the patch:
L(W ) = 1
N
Nâˆ‘
i=1
[
milog(Ë†pi +(1 âˆ’mi)log(1 âˆ’Ë†pi)
]
+1
D
Dâˆ‘
i,mi>0
||yi =Ë†yi||2 (2)
Data was divided into training (Terra/GOES-16 pairs from
2019) and validation (all pairs from 2020) sets. As training
examples, 64 by 64 pixel patches were extracted from images
with no overlap. Patches with no valid LST pixels (i.e. all
cloud/water) were discarded with 66.67% probability. LST
and geostationary bands were normalized to mean 0 standard
deviation 1. During training, patches were rotated, ï¬‚ipped
horizontally, and ï¬‚ipped vertically with 50% probability
to expand the dataset. Gradient descent was handled using
Adam optimization with Î²1 = 0.9, Î²2 = 0.999, Ïµ =
1e âˆ’7 and a learning rate 1e âˆ’4 for 300k iterations on
one NVIDIA V100 GPU on Pleiades, NASA Amesâ€™s high
performance computing system.
IV. EXPERIMENTS
A. LST PREDICTION FROM GOES-16
After training the model on coincident GOES-16 data and
MODIS Terra LST for the year 2019, we found that the
model predicted LST for the year 2020 with a mean absolute
error (MAE) of 1.733 K, 95% CI [1.483, 1.983] and root
mean squared error (RMSE) of 2.409 K, 95% CI [2.112,
2.705] using equal spatial representation sampling (Table 1).
A small positive bias of 0.176 K, 95% CI [âˆ’0.126, 0.478],
relative to Terra LST contributed to the overall error. These
metrics are relative to MODIS LST, which has been validated
within +/âˆ’1 K in clear sky conditions over various temper-
ature ranges [49], [50], [51]. LST should only be retrieved
under clear sky conditions to avoid conï¬‚ating surface tem-
perature with cloud top temperature. On the task of discrim-
inating between clear sky pixels and cloudy/water pixels, the
model achieved 94.6% accuracy, 95% CI [93.2, 95.9].
The model was trained on one full year of data, comprised
of both night and day observations and a full cycle of seasons.
Different seasons of the year are associated with dry and wet
atmospheres that can affect retrieval errors. We evaluated the
performance of LST retrieval and clear sky prediction over
different seasons and found that MAE varied by 0.5 K over
the course of the year, with higher error in spring and summer
than in winter and fall.
A quantile-quantile (Q-Q) plot with a 45 â—¦reference line
provides a direct pixel-wise comparison of the distributions
FIGURE 3. Quantile-quantile plots are presented for the two sensors used
to train the model (GOES-16 and Terra; a), a holdout LEO satellite
(GOES-16 and Aqua; b), a holdout GEO satellite (Himawari-8 and Terra;
c), and two holdout satellites (Himawari-8 and Aqua; d).
with quantiles of the reference distribution (Terra LST) on
the x-axis and quantiles of the NEXAI-LST distribution
(from GOES-16) on the y-axis. (Figure 3). Points on the
Q-Q plots formed a relatively straight line but show a some-
what lighter tail in the NEXAI-LST distribution than the
reference distribution. This indicates that compared to the
Terra LST distribution, the NEXAI-LST predictions had
less data in the distribution extremes. However, NEXAI-
LST closely reproduced the complex, multimodal, overall
shape of the LST distribution. The distributions of Terra LST
(Âµ=292.996 K, Ïƒ2 =99.698 K2) and NEXAI-LST from
GOES-16 (Âµ =292.801 K, Ïƒ2 =93.161 K2) were statisti-
cally similar for corresponding times and locations (Table 2).
The ML model also approximated the higher order statistics
of the LST distribution. The ability of a model to capture
observed skewness and kurtosis is scientiï¬cally important
as these are potentially nonstationary aspects of climatology
that are of interest for climate adaptation. Previous studies
have indicated that the negative skewness of temperature
is decreasing under climate change, resulting in fewer cold
extremes and more hot extremes [52], [53], [54].
Patterns of bias in 10 quantiles of the distribution
(Figure 4) indicate modest (<1K) biases between NEXAI-
LST (GOES-16) and Terra LST when excluding the top
decile. The ML model tends to overestimate temperature in
the lowest (coolest) quantiles of the distribution while under-
estimating temperature in the uppermost (warmest) quantiles.
This pattern is consistent with a thinner-tailed predicted dis-
tribution than the reference distributions from MODIS Terra.
However, a strong linear relation (R 2 = 0.977) indicated
that the regression model, which was trained on datasets from
2019, is well-ï¬tted to the new observations.
89226 VOLUME 10, 2022
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
FIGURE 4. Biases are presented in ten quantiles that span mean and extreme land surface temperatures. For GOES-16 NEXAI-LST and MODIS Terra (a),
Himawari-8 NEXAI-LST and MODIS Terra (b), GOES-16 NEXAI-LST and MODIS Aqua (c), and Himawari-8 NEXAI-LST and MODIS Aqua (d). Across the
training and holdout sensors, biases are less than 1.5K in magnitude in all quantiles except for the top and bottom deciles. In all four applications of the
model, the pattern of biases is consistent with thinner-tailed predicted than actual distributions.
TABLE 3. Summary of the ABI infrared bands, which each have a best
spatial resolution of 2 km.
B. GENERALIZATION TO HOLDOUT SENSORS
We used LST observations from Terraâ€™s companion satellite,
Aqua, to test generalization of the model to times of day
outside of the training dataset. Terra crosses the equator in
a descending orbit (north to south) at 10:30am local time
and an ascending orbit (south to north) at 10:30pm local
time. Aqua crosses the equator in an ascending orbit at
1:30pm local time and a descending orbit at 1:30am local
time. Thus, Aqua provides LST retrievals at times of day
unseen in the training set. We also applied the trained model
to predict land surface temperature from the infrared bands
of the Advanced Himawari Imager on board the geostation-
ary satellite Himawari-8. Himawari-8 observes Asia-Paciï¬c
every 10 minutes with similar bands to GOES-16â€™s Advanced
Baseline Imager.
Statistical performance was slightly poorer on the holdout
sensors than the training sensors, with MAE in the range of
1.908 to 2.144 K and RMSE in the range of 2.612 to 3.077 K.
NEXAI-LST values from both GOES-16 and Himawari-8
had a positive bias when compared to coincident MODIS
Aqua LST, while NEXAI-LST values from Himawari-8 had
a negative bias in reference to Terra LST. Accuracy of
clear sky discrimination held constant on the holdout sensors
at 93 to 94%.
Q-Q plots involving the two holdout sensors indicated
similar distributions between MODIS LST and NEXAI-LST
in terms of mean, variance, skew, and kurtosis. As seen for
the training sensor set, the NEXAI-LST distributions were
slightly lighter-tailed than the MODIS distributions. R2 val-
ues in the range of 0.940 to 0.979 indicated a strong linear
association held between the labels and predictions regardless
of distributional shifts in the holdout data with respect to the
training data.
Patterns of bias in 10 quantiles of the distribution reï¬‚ected
qualitatively similar patterns of error across the combinations
of holdout sensors. In general, bias became monotonically
less positive between the lowest quantile and some middle
quantile with near-zero bias, then monotonically more nega-
tive up to the highest quantile. The location of the transition
point from positive to negative bias varies across combi-
nations of sensors, which may be attributed in part to the
difference in the Aqua and Terra distributions. As a rule, the
biases were most pronounced in the tails of the distribution.
C. SPATIAL PATTERNS OF ERROR
The spatial patterns of error suggest that the sign as well
as magnitude of NEXAI-LST errors exhibited clustering in
space which may be related to spatial variation in land surface
cover, atmospheric conditions, or other factors (Figure 5).
In the region observed by GOES-16, higher magnitude errors
clustered on the west coasts of North and South America.
In the Himawari-8 region, the greatest magnitude of errors
appeared in Central Asia, where a large (5 to 6 K) nega-
tive bias predominates. Elsewhere in the Himawari-8 region,
biases were small and nearly uniformly positive.
The relatively few observations per geographic location
in the tropics reï¬‚ected the rarity of clear sky observations
from MODIS in these regions. The robustness of geostation-
ary sensors to cloud cover provides a motivation to utilize
GEO sensors in the tropics in particular [18]. However, their
underrepresentation in the training dataset does not appear
to have resulted in poorer performance in terms of absolute
error or bias. Rather, high errors and negative bias clus-
tered in arid, high elevation regions including the Tibetan
Plateau and Andes mountains, and the Rockies region in the
western United States. Poorer generalization to the Tibetan
Plateau region may be attributable to very low atmospheric
VOLUME 10, 2022 89227
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
FIGURE 5. NEXAI-LST predicted from GOES-16 (a-c) and Himawari-8
(d-f) are evaluated by comparison to the MODIS Terra and Aqua LST
products (predicted - MODIS). a,d, The number of matching LEO/GEO
pixels shows significant spatial variation in the number of clear sky
observations. For GOES-16 predictions, mean absolute error (b) is greatest
in the arid, high elevation western regions, where bias is largely
negative (c). For Himawari-8 predictions, a large negative bias also
appears in the Tibetan plateau (e-f).
water vapor content over this arid, high elevation region [55].
As the main determinant of atmospheric transmittance,
column water vapor has strong effects on retrieval error [56].
This error demonstrates the limited generalization of the ML
model to physical conditions not represented in the training
dataset, or could be attributable to exaggerated differences in
pixel size and viewing angle caused by topography.
D. VALIDATION WITH GROUND MEASUREMENTS
We compared GOES-16 NEXAI-LST inferences and
LST products from traditional physical-statistical models
with ground-based measurements of LST from the seven
SURFRAD stations. For benchmarking, NEXAI-LST was
extracted at times corresponding to MODIS Terra/Aqua LST
(Table 4 Panel a) and NOAA GOES-16 standard full-disk
LST (Table 4 Panel b). The NEXAI-LST product exhib-
ited greater agreement with SURFRAD measurements than
either product in terms of both absolute and relative ï¬t.
Relatively poor performance at station DRA in Desert Rock,
Nevada appeared across methods. Higher error at DRA likely
occurred due to heterogeneous surroundings or a negative
dewpoint bias associated with low moisture over the site [42].
In comparison to the MODIS LST products, NEXAI-LST
has a lower MAE at 5 out of 7 stations, a lower RMSE at
6 out of 7 stations, and a higher R2 value with SURFRAD
for all 7 stations. The ML model achieves a reduction of
the RMSE by nearly 0.5 K compared to the MODIS LST
product used for training. Where viewing angle differences
and atmospheric path length differences may have affected
the training pairs, these validation results suggest that the
model has learned to overcome these biases and noise.
In comparison to the NOAA standard product, NEXAI-
LST has a higher R2 value with SURFRAD for 5 out
of the 7 stations. In a mean across measurement stations,
NEXAI-LST also has lower MAE and RMSE with respect
to SURFRAD. The NOAA LST product is derived using a
split window technique from the same Level 1 data used
to predict NEXAI-LST. However, the split window algo-
rithm additionally uses ancillary information including atmo-
spheric total precipitable water, land surface emissivity, and
snow/ice mask, and a cloud mask. While results based on
ground measurements with limited spatial representativeness
should not be over interpreted, they suggest that our model
may perform as well as or better than the standard model
while requiring no ancillary data as inputs.
V. OBSERVING HEAT EVENTS USING NEXAI-LST
Over the summer of 2021, heat waves in the United States
Paciï¬c Northwest produced multiple record-setting tempera-
tures affecting ill-adapted population hubs. We applied our
model, which can generate full disk NEXAI-LST with a
frequency up to every 10 minutes, to predict LST for the
region over the period spanning from June 15 to July 15,
2021. In Figure 2a-b, MODIS Terra LST and NEXAI-LST
are presented side by side for observations taking place at
20:30 PST on July 28, 2021. A time series of NEXAI-LST
at a 10-minute time step demonstrates the temporal proï¬le
of diurnal temperature variation over the course of days
(Figure 2c). MODIS Terra and Aqua overpasses, plotted in
green and blue, exhibit agreement with the model inferences.
At the same time, their temporal sampling is insufï¬cient to
describe key characteristics of LST such as the intensity and
timing of peak temperature, the minimum temperature, and
the time-integrated exceedance of temperature thresholds.
Terra and Aqua products are not guaranteed to capture the
daily minima and maxima that are necessary to establish diur-
nal temperature range, including on the hotter-than-average
June 28, 2021 2d.
Temporal consistency, or the uniformity of predictions for
similar conditions close together in time, is a key indica-
tor of model reliability. Achieving temporal consistency can
be a challenge in ML approaches, such as this one, which
process images frame by frame. However, the correlation
of the NEXAI-LST predictions over time demonstrates the
temporal stability of this model.
VI. DISCUSSION
The GOES-R standard land surface temperature product aims
to meet an accuracy goal of 2.5 K conditional with 1) known
emissivity, 2) known atmospheric correction and 3) 80%
channel correction, and 5 K otherwise [57]. NEXAI-LST,
which did not take direct advantage of any of these condi-
tions, meets the 2.5 K goal for the set of training sensors
and exceeds the 5 K goal for all combinations of sensors,
while improving on both spatial and temporal resolution at
the full disk scale. However, our model does depend on the
89228 VOLUME 10, 2022
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
TABLE 4. Comparison of GOES-16 NEXAI-LST and coincident LST from
physical-statistical models (MODIS Terra/Aqua, panel a; GOES-16
standard product, panel b) against ground station observations from the
SURFRAD network from the year 2020.
availability of training data taking advantage of these ancil-
lary datasets.
Variance between MODIS LST and NEXAI-LST can be
attributed to sources including: imperfect approximation of
model physics by the network, spatial and temporal shifts
between LEO and GEO data, differences in viewing and
illumination geometry, error in the GEO radiance values,
and error in the LEO data product. It should be noted that
MODIS LST products are commonly validated for inter-
mediate values in the distribution: Coll et al. found a bias
of +0.1 K and standard deviation of 0.6 K for a range of
surface temperatures between 298.15 and 305.15 K [49],
while Wan et al. found better than 1 K accuracy in the
range from 263 to 300 K [50]. According to our assessment,
these ranges of intermediate temperature represent only lim-
ited portions (approximately 30% and approximately 80%,
respectively) of the observed LST distribution. Neither study
evaluated performance on the warmest 10% of observations.
Evaluating the bias of our predictions relative to MODIS
LST across multiple quantiles of the MODIS LST distribution
indicated that NEXAI-LST is less biased for the middle
of the distribution but has higher error in the warm and
cool tails of the distribution, which have less well-deï¬ned
accuracy characteristics. The proposed model was trained
with the objective of minimizing global error between out-
puts and labels, so understanding the modelâ€™s performance
in rare or complex conditions is challenging. Training with
importance weighting might improve performance for rare
cold and hot values in the future. Other ML-based studies of
LST prediction have attained MAE of 2.85 K on day ahead
prediction [58] and MAE of 0.16-0.26 K using other weather
variables as predictors [59] over limited geographic regions.
This modeling approach, which was demonstrated for two
geostationary sensors, has potential application to further
geostationary sensors, which could result in nearly full global
coverage. In contrast to hand-tuned physics-based models,
which result in spatially and temporally coarsened data,
our ML model can produce LST at spatiotemporal reso-
lutions constrained only by the attributes of Level 1 data
(2 km/10 minutes). Sub-hourly geostationary observations
result in more cloud-free observations [18], which, together
with their unprecedented spatial and temporal consistency,
can help reconstruct cloudy images and answer questions
about temperature variation over ï¬ne timescales. Remote
sensing of surface urban heat islands [9], [60] has an increas-
ingly important role in providing data to inform climate
adaptation policy and extreme heat response.
VII. CONCLUSION
In this study we used deep learning to train a data-driven
model for MODIS-like land surface temperature for applica-
tion to geostationary sensors. We found that the ML model
could predict spatially and temporally consistent LST at a
ï¬ner resolution than the standard GOES-R product. Fur-
ther, we explored the generalizability of the approach across
unseen times of day, spatial regions, and satellite sensors.
In the generalization studies, we found that a model trained
on one LEO-GEO pair (Terra and GOES-16) can credibly
transfer to holdout LEO and GEO satellites based on statis-
tical performance, but with some limitations in application
to out-of-sample conditions. In validation against indepen-
dent, ground-based measurements, we found that our model
performs as well or better than the standard GOES-16 and
MODIS LST product, while not depending on prior infor-
mation about land surface and atmosphere states. These
experiments demonstrate the capability of deep learning
models to approximate complex physics-based functions by
learning from huge, real-world datasets. Our LEO-GEO
approach is complementary to physics-based modeling, as it
draws obliquely upon the MODIS LST algorithm to generate
NEXAI-LST, a product of improved spatial and temporal
resolution.
REFERENCES
[1] G. A. Meehl and C. Tebaldi, â€˜â€˜More intense, more frequent, and longer
lasting heat waves in the 21st century,â€™â€™ Science, vol. 305, no. 5686,
pp. 994â€“997, Aug. 2004.
[2] S. E. Perkins-Kirkpatrick and S. C. Lewis, â€˜â€˜Increasing trends in regional
heatwaves,â€™â€™Nature Commun., vol. 11, no. 1, pp. 1â€“8, Dec. 2020.
[3] Centers for Disease Control and Prevention, â€˜â€˜Heat-related deathsâ€”United
States, 1999â€“2003,â€™â€™ Morbidity Mortality Weekly Rep., vol. 55, no. 29,
pp. 796â€“798, 2006.
[4] A. Hsu, G. Sheriff, T. Chakraborty, and D. Manya, â€˜â€˜Disproportionate
exposure to urban heat island intensity across major US cities,â€™â€™ Nature
Commun., vol. 12, no. 1, pp. 1â€“11, Dec. 2021.
[5] T. Li, J. Ban, R. M. Horton, D. A. Bader, G. Huang, Q. Sun, and
P. L. Kinney, â€˜â€˜Heat-related mortality projections for cardiovascular and
respiratory disease under the changing climate in Beijing, China,â€™â€™ Sci.
Rep., vol. 5, no. 1, pp. 1â€“8, Sep. 2015.
VOLUME 10, 2022 89229
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
[6] C. K. Uejio, J. D. Tamerius, J. Vredenburg, G. Asaeda, D. A. Isaacs,
J. Braun, A. Quinn, and J. P. Freese, â€˜â€˜Summer indoor heat exposure and
respiratory and cardiovascular distress calls in New York City, NY, U.S.,â€™â€™
Indoor Air, vol. 26, no. 4, pp. 594â€“604, Aug. 2016.
[7] A. Barreca and J. Schaller, â€˜â€˜The impact of high ambient temperatures on
delivery timing and gestational lengths,â€™â€™ Nature Climate Change, vol. 10,
no. 1, pp. 77â€“82, Jan. 2020.
[8] Y.-C. Chen, H.-W. Chiu, Y.-F. Su, Y.-C. Wu, and K.-S. Cheng, â€˜â€˜Does
urbanization increase diurnal land surface temperature variation? Evidence
and implications,â€™â€™ Landscape Urban Planning, vol. 157, pp. 247â€“258,
Jan. 2017.
[9] Y. Chang, J. Xiao, X. Li, S. Frolking, D. Zhou, A. Schneider, Q. Weng,
P. Yu, X. Wang, X. Li, S. Liu, and Y. Wu, â€˜â€˜Exploring diurnal cycles of
surface urban heat island intensity in Boston with land surface temperature
data derived from GOES-R geostationary satellites,â€™â€™ Sci. Total Environ.,
vol. 763, Apr. 2021, Art. no. 144224.
[10] R. R. Nemani and S. W. Running, â€˜â€˜Estimation of regional surface resis-
tance to evapotranspiration from NDVI and thermal-IR A VHRR data,â€™â€™
J. Appl. Meteorol., vol. 28, no. 4, pp. 276â€“284, Apr. 1989.
[11] M. Zink, J. Mai, M. Cuntz, and L. Samaniego, â€˜â€˜Conditioning a hydrologic
model using patterns of remotely sensed land surface temperature,â€™â€™ Water
Resour. Res., vol. 54, no. 4, pp. 2976â€“2998, Apr. 2018.
[12] H. L. Shah, T. Zhou, M. Huang, and V. Mishra, â€˜â€˜Strong inï¬‚uence of irriga-
tion on water budget and land surface temperature in Indian subcontinental
river basins,â€™â€™ J. Geophys. Res., Atmos., vol. 124, no. 3, pp. 1449â€“1462,
Feb. 2019.
[13] D. Sims, A. Rahman, V. Cordova, B. Elmasri, D. Baldocchi, P. Bolstad,
L. Flanagan, A. Goldstein, D. Hollinger, and L. Misson, â€˜â€˜A new model
of gross primary productivity for North American ecosystems based
solely on the enhanced vegetation index and land surface temperature
from MODIS,â€™â€™ Remote Sens. Environ., vol. 112, no. 4, pp. 1633â€“1646,
Apr. 2008.
[14] Y. Yan, K. Mao, J. Shi, S. Piao, X. Shen, J. Dozier, Y. Liu, H.-L. Ren, and
Q. Bao, â€˜â€˜Driving forces of land surface temperature anomalous changes
in North America in 2002â€“2018,â€™â€™ Sci. Rep., vol. 10, no. 1, pp. 1â€“13,
Dec. 2020.
[15] K. Braganza, D. J. Karoly, and J. M. Arblaster, â€˜â€˜Diurnal temperature
range as an index of global climate change during the twentieth century,â€™â€™
Geophys. Res. Lett., vol. 31, no. 13, Jul. 2004, Art. no. L13217.
[16] G. Wang and M. E. Dillon, â€˜â€˜Recent geographic convergence in diurnal
and annual temperature cycling ï¬‚attens global thermal proï¬les,â€™â€™ Nature
Climate Change, vol. 4, no. 11, pp. 988â€“992, Nov. 2014.
[17] Z.-L. Li, B.-H. Tang, H. Wu, H. Ren, G. Yan, Z. Wan, I. F. Trigo,
and J. A. Sobrino, â€˜â€˜Satellite-derived land surface temperature: Current
status and perspectives,â€™â€™ Remote Sens. Environ., vol. 131, pp. 14â€“37,
Apr. 2013.
[18] H. Hashimoto, W. Wang, J. L. Dungan, S. Li, A. R. Michaelis, H. Takenaka,
A. Higuchi, R. B. Myneni, and R. R. Nemani, â€˜â€˜New generation geostation-
ary satellite observations support seasonality in greenness of the Amazon
evergreen forests,â€™â€™ Nature Commun., vol. 12, no. 1, pp. 1â€“11, Dec. 2021.
[19] Z. Wan, â€˜â€˜New reï¬nements and validation of the collection-6 MODIS land-
surface temperature/emissivity product,â€™â€™ Remote Sens. Environ., vol. 140,
pp. 36â€“45, Jan. 2014.
[20] B.-H. Tang, â€˜â€˜Nonlinear split-window algorithms for estimating land and
sea surface temperatures from simulated Chinese Gaofen-5 satellite data,â€™â€™
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 11, pp. 6280â€“6289,
Nov. 2018.
[21] J. A. Sobrino, Z.-L. Li, and M. P. Stoll, â€˜â€˜Impact of the atmospheric
transmittance and total water vapor content in the algorithms for estimating
satellite sea surface temperatures,â€™â€™ IEEE Trans. Geosci. Remote Sens.,
vol. 31, no. 5, pp. 946â€“952, Sep. 1993.
[22] X. Shi, Z. Gao, L. Lausen, H. Wang, D.-Y. Yeung, W.-K. Wong, and
W.-C. Woo, â€˜â€˜Deep learning for precipitation nowcasting: A benchmark
and a new model,â€™â€™ in Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017,
pp. 1â€“11.
[23] T. Vandal, E. Kodra, J. Dy, S. Ganguly, R. Nemani, and A. R. Ganguly,
â€˜â€˜Quantifying uncertainty in discrete-continuous and skewed data with
Bayesian deep learning,â€™â€™ in Proc. 24th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, Jul. 2018, pp. 2377â€“2386.
[24] Y. Liu, A. R. Ganguly, and J. Dy, â€˜â€˜Climate downscaling using YNet: A
deep convolutional network with skip connections and fusion,â€™â€™ in Proc.
26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2020,
pp. 3145â€“3153.
[25] M. Reichstein, G. Camps-Valls, B. Stevens, M. Jung, J. Denzler,
N. Carvalhais, and M. Prabhat, â€˜â€˜Deep learning and process understand-
ing for data-driven earth system science,â€™â€™ Nature, vol. 566, no. 7743,
pp. 195â€“204, Feb. 2019.
[26] R. Wang, K. Kashinath, M. Mustafa, A. Albert, and R. Yu, â€˜â€˜Towards
physics-informed deep learning for turbulent ï¬‚ow prediction,â€™â€™ in Proc.
26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2020,
pp. 1457â€“1466.
[27] R. Lagerquist, D. Turner, I. Ebert-Uphoff, J. Stewart, and V. Hagerty,
â€˜â€˜Using deep learning to emulate and accelerate a radiative-transfer
model,â€™â€™ J. Atmos. Ocean. Technol., vol. 38, no. 10, pp. 1673â€“1696,
Jul. 2021.
[28] K. Duffy, T. J. Vandal, W. Wang, R. R. Nemani, and A. R. Ganguly,
â€˜â€˜A framework for deep learning emulation of numerical models with a
case study in satellite remote sensing,â€™â€™ IEEE Trans. Neural Netw. Learn.
Syst., early access, May 5, 2022, doi: 10.1109/TNNLS.2022.3169958.
[29] M. F. Kasim, D. Watson-Parris, L. Deaconu, S. Oliver, P. Hatï¬eld,
D. H. Froula, G. Gregori, M. Jarvis, S. Khatiwala, J. Korenaga,
J. Topp-Mugglestone, E. Viezzer, and S. M. Vinko, â€˜â€˜Building high
accuracy emulators for scientiï¬c simulations with deep neural archi-
tecture search,â€™â€™ Mach. Learn., Sci. Technol., vol. 3, no. 1, Mar. 2022,
Art. no. 015013.
[30] S. Basu, S. Ganguly, R. R. Nemani, S. Mukhopadhyay, G. Zhang,
C. Milesi, A. Michaelis, P. Votava, R. Dubayah, L. Duncanson, B. Cook,
Y. Yu, S. Saatchi, R. DiBiano, M. Karki, E. Boyda, U. Kumar, and S. Li,
â€˜â€˜A semiautomated probabilistic framework for tree-cover delineation from
1-m NAIP imagery using a high-performance computing architecture,â€™â€™
IEEE Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5690â€“5708,
Oct. 2015.
[31] T. J. Vandal, D. McDuff, W. Wang, K. Duffy, A. Michaelis, and
R. R. Nemani, â€˜â€˜Spectral synthesis for geostationary satellite-to-satellite
translation,â€™â€™IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1â€“11, 2021.
[32] N. Jean, M. Burke, M. Xie, W. M. Davis, D. B. Lobell, and S. Ermon,
â€˜â€˜Combining satellite imagery and machine learning to predict poverty,â€™â€™
Science, vol. 353, no. 6301, pp. 790â€“794, 2016.
[33] H. Wang, K. Mao, Z. Yuan, J. Shi, M. Cao, Z. Qin, S. Duan, and B. Tang,
â€˜â€˜A method for land surface temperature retrieval based on model-data-
knowledge-driven and deep learning,â€™â€™ Remote Sens. Environ., vol. 265,
Nov. 2021, Art. no. 112665.
[34] G. Yang, R. Pu, W. Huang, J. Wang, and C. Zhao, â€˜â€˜A novel method
to estimate subpixel temperature by fusing solar-reï¬‚ective and thermal-
infrared remote-sensing data with an artiï¬cial neural network,â€™â€™ IEEE
Trans. Geosci. Remote Sens., vol. 48, no. 4, pp. 2170â€“2178, Apr. 2010.
[35] P. Wu, Z. Yin, H. Yang, Y. Wu, and X. Ma, â€˜â€˜Reconstructing geostationary
satellite land surface temperature imagery based on a multiscale feature
connected convolutional neural network,â€™â€™ Remote Sens., vol. 11, no. 3,
p. 300, Feb. 2019.
[36] M. Chen, B. H. Newell, Z. Sun, C. A. Corr, and W. Gao, â€˜â€˜Recon-
struct missing pixels of Landsat land surface temperature product using
a CNN with partial convolution,â€™â€™ Proc. SPIE, vol. 11139, Sep. 2019,
Art. no. 111390E.
[37] K. Mao, J. Shi, H. Tang, Z.-L. Li, X. Wang, and K.-S. Chen, â€˜â€˜A neural
network technique for separating land surface emissivity and temperature
from ASTER imagery,â€™â€™ IEEE Trans. Geosci. Remote Sens., vol. 46, no. 1,
pp. 200â€“208, Jan. 2008.
[38] T. Chang and X. J. Xiong, â€˜â€˜Assessment of GOES-16/ABI middle wave
infrared band using references of Himawari-8/AHI and Aqua/MODIS,â€™â€™
Proc. SPIE, vol. 11127, Sep. 2019, Art. no. 111270T.
[39] W. Wang, S. Li, H. Hashimoto, H. Takenaka, A. Higuchi, S. Kalluri, and
R. Nemani, â€˜â€˜An introduction to the geostationary-NASA earth exchange
(GeoNEX) products: 1. Top-of-atmosphere reï¬‚ectance and brightness tem-
perature,â€™â€™Remote Sens., vol. 12, no. 8, p. 1267, Apr. 2020.
[40] J. A. Augustine, J. J. DeLuisi, and C. N. Long, â€˜â€˜SURFRADâ€”A national
surface radiation budget network for atmospheric research,â€™â€™ Bull. Amer.
Meteorol. Soc., vol. 81, no. 10, pp. 2341â€“2358, 2000.
[41] K. Wang, â€˜â€˜Estimation of surface long wave radiation and broadband
emissivity using moderate resolution imaging spectroradiometer (MODIS)
land surface temperature/emissivity products,â€™â€™ J. Geophys. Res., Atmos.,
vol. 110, no. D11, 2005, Art. no. D11109.
[42] A. K. Heidinger, I. Laszlo, C. C. Molling, and D. Tarpley, â€˜â€˜Using
SURFRAD to verify the NOAA single-channel land surface temperature
algorithm,â€™â€™ J. Atmos. Ocean. Technol., vol. 30, no. 12, pp. 2868â€“2884,
Dec. 2013.
89230 VOLUME 10, 2022
K. Duffyet al.: Multisensor Machine Learning to Retrieve High Spatiotemporal Resolution LST
[43] G. C. Hulley, S. J. Hook, and C. Hughes, â€˜â€˜MODIS MOD21 land surface
temperature and emissivity algorithm theoretical basis document,â€™â€™ Jet
Propuls. Lab., Nat. Aeronaut. Space Admin., Pasadena, CA, USA, JPL
Publication 12-17, 2012.
[44] X. Peng, W. Wu, Y. Zheng, J. Sun, T. Hu, and P. Wang, â€˜â€˜Correlation anal-
ysis of land surface temperature and topographic elements in Hangzhou,
China,â€™â€™Sci. Rep., vol. 10, no. 1, pp. 1â€“16, Dec. 2020.
[45] H. Jia, D. Yang, W. Deng, Q. Wei, and W. Jiang, â€˜â€˜Predicting land surface
temperature with geographically weighed regression and deep learning,â€™â€™
Wiley Interdiscipl. Rev., Data Mining Knowl. Discovery, vol. 11, no. 1,
Jan. 2021, Art. no. e1396.
[46] K. He, X. Zhang, S. Ren, and J. Sun, â€˜â€˜Deep residual learning for image
recognition,â€™â€™ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),
Jun. 2016, pp. 770â€“778.
[47] Y. Bengio and O. Delalleau, â€˜â€˜On the expressive power of deep
architectures,â€™â€™ in Proc. Int. Conf. Algorithmic Learn. Theory. Berlin,
Germany: Springer, 2011, pp. 18â€“36.
[48] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, â€˜â€˜The expressive power of
neural networks: A view from the width,â€™â€™ in Proc. Adv. Neural Inf. Process.
Syst., vol. 30, 2017, pp. 1â€“9.
[49] C. Coll, V. Caselles, J. M. Galve, E. Valor, R. Niclos, J. M. SÃ¡nchez, and
R. Rivas, â€˜â€˜Ground measurements for the validation of land surface tem-
peratures derived from AATSR and MODIS data,â€™â€™ Remote Sens. Environ.,
vol. 97, no. 3, pp. 288â€“300, Aug. 2005.
[50] Z. Wan, Y. Zhang, Q. Zhang, and Z.-L. Li, â€˜â€˜Validation of the land-
surface temperature products retrieved from Terra moderate resolution
imaging spectroradiometer data,â€™â€™Remote Sens. Environ., vol. 83, nos. 1â€“2,
pp. 163â€“180, Nov. 2002.
[51] Z. Wan, â€˜â€˜New reï¬nements and validation of the MODIS land-surface
temperature/emissivity products,â€™â€™ Remote Sens. Environ., vol. 112, no. 1,
pp. 59â€“74, 2008.
[52] K. A. McKinnon, A. Rhines, M. P. Tingley, and P. Huybers, â€˜â€˜The changing
shape of Northern hemisphere summer temperature distributions,â€™â€™ J. Geo-
phys. Res., Atmos., vol. 121, no. 15, pp. 8849â€“8868, Aug. 2016.
[53] A. Rhines, K. A. McKinnon, M. P. Tingley, and P. Huybers, â€˜â€˜Seasonally
resolved distributional trends of North American temperatures show con-
traction of winter variability,â€™â€™ J. Climate, vol. 30, no. 3, pp. 1139â€“1157,
Feb. 2017.
[54] B. H. Samset, C. W. Stjern, M. T. Lund, C. W. Mohr, M. Sand,
and A. S. Daloz, â€˜â€˜How daily temperature and precipitation distributions
evolve with global surface temperature,â€™â€™ Earthâ€™s Future, vol. 7, no. 12,
pp. 1323â€“1336, Dec. 2019.
[55] X. Guan, L. Yang, Y. Zhang, and J. Li, â€˜â€˜Spatial distribution, temporal vari-
ation, and transport characteristics of atmospheric water vapor over Central
Asia and the arid region of China,â€™â€™ Global Planet. Change, vol. 172,
pp. 159â€“178, Jan. 2019.
[56] F. Wang, Z. Qin, C. Song, L. Tu, A. Karnieli, and S. Zhao, â€˜â€˜An improved
mono-window algorithm for land surface temperature retrieval from
Landsat 8 thermal infrared sensor data,â€™â€™ Remote Sens., vol. 7, no. 4,
pp. 4268â€“4289, Apr. 2015.
[57] P. Yu, Y. Yu, Y. Rao, Y. Liu, and H. Wang, â€˜â€˜Status of the GOES-R land
surface temperature product,â€™â€™ in Proc. AGU Fall Meeting. Washington,
DC, USA: AGU, 2018, p. 1.
[58] S. Kartal and A. Sekertekin, â€˜â€˜Prediction of MODIS land surface tem-
perature using new hybrid models based on spatial interpolation tech-
niques and deep learning models,â€™â€™ Environ. Sci. Pollut. Res., pp. 1â€“20,
May 2022.
[59] R. Maddu, A. R. Vanga, J. K. Sajja, G. Basha, and R. Shaik, â€˜â€˜Prediction
of land surface temperature of major coastal cities of India using bidirec-
tional LSTM neural networks,â€™â€™ J. Water Climate Change, vol. 12, no. 8,
pp. 3801â€“3819, Dec. 2021.
[60] D. Zhou, S. Zhao, S. Liu, L. Zhang, and C. Zhu, â€˜â€˜Surface urban heat
island in Chinaâ€™s 32 major cities: Spatial patterns and drivers,â€™â€™ Remote
Sens. Environ., vol. 152, pp. 51â€“61, Sep. 2014.
KATE DUFFY received the B.Sc. degree in envi-
ronmental engineering from Cornell University,
Ithaca, NY, USA, in 2015, and the M.S. degree
in civil engineering and the Ph.D. degree in inter-
disciplinary engineering from Northeastern Uni-
versity, Boston, MA, USA, in 2019 and 2021,
respectively.
She is currently a Research Scientist with the
NASA Ames Research Center, Mountain View,
CA, USA, and the Bay Area Environmental
Research Institute, Moffett Field, CA, USA. Her research interests include
climate change, remote sensing, and applied machine learning.
THOMAS J. VANDAL received the B.S. degree
in mathematics from the University of Maryland,
College Park, MD, USA, in 2012, and the
Ph.D. degree in interdisciplinary engineering
from Northeastern University, Boston, MA, USA,
in 2018.
He is currently a Research Scientist with the
NASA Ames Research Center, Mountain View,
CA, USA, and the Bay Area Environmental
Research Institute, Moffett Field, CA, USA. His
research interests include machine learning and Earth sciences.
RAMAKRISHNA R. NEMANIreceived the B.S.
degree from Andhra Pradesh Agricultural Univer-
sity, Bapatla, India, in 1979, the M.S. degree from
Punjab Agricultural University, Ludhiana, India,
in 1982, and the Ph.D. degree from the University
of Montana, Missoula, MT, USA, in 1987.
He is currently a Senior Earth Scientist with the
Ecological Forecasting Laboratory, NASA Ames
Research Center, Moffett Field, CA, USA. His
research interests include ecological forecasting
and collaborative computing in the Earth sciences.
VOLUME 10, 2022 89231
--- END OF Multisensor_Machine_Learning_to_Retrieve_High_Spatiotemporal_Resolution_Land_Surface_Temperature.pdf ---

--- START OF remotesensing-12-02691-v3.pdf ---
remote sensing  
Article
Estimating Land Surface Temperature from Satellite
Passive Microwave Observations with the Traditional
Neural Network, Deep Belief Network,
and Convolutional Neural Network
Shaofei Wang 1, Ji Zhou 1, *
 , Tianjie Lei 2, Hua Wu 3
 , Xiaodong Zhang 1, Jin Ma 1 and
Hailing Zhong 1
1 School of Resources and Environment, Center for Information Geoscience, University of Electronic Science
and Technology of China, Chengdu 611731, China; 201822070311@std.uestc.edu.cn (S.W.);
201611180105@std.uestc.edu.cn (X.Z.); majin@std.uestc.edu.cn (J.M.); 201721180114@std.uestc.edu.cn (H.Z.)
2 China Institute of Water Resources and Hydropower Research (IWHR), Beijing 100038, China; leitj@iwhr.com
3 State Key Laboratory of Resources and Environment Information System, Institute of Geographic Sciences
and Natural Resources Research, Chinese Academy of Sciences, Beijing 100101, China; wuhua@igsnrr.ac.cn
* Correspondence: jzhou233@uestc.edu.cn
Received: 16 July 2020; Accepted: 17 August 2020; Published: 20 August 2020
/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001
/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046
Abstract: Neural networks, especially the latest deep learning, have exhibited good ability in
estimating surface parameters from satellite remote sensing. However, thorough examinations of
neural networks in the estimation of land surface temperature (LST) from satellite passive microwave
(MW) observations are still lacking. Here, we examined the performances of the traditional neural
network (NN), deep belief network (DBN), and convolutional neural network (CNN) in estimating
LST from the AMSR-E and AMSR2 data over the Chinese landmass. The examinations were
based on the same training set, validation set, and test set extracted from 2003, 2004, and 2009,
respectively, for AMSR-E with a spatial resolution of 0.25â—¦. For AMSR2, the three sets were extracted
from 2013, 2014, and 2016 with a spatial resolution of 0.1â—¦, respectively. MODIS LST played the role
of â€œground truthâ€ in the training, validation, and testing. The examination results show that CNN is
better than NN and DBN by 0.1â€“0.4 K. Diï¬€erent combinations of input parameters were examined to
get the best combinations for the daytime and nighttime conditions. The best combinations are the
brightness temperatures (BTs), NDVI, air temperature, and day of the year (DOY) for the daytime and
BTs and air temperature for the nighttime. By adding three and one easily obtained parameters on
the basis of BTs, the accuracies of LST estimates can be improved by 0.8 K and 0.3 K for the daytime
and nighttime conditions, respectively. Compared with the MODIS LST, the CNN LST estimates
yielded root-mean-square diï¬€erences (RMSDs) of 2.19â€“3.58 K for the daytime and 1.43â€“2.14 K for
the nighttime for diverse land cover types for AMSR-E. Validation against the in-situ LSTs showed
that the CNN LSTs yielded root-mean-square errors of 2.10â€“4.72 K for forest and cropland sites.
Further intercomparison indicated that ~50% of the CNN LSTs were closer to the MODIS LSTs than
ESAâ€™s GlobTemperature AMSR-E LSTs, and the average RMSDs of the CNN LSTs were less than
3 K over dense vegetation compared to NASAâ€™s global land parameter data record air temperatures.
This study helps better the understanding of the use of neural networks for estimating LST from
satellite MW observations.
Keywords: land surface temperature; neural network; deep belief network; convolutional neural
network; passive microwave
Remote Sens.2020, 12, 2691; doi:10.3390/rs12172691 www.mdpi.com /journal/remotesensing
Remote Sens.2020, 12, 2691 2 of 27
1. Introduction
Land surface temperature (LST) is a key factor in earthâ€“atmosphere interactions and an important
indicator for monitoring environmental changes and energy balance on Earthâ€™s surface [ 1â€“3].
Compared with ground-based LST measurements, satellite remote sensing is more e ï¬€ective at
regional and global scales [ 4]. Thermal infrared (TIR) remote sensing is the main approach for
estimating LST from satellite remote sensing data. In recent decades, many algorithms have been
proposed [5â€“7], and the TIR LST datasets have become relatively mature [8]. TIR remote sensing has
the advantages of high accuracy and ï¬ne resolution. However, it can only obtain valid observations
under clear-sky conditions.
On average, 60% of the land surface is covered by clouds, which limits the practical application of
TIR LSTs [9â€“11]. An alternative method is to use satellite passive microwave (MW) measurements
to estimate LSTs. MW has the advantage of the ability to penetrate clouds, which can cover the
deï¬ciencies of TIR. Although MW observations have coarser spatial resolution than TIR, LST estimated
from MW observations can be an important complement to TIR LST [12,13]. Several methods have
been proposed to estimate MW LST, and these methods can be grouped into three categories: empirical
algorithms [9,12,14,15], semi-empirical algorithms [16â€“20], and physically-based algorithms [13,21].
Currently, there are three main problems in estimating accurate MW LST. The ï¬rst problem is the
diï¬ƒculty of determining MW emissivity, which depends greatly on surface conditions (e.g., vegetation
cover and snow cover) [ 22,23]. The complex relationships between these inï¬‚uencing factors and
surface emissivity makes diï¬ƒcult to express MW emissivity using an analytical expression. The second
problem is the atmospheric correction. Although MW remote sensing is less aï¬€ected by the atmosphere
than TIR remote sensing, the removal of atmospheric eï¬€ects is still necessary for higher frequencies
(>10 GHz). Finally, the temperature retrieved from MW measurements is a bulk temperature, which is
evidently diï¬€erent from the skin temperature obtained by TIR. Thus, there exists a physical diï¬€erence
between MW LST and TIR LST. The diï¬€erence is caused by diï¬€erent thermal sampling depths (TSDs)
of MW and TIR [3,24].
Neural networks have strong nonlinear ï¬tting ability and have been applied in the retrieval of MW
LST. For example, Aires et al. [25] established a neural network model for the simultaneous retrieval
of atmospheric water vapor, the cloud liquid water path, LST, and emissivities from special sensor
microwave/imager (SSM/I) data with a spatial resolution of 0.25â—¦; the theoretical root-mean-square
error (RMSE) of the LST over the globe is 1.3 K under clear-sky and 1.6 K under cloudy conditions.
JimÃ©nez et al. [26] and Ermida et al. [27] used neural networks for LST retrieval from the Advanced
Microwave Scanning Radiometer for EOS (AMSR-E) data and generated the European Space Agencyâ€™s
(ESA) GlobTemperature AMSR-E LST product for the period from 2008â€“2010 with a spatial resolution
of 14 Ã—8 km; to reduce the dependence on auxiliary data, monthly AMSR-E emissivities [22,28] were
taken as the input parameters except for the brightness temperatures; the evaluation results showed
that after masking out snow and deserts, the LST di ï¬€erences between AMSR-E and MODIS were
3.0 Â±5.1 K and 1.4 Â±3.9 K for the daytime and nighttime conditions, respectively. These studies have
conï¬rmed that neural networks can be applied to the estimation of MW LST. In fact, the neural network
utilized by these studies was the traditional neural network (hereafter termed NN).
With the development of neural networks, there have been some other networks with diï¬€erent
architectures named deep learning (e.g., deep belief networkâ€”DBN, and convolutional neural
networkâ€”CNN). The DBN was proposed by Hinton et al. [ 29] to solve the optimization problem of
deep neural networks, while CNN has been well recognized by the scientiï¬c communities for use in
image processing [30]. Recently, DBN and CNN have been gradually applied to the estimation of remote
sensing parameters. For example, Li et at. [ 31] compared the performance of DBN with other methods
for the process of upscaling evapotranspiration, and the results show that the accuracy of DBN is lower
than that of the other methods. Shen et al. [ 32] employed the DBN for air temperature estimation
by fusing MODIS LST, station, simulation, and socioeconomic data, and found that the RMSE was
1.996 â—¦C at the national scale. Ge et al. [33] compared the performance of the deep convolutional neural
Remote Sens.2020, 12, 2691 3 of 27
network (DCNN) with NN for estimating soil moisture and found that DCNN performs slightly better
than NN. Tan et al. [34] constructed the LST retrieval model for the Advanced Microwave Scanning
Radiometer 2 (AMSR2) data with CNN and found that its frequencies except 6.9 GHz were able to
obtain the most accurate CNN model, which has an RMSE of 2.69 K. Sadeghi et al. [35] examined the
performance of CNN in the estimation of precipitation and conï¬rmed the good ability of CNN.
Although there have been some studies using these new networks for retrieving surface parameters
from satellite remote sensing data, examinations on the performances of the neural networks in the
estimation of LST from MW observations are still lacking. In addition, the CNN model constructed
by Tan et al. [ 34] only takes AMSR2 brightness temperatures (BTs) as inputs. However, whether
adding more input parameters that can be easily obtained would improve the accuracy of the LST
estimate is a question that is important for the scientiï¬c communities. Under this context, the ï¬rst
objective of this study was to examine the performances of NN, DBN, and CNN based on multiple
input parameters, including BTs, surface parameters, atmospheric related parameters, and day of
the year (DOY), and the second objective was to estimate LST from MW observations based on the
best performing neural network with the best input parameter combinations. In this study, China
was selected as the study area for two main reasons. The ï¬rst reason is that China has diverse land
cover types, high elevation variations, and diï¬€erent climate types. The second reason is that China,
especially South China, suï¬€ers from frequent cloud coverage. These two reasons make it a good area
for examining the performances of these networks in MW LST estimation. The time period under
examination was 2003â€“2010 for AMSR-E and 2013â€“2016 for AMSR2.
2. Datasets
2.1. AMSR-E and AMSR2 Data
AMSR-E is a twelve-channel and six-frequency passive microwave radiometer that measures the
BTs of the Earthâ€™s surface at 6.925, 10.65, 18.7, 23.8, 36.5, and 89.0 GHz. Vertically and horizontally
polarized measurements are taken at all frequencies. AMSR-E was onboard the Aqua satellite from
2002 to 2011. The native spatial resolution is ~5 km at 89.0 GHz and 60 km at 6.9 GHz. The AMSR-E
BTs used here are from the AMSR-E level 3 product with a spatial resolution of 0.25â—¦. This product
is the daily average of BT in level 1B product and projected by equi-rectangular. As the successor of
AMSR-E, AMSR2 is onboard the Global Change Observation Mission 1st-Water (GCOM-W1) satellite
and has seven frequencies (i.e., 6.925, 7.3, 10.65, 18.7, 23.8, 36.5, and 89.0 GHz) in both horizontal and
vertical polarizations. Compared with AMSR-E, AMSR2 has higher spatial resolutions. The native
spatial resolution of AMSR2 is 3 Ã—5 km at 89.0 GHz and 35 Ã—62 km at 6.925 GHz. The AMSR2 BTs
used here are from the AMSR2 level 3 product with a spatial resolution of 0.1â—¦. This product is also
daily average data of level 1B data and projected by equi-rectangular. The overpass time of AMSR-E
and AMSR2 are both ~13:30 (ascending) and ~01:30 (descending) local solar time. The level 3 products
of AMSR-E and AMSR2 were downloaded at https://gportal.jaxa.jp/gpr/.
2.2. MODIS Land Surface Products
Three MODIS land surface products were used to parameterize MW emissivity, including
MODIS/Aqua Snow Cover Daily L3 (MYD10C1), MODIS /Terra+Aqua Land Cover Type Yearly
L3 (MCD12C1), and MODIS /Aqua Vegetation Indices 16-Day L3 (MYD13C1). Spatially averaged
MYD11C1 LST was used as the target LST of the samples. All products were in version 6. MODIS LST
is currently the most accurate and widely used TIR satellite LST product and has been used as the
target LST to establish the MW LST retrieval model [12,17,20,34] and to retrieve MW LSE [28,36,37].
Validation results over homogeneous surfaces showed that MODIS LST products generally have higher
accuracies than 1.0 K [ 38,39]. The aforementioned MYD**C1 products have a spatial resolution of
0.05â—¦and were derived from Aqua observations. Thus, these products achieved better spatial and
temporal matching with the AMSR-E and AMSR2 pixels. The snow cover (SC, from MYD10C1),
Remote Sens.2020, 12, 2691 4 of 27
NDVI (from MYD13C1), land cover type percent (LCTP , from MCD12C1, i.e., the percent cover of
17 IGBP classes at each 0.05â—¦pixel), and LST (from MYD11C1) were upscaled to the spatial resolution of
AMSR-E and AMSR2 by spatial averaging. In addition, the latest MODIS LST and emissivity product
MYD21A1 with a spatial resolution of 1 km was used to (i) derive the MODIS channel emissivities
for the calculation of the broadband emissivity (BBE) of the ground stations and (ii) to quantify the
thermal heterogeneity within the MW pixels. These MODIS land products were downloaded from
EARTHDATA (https://search.earthdata.nasa.gov).
2.3. Reanalysis and Assimilation Datasets
To quantify the atmospheric conditions for the corresponding MW observations, we collected
the 2-meter air temperature (AT-2m) and total precipitable water vapor (TPWV) from the second
Modern-Era Retrospective analysis for Research and Applications (MERRA-2) from https://disc.gsfc.
nasa.gov. In addition, the surface skin temperature was also extracted to synchronize the Aqua MODIS
with AMSR-2 observations. The dataset used in this study was tavg1_2d_slv_Nx, which has a grid size
of 0.5â—¦Ã—0.625â—¦and temporal resolution of 1 hour from 00:30 UTC.
The soil moisture (SM) values in four layers (i.e., 0â€“10 cm, 10â€“40 cm, 40â€“100 cm, and 100â€“200 cm)
were extracted from the Global Land Data Assimilation System (GLDAS) NOAH025_3H (in version
2.1) with a grid size of 0.25â—¦Ã—0.25â—¦and temporal resolution of 3 hours. The GLDAS dataset was also
downloaded from https://disc.gsfc.nasa.gov. The 1-hourly MERRA-2 product and 3-hourly GLDAS
product were interpolated to the overpass time of the MW sensors and the spatial resolution of the
MW BT products.
2.4. In-Situ Measurements
The in-situ measurements of ï¬ve ground stations (CBS, TYU, DXI, SDQ, and HMO) with ground
longwave radiation were collected to validate the LST estimates. Details about the ï¬ve stations
are shown in Table 1. Among these ï¬ve stations, the measurements of CBS were from ChinaFlux
(http://www.chinaï¬‚ux.org/); the measurements of TYU were from the CEOP Asia-Australia Monsoon
Project (CAMP) [40]; the measurements of DXI were from the Haihe experiments in the Hai River
Basin, China [41]; and the measurements of SDQ and HMO were from the Heihe Watershed Allied
Telemetry Experimental Research (HiWATER) program [42â€“44]. The elevations of these stations range
from 20 m to 1050 m. The instruments for measuring the outgoing and incoming longwave radiation
are the Kipp and Zonen CNR1 (spectral range: 5-50 Âµm; uncertainty in daily total: 10%) at CBS, DXI,
and HMO; Kipp and Zonen CG4 pyrgeometer (spectral range: 4.5â€“42 Âµm; uncertainty in daily total:
3%) at TYU; and Kipp and Zonen CNR4 (spectral range: 4.5â€“42 Âµm; uncertainty in daily total: <10%)
at SDQ. Although CNR1, CG4, and CNR4 have diï¬€erent uncertainties, the average errors of incoming
and outgoing longwave radiation are ~6 and 3 W/m2 [45,46], respectively, leading to an in-situ LST
uncertainty of ~0.6 K [3]. According to Stefanâ€“Boltzmannâ€™s law, in-situ LST can be calculated from
the outgoing and incoming longwave radiation, and the BBE was calculated from the emissivities
of MODIS channels 29, 31, and 32 [ 47]. Liang et al. [ 47] stated a theoretical residual standard error
of 0.0289 for the BBE, leading to in-situ LST uncertainties of ~0.7 K for CBS, TYU, DXI, and SDQ,
and ~0.9 K for HMO. The in-situ LST total uncertainties caused by measurements and BBE are~0.9 K
for CBS, TYU, DXI, and SDQ and ~1.1 K for HMO. Additionally, the 3Ïƒ(standard deviations, STDs)
criterion was used to ï¬lter the matched-up CNN LST and in-situ LST and remove the outliers [45,48,49].
Remote Sens.2020, 12, 2691 5 of 27
Table 1. Details of the selected ï¬ve ground stations.
Station Longitude Latitude
Instrument
Surface Type
at Station IGBP Class Percentin MW Pixel * Period of Measurement
Interval of
Measurement (min)Model Elevation
(m)
Height
(m)
Diameter of FOV
(m)
CBS 128.10 â—¦E 42.40 â—¦N Kipp & Zonen
CNR1 736 6 44.78
Deciduous
broadleaf
forest
Deciduous broadleaï¬€orest: 62.1%
Mixed Forests: 29.3%
Woody Savannas: 4.4%
Grasslands: 0.8%
Croplands: 1.1%
Urban and Built-up Lands: 2.3%
January 2003â€“December 2005 30
TYU 122.87 â—¦E 44.42 â—¦N Kipp & Zonen
CG4 184 3 22.39 Cropland Grasslands: 91.6%
Croplands: 8.4% January 2003â€“December 2004 30
DXI 116.43 â—¦E 39.62 â—¦N Kipp & Zonen
CNR1 20 28 208.99 Cropland
Grasslands: 0.8%
Croplands: 67.9%
Urban and Built-up Lands: 31.3%
January 2009â€“December 2010 10
SDQ 101.14 â—¦E 42.00 â—¦N Kipp & Zonen
CNR4 873 10 74.63 Tamarix Grasslands: 66.8%
Barren: 33.2% January 2015â€“December 2016 10
HMO 100.99 â—¦E 42.11 â—¦N Kipp & Zonen
CNR1 1054 6 44.78 Desert Barren: 100% May 2015â€“December 2016 10
Note: * AMSR-E pixel for CBS, TYU, and DXI; AMSR2 pixel for SDQ and HMO.
Remote Sens.2020, 12, 2691 6 of 27
As shown in Table 1, the mounting heights of these instruments vary from 3 m to 28 m above
the ground surface, resulting in ï¬eld-of-view (FOV) diameters ranging from 22.39 m to 208.99 m.
However, the FOV is very small when compared with the spatial resolutions of AMSR-E and AMSR2.
To assess the homogeneities of the MW pixels containing the stations, the percentage of the IGBP
classes within the MW pixels were determined (Table 1). Furthermore, the LST diï¬€erences between
the MW pixels (reprojected by spatially averaging from MYD21A1 LST) and the 1-km MODIS pixels
containing the ground stations were determined to evaluate thermal heterogeneities within the MW
pixels (Figure 1). The statistical results were based on the MYD21A1 LST data of the time period of
the in-situ measurements. For the daytime conditions, the LST of the MW spatial resolution has an
underestimation for CBS, overestimation for DXI and SDQ, and no obvious systemic bias for TYU and
HMO when compared with the 1-km MODIS LST. The underestimation of CBS may be induced by the
elevation distribution, and the overestimations of DXI and SDQ are induced by over 30% of built-up
surfaces and barren within the MW pixels, respectively. As for the nighttime conditions, only CBS has
an underestimation induced by the elevation distribution. In addition, larger STDs can be seen during
the daytime.
Remote Sens. 2019, 11, x FOR PEER REVIEW 5 of 28 
 
uncertainty of ~0.6 K [3]. According to Stefanâ€“Boltzmannâ€™s law, in-situ LST can be calculated 
from the outgoing and incoming longwave ra diation, and the BBE wa s calculated from the 
emissivities of MODIS channels 29, 31, and 32 [47]. Liang et al. [47] stated a theoretical residual 
standard error of 0.0289 for the BBE, leading to in-situ LST uncertainties of ~0.7 K for CBS, TYU, 
DXI, and SDQ, and ~0.9 K for HMO. The in-situ LST total uncertainties caused by 
measurements and BBE are~0.9 K for CBS, TYU, DXI, and SDQ and ~1.1 K for HMO. 
Additionally, the 3 Ïƒ (standard deviations, STDs) criterion was used to filter the matched-up 
CNN LST and in-situ LST and remove the outliers [45,48,49]. 
As shown in Table 1, the mounting heights of these instruments vary from 3 m to 28 m 
above the ground surface, resulting in field-of-view (FOV) diameters ranging from 22.39 m to 
208.99 m. However, the FOV is very small when compared with the spatial resolutions of 
AMSR-E and AMSR2. To assess the homogeneities of the MW pixels containing the stations, 
the percentage of the IGBP classes within the MW pixels were determined (Table 1). 
Furthermore, the LST differences between the MW pixels (reprojected by spatially averaging 
from MYD21A1 LST) and the 1-km MODIS pixels containing the ground stations were 
determined to evaluate thermal heterogeneities within the MW pixels (Figure 1). The statistical 
results were based on the MYD21A1 LST data of the time period of the in-situ measurements. 
For the daytime conditions, the LST of the MW spatial resolution has an underestimation for 
CBS, overestimation for DXI and SDQ, and no obvious systemic bias for TYU and HMO when 
compared with the 1-km MODIS LST. The underestimation of CBS may be induced by the 
elevation distribution, and the overestimations of DXI and SDQ are induced by over 30% of 
built-up surfaces and barren within the MW pixels, respectively. As for the nighttime 
conditions, only CBS has an underestimation induced by the elevation distribution. In addition, 
larger STDs can be seen during the daytime. 
 
Figure 1. The LST differences between the MW pixels and the 1-km MODIS pixels containing 
the ground stations for the daytime ( a) and nighttime (b). For CBS, TYU, and DXI, the spatial 
resolutions of the MW pixels are 0.25Â°; for SD Q and HMO the spatial resolutions of the MW 
pixels are 0.1Â°. The symbol and error bar re present the mean value and STD of the LST 
difference, respectively. 
Figure 1. The LST diï¬€erences between the MW pixels and the 1-km MODIS pixels containing the
ground stations for the daytime (a) and nighttime (b). For CBS, TYU, and DXI, the spatial resolutions
of the MW pixels are 0.25 â—¦; for SDQ and HMO the spatial resolutions of the MW pixels are 0.1 â—¦.
The symbol and error bar represent the mean value and STD of the LST diï¬€erence, respectively.
2.5. Other Datasets
In addition to the aforementioned datasets, the ESAâ€™s GlobTemperature AMSR-E LST
(14 Ã—8 km) [ 26,27] and the NASAâ€™s global land parameter data record (LPDR) air temperatures
(ATs) (25 km) [50â€“53] are used for intercomparison. DEM data with a spatial resolution of 90 m were
downloaded from the Geospatial Data Cloud (http: //www.gscloud.cn/). The DEM data were used
to ï¬lter the valid samples rather than directly as input parameters. More details can be found in
Section 3.3.
3. Methodology
3.1. Neural Networks
Three diï¬€erent neural networks, i.e., NN, DBN, and CNN, were selected for assessments of their
performances in the estimation of LST from the aforementioned AMSR-E and AMSR2 observations.
Their architectures are shown in Figure 2. NN consists of three layers, including an input layer, hidden
layer, and output layer (see Figure 2a). In this study, NN was designed with 2 hidden layers, and the
Remote Sens.2020, 12, 2691 7 of 27
numbers of hidden neurons were set as 64 and 48, respectively. Discussions on this point can be found
in Section 5. The output of the hidden layer h and the output layer Ë†y can be written as follows:
h = Ïƒ(W1x + b1)
Ë†y = W2h + b2
(1)
where x is the input vector; W1 is the weight matrix between the input vector and output vector of
the hidden layer; b1 is the bias matrix of the hidden layer; h is the output vector of the hidden layer;
W2 is the weight matrix between the hidden layer and the output layer; b2 is the bias matrix of output
layer; and Ïƒis the activation function. The activation function is used to build a nonlinear model,
and the activation function used in this study was the rectiï¬ed linear unit (ReLU), which has the
following formula:
f (x) = max(x, 0) (2)
Remote Sens. 2019, 11, x FOR PEER REVIEW 8 of 28 
 
the final output. In this study, the number of  RBMs was 1 and the number of hidden units in 
the RBM was set as 112 (see discussion). 
For CNN, the neurons between two adjacent layers are partly connected (see Figure 2c), 
which is different from NN. A CNN consists of five parts, including an input layer, 
convolutional layer, pooling layer, fully connected layer, and output layer. Convolutional layer 
is used to extract local features, and the pooling layer is used to reduce the number of 
parameters in the network. However, the pooling layer has little effect on the accuracy of CNN 
[54]. The fully connected layer is used to integrate the local features extracted by the previous 
convolution layer. In this study, CNN was designed with one convolutional layer and one fully 
connected layer. The kernel size was set as 1 Ã— 7, the number of convolution kernels was set as 
96, and the number of neurons in the fully conne cted layer was set as 96 (see discussion). The 
output of the convolutional layer y
l and the output layer Å· can be written as follows:  
()
()()
21 1 2Ë†
ll l l
l
yW x b
yW W y b b
Ïƒ
Ïƒ
=+
=+ +
  (4) 
where Wl is the convolution kernel weight; xl is the input of the convolutional layer; bl is the 
bias of the convolutional layer; W1 and W2 are the weights of the fully connected layer and 
output layer, respectively; b1 and b2 are the biases of the fully connected layer and output layer, 
respectively; and Ïƒ is the activation function. 
 
Figure 2. The architectures of NN (a), DBN (b), and CNN (c). 
3.2. Determination of Inputs for the Networks 
The MW brightness temperatures measured by satellite sensors can be written as follows 
[20]:  
Figure 2. The architectures of NN (a), DBN (b), and CNN (c).
DBN (see Figure 2b) is a generative model that generates training data according to the maximum
probability by training the weights between its neurons. DBN is a stack of layers of restricted Boltzmann
machine (RBM), which has only two layers of neurons, including the visible layer and hidden layer [29].
The characteristics of RBM are as follows: given the state of the visible units, the activation conditions of
each hidden unit are independent; when determining the state of the hidden layer units, the activation
conditions of the visible layer units are also independent. The signiï¬cance of training an RBM is
adjusting the parameters of the model to ï¬t the given input data and ultimately making the probability
distribution of the visible units consistent with the input data. In an RBM, the activation probabilities
of hidden unit hj and visible unit vi can be written as follows:
P
(
hj|v
)
= Ïƒ
(
bj + âˆ‘
i Wi,jxi
)
P
(
vj|h
)
= Ïƒ
(
ci + âˆ‘
j Wi,jhj
) (3)
Remote Sens.2020, 12, 2691 8 of 27
where x is the input data of RBM; W is the RBM weight matrix; b is the bias vector for the visible units;
c is the bias vector for the hidden units; and Ïƒ is the activation function.
In contrast with NN, DBN uses a layer-by-layer training method to update the network parameters.
The ï¬rst RBM is trained, and then the output of the previous RBM is used as the input of the next RBM.
After the training of the RBMs, a fully connected layer is set to obtain the ï¬nal output. In this study,
the number of RBMs was 1 and the number of hidden units in the RBM was set as 112 (see discussion).
For CNN, the neurons between two adjacent layers are partly connected (see Figure 2c), which is
diï¬€erent from NN. A CNN consists of ï¬ve parts, including an input layer, convolutional layer, pooling
layer, fully connected layer, and output layer. Convolutional layer is used to extract local features, and
the pooling layer is used to reduce the number of parameters in the network. However, the pooling
layer has little eï¬€ect on the accuracy of CNN [54]. The fully connected layer is used to integrate the
local features extracted by the previous convolution layer. In this study, CNN was designed with one
convolutional layer and one fully connected layer. The kernel size was set as 1 Ã—7, the number of
convolution kernels was set as 96, and the number of neurons in the fully connected layer was set as
96 (see discussion). The output of the convolutional layer yl and the output layer Ë†y can be written as
follows:
yl = Ïƒ
(
Wlxl + bl
)
Ë†y = W2
(
Ïƒ
(
W1 yl + b1
))
+ b2 (4)
where Wl is the convolution kernel weight; xl is the input of the convolutional layer; bl is the bias of
the convolutional layer; W1 and W2 are the weights of the fully connected layer and output layer,
respectively; b1 and b2 are the biases of the fully connected layer and output layer, respectively; and Ïƒ
is the activation function.
3.2. Determination of Inputs for the Networks
The MW brightness temperatures measured by satellite sensors can be written as follows [20]:
Tp = Ï„ÎµpTs +
(
1 âˆ’Îµp
)
Ï„Tâ†“
a + Tâ†‘
a (5)
where the subscript p is the polarization ( p âˆˆ{v, h}); Tp, Ts, Tâ†‘
a , and Tâ†“
a are the MW brightness
temperature, LST, atmospheric upwelling brightness temperature, and atmospheric downwelling
brightness temperature in K, respectively; Ï„is the atmospheric transmissivity; and Îµp is the MW land
surface emissivity (LSE).
Equation (5) demonstrates that LSE and three atmospheric parameters are important factors in the
estimation of the MW LST. However, these four parameters are diï¬ƒcult to obtain directly. Fortunately,
the LSE of MW is closely related to soil moisture, snow, and vegetation [23,55]. Therefore, here, we used
surface parameters including NDVI, SC, LCTP , and SM to implicitly present the LSE in the subsequent
networks. In addition, atmospheric parameters rely on some easily available meteorological variables,
such as the TPWV [20]. Thus, we used the TPWV and AT-2m to implicitly parameterize Tâ†‘
a , Tâ†“
a , and Ï„.
The reason for including AT-2m is that it is better related to atmospheric radiances in conditions with
high air temperature but low atmospheric humidity [38].
Soil temperatures at di ï¬€erent depths are important parameters for solving the problem of
TSD [23,28,56,57]. Currently, soil temperatures mainly come from reanalysis and assimilation datasets.
For GLDAS, the ï¬rst layer depth of soil temperature is 0â€“10 cm, which means that the soil temperature
of the ï¬rst layer may be very close to the LST. The accuracy of the LST retrieval model may greatly
depend on the accuracy of the soil temperature, and microwave BTs will have small contributions
when the soil temperature is used as the input parameter. Considering that MW BT products are
more reliable than soil temperature products from reanalysis or assimilation datasets, we did not take
soil temperatures as input parameters. In addition to the aforementioned parameters, DOY is used
to characterize the changes in LST at the annual scale. Finally, the input parameter set includes BTs
(all frequencies and all polarizations), NDVI, SC, LCTP , SM, AT-2m, TPWV , and DOY.
Remote Sens.2020, 12, 2691 9 of 27
3.3. Extraction of the Samples
The overpass time of AMSR2 is not the same as that of Aqua MODIS. This di ï¬€erence in the
overpass time prevents the MYD11C1 LST from representing the LST at the overpass time of AMSR2.
The daytime and nighttime LST at the overpass time of AMSR2 and Aqua MODIS is close to the
maximum and minimum LST of the day, respectively. Furthermore, we found that over 90% of the
overpass time diï¬€erences between Aqua MODIS and AMSR2 over China are less than 20 minutes.
Therefore, it is reasonable to assume that MERRA-2 skin temperature changes linearly with time
within the hour that is nearest to the overpass time of AMSR2. First, the MERRA-2 surface skin
temperatures were interpolated over time to obtain the skin temperature at the overpass times of
MODIS (TMERRA-2(tMODIS)) and AMSR2 (TMERRA-2(tAMSR2)). Then, the diï¬€erence âˆ†T(tMODIS) between
TMODIS(tMODIS) and TMERRA-2(tMODIS) at the overpass time of MODIS can be obtained by:
âˆ†T(tMODIS) = TMODIS(tMODIS) âˆ’TMERRAâˆ’2(tMODIS) (6)
Assuming that âˆ†T(tAMSR2) is equal toâˆ†T(tMODIS), TMODIS(tAMSR2) (the LST at the AMSR2 overpass
time) can be obtained by:
TMODIS(tAMSR2) = TMERRAâˆ’2(tAMSR2) + âˆ†T(tAMSR2) (7)
The accuracy of neural networks relies on the validity of the training samples. To ensure the
validity of the samples, we used two criteria to ï¬lter the valid samples. The ï¬rst criterion is that the
pixel quality is ï¬‚agged as â€œgoodâ€ (LST error < 1 K) and the view zenith angle (VZA) is less than 40â—¦
for all MYD11C1 pixels within the corresponding MW pixel. The second criterion is that the STD of
the elevation in MW pixel is less than the threshold (i.e., 100 m for AMSR-E with a spatial resolution
of 0.25â—¦and 10 m for AMSR2 with a spatial resolution of 0.1 â—¦). Compared with AMSR-E, AMSR2
was ï¬ltered more strictly because it has a higher resolution (which means more samples) and lower
training sample accuracy (aï¬€ected by synchronization of MODIS and AMSR2).
The extracted samples were categorized into 4 groups, including daytime AMSR-E (Group
I), nighttime AMSR-E (Group II), daytime AMSR2 (Group III), and nighttime AMSR2 (Group IV).
Each group contains a training set, a validation set, and a test set. For AMSR-E, the training set,
validation set, and test set were the samples from 2003, 2004, and 2009, respectively; for AMSR2,
the training set, validation set, and test set were the samples from 2013, 2014, and 2016, respectively.
Details for the extracted four groups of samples are shown in Table 2.
Table 2. Details for the extracted four groups of samples.
Group Sensor Condition Sample Size
Training Set Validation Set Test Set
Group I AMSR-E Daytime 399,351 450,171 420,655
Group II AMSR-E Nighttime 170,144 262,803 315,708
Group III AMSR2 Daytime 1,057,257 589,205 832,588
Group IV AMSR2 Nighttime 570,848 400,695 448,753
Note: Sample size represents the number of valid pixels.
3.4. Implementation of the Networks
In this study, the three networks performed based on Tensorï¬‚ow using a GPU with a computation
capability of 5.0. Figure 3 shows the ï¬‚owchart of this study. The process can be divided into three
stages. Stage I was to examine the performances of NN, DBN, and CNN and to determine the best
performing neural network. The accuracy indices in the comparison included the mean bias deviation
(MBD), root-mean-square di ï¬€erences (RMSD), and coe ï¬ƒcient of determination ( R2) based on the
training set, validation set, and test set. For neural networks, more relevant input parameters may
Remote Sens.2020, 12, 2691 10 of 27
lead to better accuracy. However, some parameters may have small contributions and are diï¬ƒcult to
obtain directly. Thus, in stage II, diï¬€erent input parameter combinations were examined to get the best
combination for the daytime and nighttime. In stage III, the LST estimated from the MW observations
with the determined best network and the best combination were validated based on the in-situ LST.
Additionally, the latest MYD21A1 LST product, ESAâ€™s GlobTemperature AMSR-E LST, and NASAâ€™s
LPDR ATs were also investigated for intercomparison purposes. Note that the criteria for ï¬ltering
valid pixels are only applicable to the extraction of samples with the purpose of obtaining an accurate
BT-LST conversion relationship. During the comparison in stage III, CNN LST estimates were from all
available samples. Before intercomparison, GT LST, LPDR ATs, and MODIS LST were reprojected to
the MW spatial resolutions (namely, 0.25â—¦for AMSR-E and 0.1â—¦for AMSR2).
Remote Sens. 2019, 11, x FOR PEER REVIEW 11 of 28 
 
 
Figure 3. The flow chart of the proposed method. 
4. Results 
4.1. Performances of the Different Neural Networks 
The statistical results of the three networks on the training set, validation set, and test set 
are shown in Table 3. Table 3 shows that the LS Ts estimated by the three networks have no 
obvious systematic deviation when compared to the MYD11C1 LST. The differences between 
the validation RMSDs and test RMSDs were lower than 0.30 K (lower than 0.10 K for Groups I, 
II, and IV), indicating the stability of the three network models. However, CNN had the lowest 
RMSDs: the RMSDs on the validation set and test set for CNN were lower by ~0.1 K and ~0.3 
K than NN and DBN, respectively, for the four groups. In addition, we found that the 
accuracies of NN and CNN were extremely close.  Therefore, a statistical test was used to 
evaluate the differences between the NN esti mates and CNN estimates. Here, equivalence 
testing was chosen to reduce the risk of Type I inferential error [58,59]. The null hypothesis H
0 
was |T1 âˆ’ T2| â‰¥ Î”T, and the alternative hypothesis H1 was |T1 âˆ’ T2| < Î”T. Note that the H0 would 
be rejected when both one-sided components would be rejected [58]. In this study, Î”T was set 
as 0.1 K and the test results are shown in Table 4. It can be seen that the CNN estimates were 
different (null hypothesis is accepted) from  the NN estimates for both AMSR-E and AMSR2 
during the daytime. In contrast, they were eq uivalent (null hypothesis is rejected) on the 
validation and test sets for AMSR-E and the training and validation sets for AMSR2 during the 
nighttime.  
 
Figure 3. The ï¬‚ow chart of the proposed method.
4. Results
4.1. Performances of the Diï¬€erent Neural Networks
The statistical results of the three networks on the training set, validation set, and test set are shown
in Table 3. Table 3 shows that the LSTs estimated by the three networks have no obvious systematic
deviation when compared to the MYD11C1 LST. The diï¬€erences between the validation RMSDs and
test RMSDs were lower than 0.30 K (lower than 0.10 K for Groups I, II, and IV), indicating the stability
of the three network models. However, CNN had the lowest RMSDs: the RMSDs on the validation set
and test set for CNN were lower by ~0.1 K and ~0.3 K than NN and DBN, respectively, for the four
groups. In addition, we found that the accuracies of NN and CNN were extremely close. Therefore,
a statistical test was used to evaluate the diï¬€erences between the NN estimates and CNN estimates.
Here, equivalence testing was chosen to reduce the risk of Type I inferential error [58,59]. The null
hypothesis H0 was |T1 âˆ’T2| â‰¥âˆ†T, and the alternative hypothesis H1 was |T1 âˆ’T2| < âˆ†T. Note that the
H0 would be rejected when both one-sided components would be rejected [58]. In this study, âˆ†T was
set as 0.1 K and the test results are shown in Table 4. It can be seen that the CNN estimates were
diï¬€erent (null hypothesis is accepted) from the NN estimates for both AMSR-E and AMSR2 during the
Remote Sens.2020, 12, 2691 11 of 27
daytime. In contrast, they were equivalent (null hypothesis is rejected) on the validation and test sets
for AMSR-E and the training and validation sets for AMSR2 during the nighttime.
Then, NN, DBN, and CNN were further compared for diï¬€erent NDVI-based land cover types
and diï¬€erent seasons. We classiï¬ed the microwave pixels of the test sets into three types based on
NDVI [3]: (i) barren land (NDVI < 0.2); (ii) sparsely vegetated (0.2 â‰¤NDVI â‰¤0.5); and (iii) densely
vegetated (NDVI > 0.5). For the convenience of statistics, the spring represents March, April, and May;
the summer represents June, July, and August; the autumn represents September, October, and
November; and the winter represents December, January, and February. The accuracies of the three
networks for diï¬€erent NDVI-based land cover types and diï¬€erent seasons are shown in Figures 4 and 5,
respectively. It can be seen that CNN had the lowest RMSDs in almost all conditions and the RMSDs
of NN and DBN were higher than those of CNN by 0.1 Kâ€“0.5 K.
Remote Sens. 2019, 11, x FOR PEER REVIEW 12 of 28 
Remote Sens. 2019, 11, x; doi: FOR PEER REVIEW www. mdpi.com/journal/remotesensing 
Then, NN, DBN, and CNN were further compar ed for different NDVI-based land cover 
types and different seasons. We classified the micr owave pixels of the test sets into three types 
based on NDVI [3]: (i) barren land (NDVI < 0.2); (ii) sparsely vegetated (0.2 â‰¤ NDVI â‰¤ 0.5); and (iii) 
densely vegetated (NDVI > 0.5). For the convenienc e of statistics, the spring represents March, 
April, and May; the summer represents June, July, and August; the autumn represents September, 
October, and November; and the winter repres ents December, January, and February. The 
accuracies of the three networks for different NDVI-based land cover types and different seasons 
are shown in Figures 4 and 5, respectively. It can be seen that CNN had the lowest RMSDs in 
almost all conditions and the RMSDs of NN and DBN were higher than those of CNN by 0.1 Kâ€“
0.5 K.  
 
Figure 4. Histograms of the differences between the LSTs estimated from neural networks and 
the MYD11C1 LSTs on the test sets for NN (a,d,g,j), DBN (b,e,h,k), and CNN (c,f,i,l). 
Figure 4. Histograms of the diï¬€erences between the LSTs estimated from neural networks and the
MYD11C1 LSTs on the test sets for NN (a,d,g,j), DBN (b,e,h,k), and CNN (c,f,i,l).
Remote Sens.2020, 12, 2691 12 of 27
Table 3. The performances of NN, DBN, and CNN on the training sets, validation sets, and test sets.
Group Set NN DBN CNN
MBD (K) RMSD (K) R2 MBD (K) RMSD (K) R2 MBD (K) RMSD (K) R2
Group I (Daytime AMSR-E)
Training âˆ’0.04 2.77 0.97 âˆ’0.06 3.28 0.96 0.04 2.50 0.98
Validation âˆ’0.03 2.99 0.97 âˆ’0.03 3.34 0.96 0.08 2.88 0.97
Test 0.04 3.11 0.97 âˆ’0.07 3.38 0.96 0.13 3.00 0.97
Group II (Nighttime AMSR-E)
Training âˆ’0.09 1.56 0.98 âˆ’0.05 2.03 0.96 âˆ’0.01 1.32 0.98
Validation 0.08 1.81 0.97 0.01 2.10 0.96 0.12 1.66 0.98
Test 0.13 1.83 0.97 0.03 2.20 0.96 0.19 1.74 0.97
Group III (Daytime AMSR2)
Training 0.12 3.12 0.97 âˆ’0.03 3.46 0.96 0.01 2.90 0.97
Validation âˆ’0.03 3.32 0.96 âˆ’0.14 3.55 0.95 âˆ’0.12 3.22 0.96
Test 0.05 3.62 0.96 âˆ’0.21 3.83 0.95 âˆ’0.08 3.48 0.96
Group IV (Nighttime AMSR2)
Training âˆ’0.06 1.85 0.98 0.01 2.12 0.98 âˆ’0.07 1.70 0.98
Validation 0.23 2.12 0.97 0.37 2.34 0.96 0.22 2.02 0.97
Test âˆ’0.13 2.19 0.97 âˆ’0.01 2.38 0.96 âˆ’0.06 2.10 0.97
Table 4. The probability values (p-values) of the equivalence testing for the NN LST and the CNN LST on the training sets, validation sets, and test sets. Note that the
H0 would be rejected when both one-sided components would be rejected. The p-values of the sets on which the CNN estimates and the NN estimates are equivalent
are highlighted by underline.
Set Group I Group II Group III Group IV
T1 âˆ’T2 â‰¤âˆ’0.1 K T1 âˆ’T2 â‰¥0.1 K T1 âˆ’T2 â‰¤âˆ’0.1 K T1 âˆ’T2 â‰¥0.1 K T1 âˆ’T2 â‰¤âˆ’0.1 K T1 âˆ’T2 â‰¥0.1 K T1 âˆ’T2 â‰¤âˆ’0.1 K T1 âˆ’T2 â‰¥0.1 K
Training set 0.35 <0.01 0.31 <0.01 <0.01 0.86 <0.01 <0.01
Validation
set 0.58 <0.01 0.03 <0.01 <0.01 0.41 <0.01 <0.01
Test set 0.35 <0.01 0.04 <0.01 <0.01 0.90 0.16 <0.01
Remote Sens.2020, 12, 2691 13 of 27
Remote Sens. 2019, 11, x FOR PEER REVIEW 13 of 28 
 
 
Figure 5. Seasonal statistics of the difference between the LSTs estimated from neural networks 
and the MYD11C1 LSTs on the test sets for NN (a), DBN (b), and CNN (c). The bars are centered 
in the centroid of the symbol. The centroid of  the symbol represents the mean value of the 
difference. The bar represents the STD of the di fference. In addition, the RMSDs of the three 
networks compared to MYD11C1 LST are also annotated in the figure. 
From the previous analysis, it is clear that CNN outperforms NN and DBN. The training 
times of NN, DBN, and CNN were related to the number of training samples and were 6, 8, and 
12 min for Group I, respectively. Although C NN had the highest time cost among these three 
networks, it was still acceptab le. Therefore, only CNN was employed in further analysis, 
including the analysis of input parameters, validation based on in-situ LST, and intercomparison 
with other products. 
4.2. Determination of the Best Input Parameter Combinations 
Considering that BTs of all frequencies are acquired at the same time, we only examined the 
following input parameters: NDVI, SC, LCTP, SM, TPWV, AT-2m, and DOY. Due to space 
limitations, only some representative input parameter combinations and their total numbers of 
parameters are listed in Table 5. In these combinations, combination C0 contains all parameters 
in the input parameter set; C1â€“C4 were used for examining the surface parameters; C5â€“C7 were 
used for examining AT-2m and TPWV; C8 is for DOY; and C9-C14 were designed based on the 
results from C1â€“C8 to determine the best combinations for the daytime and nighttime conditions. 
The RMSD differences ( Î”RMSDs) between combination C1â€“C14 and C0 for the validation sets 
and test sets are shown in Figure 6. The best input parameter combinations were selected based 
on (i) Î”RMSD less than 0.3 K; (ii) fewer input parameters. 
  
Figure 5. Seasonal statistics of the diï¬€erence between the LSTs estimated from neural networks and
the MYD11C1 LSTs on the test sets for NN (a), DBN (b), and CNN (c). The bars are centered in the
centroid of the symbol. The centroid of the symbol represents the mean value of the diï¬€erence. The bar
represents the STD of the di ï¬€erence. In addition, the RMSDs of the three networks compared to
MYD11C1 LST are also annotated in the ï¬gure.
From the previous analysis, it is clear that CNN outperforms NN and DBN. The training times of
NN, DBN, and CNN were related to the number of training samples and were 6, 8, and 12 min for
Group I, respectively. Although CNN had the highest time cost among these three networks, it was
still acceptable. Therefore, only CNN was employed in further analysis, including the analysis of input
parameters, validation based on in-situ LST, and intercomparison with other products.
4.2. Determination of the Best Input Parameter Combinations
Considering that BTs of all frequencies are acquired at the same time, we only examined the
following input parameters: NDVI, SC, LCTP , SM, TPWV , AT-2m, and DOY. Due to space limitations,
only some representative input parameter combinations and their total numbers of parameters are
listed in Table 5. In these combinations, combination C0 contains all parameters in the input parameter
set; C1â€“C4 were used for examining the surface parameters; C5â€“C7 were used for examining AT-2m
and TPWV; C8 is for DOY; and C9-C14 were designed based on the results from C1â€“C8 to determine
the best combinations for the daytime and nighttime conditions. The RMSD di ï¬€erences (âˆ†RMSDs)
between combination C1â€“C14 and C0 for the validation sets and test sets are shown in Figure 6.
The best input parameter combinations were selected based on (i) âˆ†RMSD less than 0.3 K; (ii) fewer
input parameters.
Remote Sens.2020, 12, 2691 14 of 27
Table 5. Some of the input parameter combinations and the corresponding number of parameters.âˆšrepresents the parameter contained in the input parameters, and Ã—is the opposite. The numbers
outside (within) the brackets represent the total number of input parameters for AMSR-E (AMSR2).
CombinationBTs Surface Parameters Atmospheric Related Parameters DOY Number of Parameters
NDVI LCTP SM* and SC AT-2m TPWV
C0 âˆš âˆš âˆš âˆš âˆš âˆš âˆš 38 (40)
C1 âˆš âˆš âˆš Ã— âˆš âˆš âˆš 33 (35)
C2 âˆš âˆš Ã— Ã— âˆš âˆš âˆš 16 (18)
C3 âˆš Ã— âˆš Ã— âˆš âˆš âˆš 32 (34)
C4 âˆš Ã— Ã— Ã— âˆš âˆš âˆš 15 (17)
C5 âˆš âˆš âˆš âˆš âˆš Ã— âˆš 37 (39)
C6 âˆš âˆš âˆš âˆš Ã— âˆš âˆš 37 (39)
C7 âˆš âˆš âˆš âˆš Ã— Ã— âˆš 36 (38)
C8 âˆš âˆš âˆš âˆš âˆš âˆš Ã— 37 (39)
C9 âˆš âˆš âˆš Ã— âˆš Ã— âˆš 32 (34)
C10 âˆš âˆš Ã— Ã— âˆš Ã— âˆš 15 (17)
C11 âˆš Ã— âˆš Ã— âˆš Ã— âˆš 31 (33)
C12 âˆš Ã— Ã— Ã— âˆš Ã— âˆš 14 (16)
C13 âˆš Ã— Ã— Ã— âˆš Ã— Ã— 13 (15)
C14 âˆš Ã— Ã— Ã— Ã— Ã— Ã— 12 (14)
Note: SM* is soil moisture values in four layers. LCTP contains the percent of the 17 IGBP classes.
Remote Sens. 2019, 11, x FOR PEER REVIEW 14 of 28 
 
Table 5. Some of the input parameter combinations and the corresponding number of parameters. 
âˆš represents the parameter contained in the input parameters, and Ã— is the opposite. The numbers 
outside (within) the brackets represent the to tal number of input parameters for AMSR-E 
(AMSR2). 
Combination BTs 
Surface Parameters Atmospheric Related 
Parameters DOY Number of 
Parameters NDVI LCTP SM* 
and SC AT-2m TPWV 
C0 âˆš âˆš âˆš âˆš âˆš âˆš âˆš 38 (40) 
C1 âˆš âˆš âˆš Ã— âˆš âˆš âˆš 33 (35) 
C2 âˆš âˆš Ã— Ã— âˆš âˆš âˆš 16 (18) 
C3 âˆš Ã— âˆš Ã— âˆš âˆš âˆš 32 (34) 
C4 âˆš Ã— Ã— Ã— âˆš âˆš âˆš 15 (17) 
C5 âˆš âˆš âˆš âˆš âˆš Ã— âˆš 37 (39) 
C6 âˆš âˆš âˆš âˆš Ã— âˆš âˆš 37 (39) 
C7 âˆš âˆš âˆš âˆš Ã— Ã— âˆš 36 (38) 
C8 âˆš âˆš âˆš âˆš âˆš âˆš Ã— 37 (39) 
C9 âˆš âˆš âˆš Ã— âˆš Ã— âˆš 32 (34) 
C10 âˆš âˆš Ã— Ã— âˆš Ã— âˆš 15 (17) 
C11 âˆš Ã— âˆš Ã— âˆš Ã— âˆš 31 (33) 
C12 âˆš Ã— Ã— Ã— âˆš Ã— âˆš 14 (16) 
C13 âˆš Ã— Ã— Ã— âˆš Ã— Ã— 13 (15) 
C14 âˆš Ã— Ã— Ã— Ã— Ã— Ã— 12 (14) 
Note: SM* is soil moisture values in four layers. LCTP contains the percent of the 17 IGBP classes. 
 
Figure 6. Î”RMSDs between combination C1â€“C14 and C0 fo r the validation sets and test sets 
during the daytime (a) and nighttime (b). 
For the daytime condition, SM and SC have small contributions to the improvement of model 
accuracy in surface parameters. When the input parameters did not contain SM and SC (i.e., 
combination C1), the Î”RMSDs were below 0.15 K for both the validation sets and the test sets. If 
NDVI and LCTP were not removed at the same time, the increase in Î”RMSD was small. For 
example, on the basis of C1, only LCTP (NDVI)  was removed for C2 (C3), and the increases in 
Î”RMSD were below 0.10 K; however, the increases in Î”RMSD were above 0.25 K when NDVI 
and LCTP were removed at the same time (C4) . AT-2m had a higher impact than TPWV. When 
Figure 6. âˆ†RMSDs between combination C1â€“C14 and C0 for the validation sets and test sets during the
daytime (a) and nighttime (b).
For the daytime condition, SM and SC have small contributions to the improvement of model
accuracy in surface parameters. When the input parameters did not contain SM and SC (i.e., combination
C1), the âˆ†RMSDs were below 0.15 K for both the validation sets and the test sets. If NDVI and LCTP
were not removed at the same time, the increase in âˆ†RMSD was small. For example, on the basis of
C1, only LCTP (NDVI) was removed for C2 (C3), and the increases in âˆ†RMSD were below 0.10 K;
however, the increases in âˆ†RMSD were above 0.25 K when NDVI and LCTP were removed at the same
time (C4). AT-2m had a higher impact than TPWV . When AT-2m was not used as an input parameter
(i.e., combination C6), the âˆ†RMSD was higher than 0.20 K. In addition, DOY (i.e., combination C8)
can help improve the accuracies of the daytime models. In these combinations, âˆ†RMSEs of C10
and C11 were both below 0.30 K. Considering that NDVI is a quantitative parameter and C10 has
Remote Sens.2020, 12, 2691 15 of 27
fewer input parameters, we ï¬nally selected C10 as the best combination of input parameters for the
daytime models.
For the nighttime conditions, SM, SC, and TPWV have small contributions, and AT-2m has a large
contribution. This ï¬nding is similar to that under daytime conditions. However, the nighttime NDVI,
LCTP , and DOY values have fewer contributions than those in the daytime. For example, theâˆ†RMSDs
of C13 (only BTs and AT-2m) were all below 0.30 K. In these combinations, only C14 had fewer
parameters than C13. However, it can be observed that C14 had the RMSDs of 0.5â€“0.8 K. Therefore,
C13 was ï¬nally selected as the best combination of input parameters for the nighttime models.
The statistical results of the best input parameter combinations for diï¬€erent NDVI-based land cover
types and seasons are shown in Figures 7 and 8. For the three NDVI-based land cover types, the highest
accuracies occurred in the densely vegetated pixels and lowest accuracies occurred in the barren land
pixels. During the daytime, the RMSDs of the best combinations were 2.19 K, 2.66 K, and 3.58 K for
densely vegetated pixels, sparsely vegetated pixels, and barren land pixels for AMSR-E, respectively.
The corresponding RMSDs were 1.43 K, 1.73 K, and 2.14 K during the nighttime. For AMSR2, there were
higher RMSDs on all land cover types compared to AMSR-E (~0.6 K and ~0.3 K for the daytime and
nighttime conditions, respectively). The lowest accuracies for the barren land may be concluded by the
diï¬ƒculty in determining the LSE of barren land. Figure 8 shows that the RMSDs of spring and summer
are higher than those of autumn and winter for the daytime conditions. Spring and summer are the
seasons of vegetation growth, increasing the heterogeneity within the pixels, and, thus, the upscaling
method of spatial averaging introduces more uncertainty than other seasons. In contrast, the RMSDs
of all seasons are below 2.20 K and 2.80 K for AMSR-E and AMSR2 at night, respectively.
Remote Sens. 2019, 11, x FOR PEER REVIEW 15 of 28 
 
AT-2m was not used as an input parameter (i.e., combination C6), the Î”RMSD was higher than 
0.20 K. In addition, DOY (i.e., combination C8) can help improve the accuracies of the daytime 
models. In these combinations, Î”RMSEs of C10 and C11 were both below 0.30 K. Considering that 
NDVI is a quantitative parameter and C10 has fewer input parameters, we finally selected C10 as 
the best combination of input parameters for the daytime models.  
For the nighttime conditions, SM, SC, and TPWV  have small contributions, and AT-2m has 
a large contribution. This finding is similar to that under daytime conditions. However, the 
nighttime NDVI, LCTP, and DOY values have fewer contributions than those in the daytime. For 
example, the Î”RMSDs of C13 (only BTs and AT-2m) were all below 0.30 K. In these combinations, 
only C14 had fewer parameters than C13. However, it can be observed that C14 had the RMSDs 
of 0.5â€“0.8 K. Therefore, C13 was finally selected as the best combination of input parameters for 
the nighttime models. 
The statistical results of the best input parameter combinations for different NDVI-based 
land cover types and seasons are shown in Figures 7 and 8. For the three NDVI-based land cover 
types, the highest accuracies oc curred in the densely vegetated pixels and lowest accuracies 
occurred in the barren land pixels. During the daytime, the RMSDs of the best combinations were 
2.19 K, 2.66 K, and 3.58 K for densely vegetated pixels, sparsely vegetated pixels, and barren land 
pixels for AMSR-E, respectively. The correspondi ng RMSDs were 1.43 K,  1.73 K, and 2.14 K 
during the nighttime. For AMSR2, there were higher RMSDs on all land cover types compared to 
AMSR-E (~0.6 K and ~0.3 K for the daytime and nighttime conditions, respectively). The lowest 
accuracies for the barren land may be concluded by the difficulty in determining the LSE of barren 
land. Figure 8 shows that the RMSDs of spring and summer are higher than those of autumn and 
winter for the daytime conditions. Spring and summer are the seasons of vegetation growth, 
increasing the heterogeneity within the pixels, and, thus, the upscaling method of spatial 
averaging introduces more uncertainty than other seasons. In contrast, the RMSDs of all seasons 
are below 2.20 K and 2.80 K for AMSR-E and AMSR2 at night, respectively.  
 
Figure 7. Histograms of the difference between the LSTs estimated from CNN with the best 
combinations of input parameters and the MYD11C1 LSTs on the test sets for AMSR-E (a,b) and 
AMSR2 (c,d). 
Figure 7. Histograms of the diï¬€erence between the LSTs estimated from CNN with the best combinations
of input parameters and the MYD11C1 LSTs on the test sets for AMSR-E (a,b) and AMSR2 (c,d).
Remote Sens.2020, 12, 2691 16 of 27
Remote Sens. 2019, 11, x FOR PEER REVIEW 16 of 28 
 
 
Figure 8. Similar to Figure 5, but only showing the difference between the LSTs estimated from 
CNN with the best combinations of input parameters and the MYD11C1 LSTs on the test sets. 
4.3. Validation Based on In-Situ LST 
Based on the best combinations of input parameters determined in Section 4.2, the CNN LST 
was validated against the in-situ LST. In addi tion, the MODIS LST derived from the MYD21A1 
products was also compared. Since the obtained in-situ measurements were from different years, 
the validations of the retrieved LST from AMSR - E  w e r e  b a s e d  o n  C B S ,  T Y U ,  a n d  D X I ;  t h e  
validations of the retrieved LST from the AMSR 2 observations were based on SDQ and HMO. 
The results are shown in Figures 9 and 10. 
For CBS, the MODIS LST had a slight overesti mation and underestimation in the daytime 
and nighttime values, respectively. This could be  related to the built-up  surfaces around the 
station. Although only ~2% of MW pixels were covered by built-up surfaces, it can be seen from 
Google Earth that the built-up surfaces were concentrated near the station. The CNN LST is close 
to the in-situ LST during the daytime and had an evident underestimation during the nighttime: 
the MBE values were 0.75 K and â€“3.06 K during  the daytime and nighttime, respectively; the 
corresponding RMSE values were 2.10 K and 3.53 K. The underestimations of the nighttime values 
are induced by the higher elevation (the highest elevation in MW pixel is 1210 m) at the south of 
the MW pixel. 
For TYU, the MODIS LSTs and the CNN LSTs had no obvious systemic biases in either the 
daytime or nighttime. The absolute values of the MBEs were all below 0.50 K. The RMSEs of CNN 
LST were below 3.5 K and 3 K for the daytime and nighttime, respectively. Although the AMSR-
E pixel containing the TYU station was not a pure pixel, the main land cover types, i.e., croplands 
and grasslands, had very similar thermal properti es and LSTs. Thus, it is understandable that 
CNN LST was close to the in-situ LST and only a slight RMSD difference existed between the 
CNN LST and the MODIS LST. 
For DXI, an overestimation of the daytime va lues and an underestimation of the nighttime 
values were observed for both the MODIS LST and the CNN LST. This may be induced by the 
built-up surfaces. Table 1 shows that over 30% of the AMSR-E pixel was covered by built-up 
surfaces. The MBE values were 1.95 K and â€“2 .84 K for the CNN LST during the daytime and 
nighttime, respectively. The corresponding RMSE values were 3 K and 3.43 K. 
For SDQ, the CNN LST had an RMSE of 4.72 K (MBE is 3.64 K) for the daytime conditions. 
Over 30% of the AMSR2 pixels were covered by barren, resulting in an overestimation of above 3 
K. For the nighttime values, the MODIS LST had good performance: the MBE and RMSE were â€“
0.56 K and 1.32 K, respectively. In contrast, the corresponding values were 1.84 K and 4.08 K for 
the CNN LST. From Figure 10d, evident overesti mation can be observed when LST is less than 
270 K. Based on the discriminant function algo rithm (DFA) proposed by Wang et al. [60], we 
found that this overestimation is due to the frozen soil. Frozen soil has higher emissivity than 
unfrozen soil. Thus, higher BTs can be observed wh en soil freezing occurs [61], resulting in an 
overestimation of the CNN LST.  
For HMO, both the MODIS LST and the CNN LS T have high systematic overestimation 
(above 4 K) during the daytime and have better accuracy at nighttime (RMSE was 1.39 K for the 
Figure 8. Similar to Figure 5, but only showing the diï¬€erence between the LSTs estimated from CNN
with the best combinations of input parameters and the MYD11C1 LSTs on the test sets.
4.3. Validation Based on In-Situ LST
Based on the best combinations of input parameters determined in Section 4.2, the CNN LST
was validated against the in-situ LST. In addition, the MODIS LST derived from the MYD21A1
products was also compared. Since the obtained in-situ measurements were from di ï¬€erent years,
the validations of the retrieved LST from AMSR-E were based on CBS, TYU, and DXI; the validations
of the retrieved LST from the AMSR2 observations were based on SDQ and HMO. The results are
shown in Figures 9 and 10.
For CBS, the MODIS LST had a slight overestimation and underestimation in the daytime and
nighttime values, respectively. This could be related to the built-up surfaces around the station.
Although only ~2% of MW pixels were covered by built-up surfaces, it can be seen from Google Earth
that the built-up surfaces were concentrated near the station. The CNN LST is close to the in-situ
LST during the daytime and had an evident underestimation during the nighttime: the MBE values
were 0.75 K and â€“3.06 K during the daytime and nighttime, respectively; the corresponding RMSE
values were 2.10 K and 3.53 K. The underestimations of the nighttime values are induced by the higher
elevation (the highest elevation in MW pixel is 1210 m) at the south of the MW pixel.
For TYU, the MODIS LSTs and the CNN LSTs had no obvious systemic biases in either the
daytime or nighttime. The absolute values of the MBEs were all below 0.50 K. The RMSEs of CNN
LST were below 3.5 K and 3 K for the daytime and nighttime, respectively. Although the AMSR-E
pixel containing the TYU station was not a pure pixel, the main land cover types, i.e., croplands and
grasslands, had very similar thermal properties and LSTs. Thus, it is understandable that CNN LST
was close to the in-situ LST and only a slight RMSD diï¬€erence existed between the CNN LST and the
MODIS LST.
For DXI, an overestimation of the daytime values and an underestimation of the nighttime values
were observed for both the MODIS LST and the CNN LST. This may be induced by the built-up
surfaces. Table 1 shows that over 30% of the AMSR-E pixel was covered by built-up surfaces. The MBE
values were 1.95 K and â€“2.84 K for the CNN LST during the daytime and nighttime, respectively.
The corresponding RMSE values were 3 K and 3.43 K.
For SDQ, the CNN LST had an RMSE of 4.72 K (MBE is 3.64 K) for the daytime conditions.
Over 30% of the AMSR2 pixels were covered by barren, resulting in an overestimation of above 3 K.
For the nighttime values, the MODIS LST had good performance: the MBE and RMSE were â€“0.56 K
and 1.32 K, respectively. In contrast, the corresponding values were 1.84 K and 4.08 K for the CNN LST.
From Figure 10d, evident overestimation can be observed when LST is less than 270 K. Based on the
discriminant function algorithm (DFA) proposed by Wang et al. [60], we found that this overestimation
is due to the frozen soil. Frozen soil has higher emissivity than unfrozen soil. Thus, higher BTs can be
observed when soil freezing occurs [61], resulting in an overestimation of the CNN LST.
Remote Sens.2020, 12, 2691 17 of 27
Remote Sens. 2019, 11, x FOR PEER REVIEW 17 of 28 
 
MODIS LST and 2.63 K for the CNN LST). The significant overestimation of the CNN LST during 
the daytime may be induced by the overestimati on of the MODIS LST, which was the basis for 
the training of the CNN models. In addition, an overestimation also occurred when the LST was 
less than 270 K at night, which is similar to that at SDQ.  
The validation results of the CNN LST under clear-sky conditions and cloudy conditions are 
also shown in Figures 9 and 10. It can be seen that the RMSE differences of the clear-sky conditions 
and the cloudy conditions were lower than 0.5 K in most cases. Exceptions occurred during the 
daytime of HMO and the nighttime of SDQ. The reason was the uneven distribution of the clear-
sky and the cloudy samples. However, the CNN LSTs under cloudy conditions do not show 
significant outliers compared with the CNN LSTs under clear-sky conditions in all cases, 
indicating that CNN models established with clear-sky samples have good ability when extended 
to cloudy conditions. Overall, the CNN LST agrees well with the in-situ LST, with R
2 values 
ranging from 0.94 to 0.98. Although the RMSE of CNN LST was above 4 K in a few cases, the 
RMSE was mainly due to the different land cover types and elevation changes within the MW 
pixels. 
 
Figure 9. Scatter plots between (1) the MODIS LST and (2) the CNN LST estimates from AMSR-E 
data and the in-situ LST at CBS (aâ€“d), TYU (eâ€“h), and DXI (iâ€“l). 
Figure 9. Scatter plots between (1) the MODIS LST and (2) the CNN LST estimates from AMSR-E data
and the in-situ LST at CBS (aâ€“d), TYU (eâ€“h), and DXI (iâ€“l).
For HMO, both the MODIS LST and the CNN LST have high systematic overestimation (above 4 K)
during the daytime and have better accuracy at nighttime (RMSE was 1.39 K for the MODIS LST and
2.63 K for the CNN LST). The signiï¬cant overestimation of the CNN LST during the daytime may be
induced by the overestimation of the MODIS LST, which was the basis for the training of the CNN
models. In addition, an overestimation also occurred when the LST was less than 270 K at night,
which is similar to that at SDQ.
The validation results of the CNN LST under clear-sky conditions and cloudy conditions are also
shown in Figures 9 and 10. It can be seen that the RMSE di ï¬€erences of the clear-sky conditions and the
cloudy conditions were lower than 0.5 K in most cases. Exceptions occurred during the daytime of
HMO and the nighttime of SDQ. The reason was the uneven distribution of the clear-sky and the cloudy
samples. However, the CNN LSTs under cloudy conditions do not show signiï¬cant outliers compared
with the CNN LSTs under clear-sky conditions in all cases, indicating that CNN models established
with clear-sky samples have good ability when extended to cloudy conditions. Overall, the CNN LST
agrees well with the in-situ LST, withR2 values ranging from 0.94 to 0.98. Although the RMSE of CNN
Remote Sens.2020, 12, 2691 18 of 27
LST was above 4 K in a few cases, the RMSE was mainly due to the di ï¬€erent land cover types and
elevation changes within the MW pixels.
Remote Sens. 2019, 11, x FOR PEER REVIEW 18 of 28 
 
 
Figure 10. Scatter plots between (1) the MODIS LST and (2) the CNN LST estimates from AMSR2 
data and the in-situ LST at SDQ (aâ€“d) and HMO (eâ€“h). 
4.4. Intercomparison with GlobTemperature AMSR-E LST and LPDR Air Temperature 
After the validations with the in-situ LST, the CNN LST was intercompared with the AMSR-
E LST provided by the GlobTemperature (hereafter  termed GT LST) [26,27]. Since GT LST was 
only available from 2008 to 2010, the CNN LST,  GT LST, and MODIS LST were intercompared 
based on the data of 2009 for AMSR-E (Figure 11). The pixel percentages of the RMSD difference 
between CNN LST and GT LST in different ranges over China are shown in Table 6. The statistical 
results show that ~50% of the pixels of the CN N LST have smaller RMSDs and only ~20% of the 
pixels of the CNN LST have larger RMSDs than GT LST. It can be observed that the pixels with 
significant underestimation for CNN LST are concentrated on the boundaries of the Tibetan 
Plateau. This phenomenon could be partly due to snow. Based on MYD10C1 product, we found 
that these pixels are frequently covered with snow, and snow pixels tend to have larger bias [27]. 
By analyzing the BTs of these pixels, we found an evident underestimation for the BTs, especially 
at high frequency (i.e., 36.5 and 89.0 GHz), resulting in the underestimation of the CNN LST for 
snow pixels. This corresponds to the strong volume scattering of snow [27,62].  
In addition, the overpass times of AMSR-E and AMSR2 were both ~13:30 and 01:30 local solar 
time, which are close to the time when daily ma ximum and minimum AT appear [12]. Fily et al. 
[16] showed LST was close to AT at surface level over dense vegetation. Thus, a comparison 
between the CNN LST and the LPDR ATs was also performed. Figure 12 shows maps of RMSDs 
and R
2 between the CNN LST and the LPDR ATs. It can be observed that the CNN LST agrees 
well with the LPDR ATs for most pixels (over 80% of pixel R2 values are above 0.8) and there are 
small RMSDs for pixels with high vegetation co verage (e.g., the average RMSDs of South China 
and Northeast China are less than 3 K). This phenomenon further confirms that the CNN LST and 
Figure 10. Scatter plots between (1) the MODIS LST and (2) the CNN LST estimates from AMSR2 data
and the in-situ LST at SDQ (aâ€“d) and HMO (eâ€“h).
4.4. Intercomparison with GlobTemperature AMSR-E LST and LPDR Air Temperature
After the validations with the in-situ LST, the CNN LST was intercompared with the AMSR-E LST
provided by the GlobTemperature (hereafter termed GT LST) [26,27]. Since GT LST was only available
from 2008 to 2010, the CNN LST, GT LST, and MODIS LST were intercompared based on the data of
2009 for AMSR-E (Figure 11). The pixel percentages of the RMSD diï¬€erence between CNN LST and
GT LST in diï¬€erent ranges over China are shown in Table 6. The statistical results show that ~50% of
the pixels of the CNN LST have smaller RMSDs and only ~20% of the pixels of the CNN LST have
larger RMSDs than GT LST. It can be observed that the pixels with signiï¬cant underestimation for
CNN LST are concentrated on the boundaries of the Tibetan Plateau. This phenomenon could be partly
due to snow. Based on MYD10C1 product, we found that these pixels are frequently covered with
snow, and snow pixels tend to have larger bias [27]. By analyzing the BTs of these pixels, we found an
evident underestimation for the BTs, especially at high frequency (i.e., 36.5 and 89.0 GHz), resulting in
the underestimation of the CNN LST for snow pixels. This corresponds to the strong volume scattering
of snow [27,62].
In addition, the overpass times of AMSR-E and AMSR2 were both ~13:30 and 01:30 local solar
time, which are close to the time when daily maximum and minimum AT appear [12]. Fily et al. [16]
showed LST was close to AT at surface level over dense vegetation. Thus, a comparison between the
CNN LST and the LPDR ATs was also performed. Figure 12 shows maps of RMSDs and R2 between
the CNN LST and the LPDR ATs. It can be observed that the CNN LST agrees well with the LPDR ATs
for most pixels (over 80% of pixel R2 values are above 0.8) and there are small RMSDs for pixels with
Remote Sens.2020, 12, 2691 19 of 27
high vegetation coverage (e.g., the average RMSDs of South China and Northeast China are less than
3 K). This phenomenon further conï¬rms that the CNN LST and AT have similar annual variations
and are close over dense vegetation. Therefore, it can be concluded that CNN LST estimates have
high accuracy.
Remote Sens. 2019, 11, x FOR PEER REVIEW 19 of 28 
 
AT have similar annual variations and are clos e over dense vegetation. Therefore, it can be 
concluded that CNN LST estimates have high accuracy. 
 
Figure 11. Spatial distribution of the bias ( a,c,e,g) of the difference and RMSD ( b,d,f,h) between 
(1) the CNN LST and (2) the GT LST and the MO DIS LST during the daytime and nighttime for 
AMSR-E. The statistics are based on the data of 2009. 
Table 6. The pixel percentages of the RMSD difference between the CNN LST and the GT LST in 
different ranges: (i) Î”RMSD < -0.5 K represents CNN LST has better accuracy; (ii) âˆ’0.5 K â‰¤ Î”RMSD 
â‰¤ 0.5 K represents CNN LST has similar accuracy to GT LST; (iii) Î”RMSD > 0.5 represents GT LST 
has better accuracy. 
 Î”RMSD < âˆ’0.5 K âˆ’0.5 K â‰¤ Î”RMSD â‰¤ 0.5 K Î”RMSD > 0.5 
Daytime 47.7% 29.4% 22.9% 
Nighttime 52.3% 29.1% 18.6% 
 
Figure 12. Spatial distribution of the RMSD ( a,c,e,g) and R2 (b,d,f,h) between the CNN LST and 
the LPDR ATs for AMSR-E (the top panels) and AMSR2 (the bottom panels). The statistics are 
based on the data of 2009 and 2016 for AMSR-E and AMSR2, respectively. 
5. Discussion 
Considering that the comparison of the three networks should be performed when they are 
optimal respectively, we examined the structural parameters of NN, DBN, and CNN. The 
Figure 11. Spatial distribution of the bias ( a,c,e,g) of the diï¬€erence and RMSD (b,d,f,h) between (1)
the CNN LST and (2) the GT LST and the MODIS LST during the daytime and nighttime for AMSR-E.
The statistics are based on the data of 2009.
Table 6. The pixel percentages of the RMSD diï¬€erence between the CNN LST and the GT LST in diï¬€erent
ranges: (i) âˆ†RMSD < -0.5 K represents CNN LST has better accuracy; (ii) âˆ’0.5 K â‰¤âˆ†RMSD â‰¤0.5 K
represents CNN LST has similar accuracy to GT LST; (iii) âˆ†RMSD > 0.5 represents GT LST has
better accuracy.
âˆ†RMSD < âˆ’0.5 K âˆ’0.5 K â‰¤âˆ†RMSD â‰¤0.5 K âˆ†RMSD > 0.5
Daytime 47.7% 29.4% 22.9%
Nighttime 52.3% 29.1% 18.6%
Remote Sens. 2019, 11, x FOR PEER REVIEW 19 of 28 
 
AT have similar annual variations and are clos e over dense vegetation. Therefore, it can be 
concluded that CNN LST estimates have high accuracy. 
 
Figure 11. Spatial distribution of the bias ( a,c,e,g) of the difference and RMSD ( b,d,f,h) between 
(1) the CNN LST and (2) the GT LST and the MO DIS LST during the daytime and nighttime for 
AMSR-E. The statistics are based on the data of 2009. 
Table 6. The pixel percentages of the RMSD difference between the CNN LST and the GT LST in 
different ranges: (i) Î”RMSD < -0.5 K represents CNN LST has better accuracy; (ii) âˆ’0.5 K â‰¤ Î”RMSD 
â‰¤ 0.5 K represents CNN LST has similar accuracy to GT LST; (iii) Î”RMSD > 0.5 represents GT LST 
has better accuracy. 
 Î”RMSD < âˆ’0.5 K âˆ’0.5 K â‰¤ Î”RMSD â‰¤ 0.5 K Î”RMSD > 0.5 
Daytime 47.7% 29.4% 22.9% 
Nighttime 52.3% 29.1% 18.6% 
 
Figure 12. Spatial distribution of the RMSD ( a,c,e,g) and R2 (b,d,f,h) between the CNN LST and 
the LPDR ATs for AMSR-E (the top panels) and AMSR2 (the bottom panels). The statistics are 
based on the data of 2009 and 2016 for AMSR-E and AMSR2, respectively. 
5. Discussion 
Considering that the comparison of the three networks should be performed when they are 
optimal respectively, we examined the structural parameters of NN, DBN, and CNN. The 
Figure 12. Spatial distribution of the RMSD (a,c,e,g) and R2 (b,d,f,h) between the CNN LST and the
LPDR ATs for AMSR-E (the top panels) and AMSR2 (the bottom panels). The statistics are based on the
data of 2009 and 2016 for AMSR-E and AMSR2, respectively.
Remote Sens.2020, 12, 2691 20 of 27
5. Discussion
Considering that the comparison of the three networks should be performed when they are
optimal respectively, we examined the structural parameters of NN, DBN, and CNN. The structural
parameters were examined with the samples from Group I. The kernel size of the CNN was ï¬rst
examined, and the examination results based on the validation set are shown in Figure 13. As the size
of the convolution kernel increased, the RMSD ï¬rst decreased signiï¬cantly; when the kernel size was
greater than 1 Ã—7, the RMSD remained relatively stable. The MBDs did not exhibit a clear pattern but
were lower than 0.15 K. Therefore, 1 Ã—7 was designated as the eventual kernel size of the CNN model.
Remote Sens. 2019, 11, x FOR PEER REVIEW 20 of 28 
 
structural parameters were examined with the samples from Group I. The kernel size of the CNN 
was first examined, and the examination results based on the validation set are shown in Figure 
13. As the size of the convolution kernel increased, the RMSD first decreased significantly; when 
the kernel size was greater than 1 Ã— 7, the RMSD  remained relatively stable. The MBDs did not 
exhibit a clear pattern but were lower than 0.15 K. Therefore, 1 Ã— 7 was designated as the eventual 
kernel size of the CNN model. 
 
Figure 13. The mean bias deviation (MBD) and RMSD of different convolutional kernel sizes for 
CNN based on the validation set of Group I. 
The examination results for layers are shown in Figure 14. The DBN models with two or 
three RBMs had similar accuracies to one RBM wi th more than 112 hidd en units. For NN and 
CNN, the accuracies of the two-layer models had an increase of more than 0.10 K compared to 
the one-layer models. However, the accuracies of  three-layer models were almost the same as 
those of two-layer models and were sometimes even lower. This situation shows that two hidden 
layers for NN, one RBM for DBN, and one convolutional layer and one fully connected layer for 
CNN are sufficient to describe the nonlinear relationship between the LST and the input 
parameters. For NN, the numbers of hidden neuron s were designated as 64 and 48 for the first 
hidden layer and the second hidden layer, respectively. For DBN, the number of hidden units was 
designated as 112 for the RBM. For CNN, the numb er of convolution kernels was designated as 
112, and the number of neurons in the fully connected layer was designated as 64. 
In general, the accuracies for AMSR2 were lower than those for AMSR-E. The possible 
reasons are: (i) AMSR2 observations had higher spatial resolution than AMSR-E, resulting in 
greater uncertainty for AMSR2 samples during the process of spatial matching with MERRA-2 
product and GLDAS product, and (ii) there stil l existed deviation between the synchronized 
MYD11C1 LST and â€œtrueâ€ LST at the overpass time of AMSR2. The nighttime models had better 
accuracies than the daytime models, and the nighttime RMSDs were more than 1 K lower than 
those of the daytime RMSDs. The reason is that the daytime data are noisier than the nighttime 
data. 
The best combinations of input parameters are BTs, NDVI, AT-2m, and DOY for the daytime 
conditions and BTs and AT-2m for the nighttime conditions. The small contributions of SM, SC, 
and TPWV can be explained by the fact that th ese parameters can be expressed by the MW BTs 
[51,62]. NDVI can implicitly characterize the proportion of different land cover types within a 
pixel, and it is also the reason why NDVI and LCTP have similar contributions. Thus, taking 
NDVI as an input can improve the accuracies of  the daytime models. In contrast, the different 
Figure 13. The mean bias deviation (MBD) and RMSD of diï¬€erent convolutional kernel sizes for CNN
based on the validation set of Group I.
The examination results for layers are shown in Figure 14. The DBN models with two or three
RBMs had similar accuracies to one RBM with more than 112 hidden units. For NN and CNN,
the accuracies of the two-layer models had an increase of more than 0.10 K compared to the one-layer
models. However, the accuracies of three-layer models were almost the same as those of two-layer
models and were sometimes even lower. This situation shows that two hidden layers for NN, one RBM
for DBN, and one convolutional layer and one fully connected layer for CNN are suï¬ƒcient to describe
the nonlinear relationship between the LST and the input parameters. For NN, the numbers of
hidden neurons were designated as 64 and 48 for the ï¬rst hidden layer and the second hidden layer,
respectively. For DBN, the number of hidden units was designated as 112 for the RBM. For CNN,
the number of convolution kernels was designated as 112, and the number of neurons in the fully
connected layer was designated as 64.
In general, the accuracies for AMSR2 were lower than those for AMSR-E. The possible reasons are:
(i) AMSR2 observations had higher spatial resolution than AMSR-E, resulting in greater uncertainty
for AMSR2 samples during the process of spatial matching with MERRA-2 product and GLDAS
product, and (ii) there still existed deviation between the synchronized MYD11C1 LST and â€œtrueâ€ LST
at the overpass time of AMSR2. The nighttime models had better accuracies than the daytime models,
and the nighttime RMSDs were more than 1 K lower than those of the daytime RMSDs. The reason is
that the daytime data are noisier than the nighttime data.
Remote Sens.2020, 12, 2691 21 of 27
Remote Sens. 2019, 11, x FOR PEER REVIEW 21 of 28 
 
land cover types within the pixels have similar thermal properties and LSTs at nighttime. Hence, 
it is understandable that NDVI has a small contribution to the nighttime models. As for DOY, it 
is interesting that we found DOY can improve th e accuracies of the daytime models. However, 
DOY is irrelevant to Equation (5). The relationship between the MODIS LST (T
MODIS) and the CNN 
LST (fi(xi)) can be written as: 
Î´MODIS =() +ii iTf x  (8) 
where xi is the input parameters of combination C i; Î´i is the residual corresponding to C i and 
denotes the tiny fraction that cannot be interpreted by input parameters; and fi is the conversion 
relationship established by CNN based on Ci. The purpose of inputting multiple parameters is to 
decompose the residuals as much as possible. Generally, the component temperature differences 
between barren land and vegetation within the pixels are larger in summer compared to those in 
winter. Therefore, it is under standable that the component temperature differences can be 
expressed as a function with DOY as the indep endent variable. In other words, DOY helps to 
characterize the component temperature differen ces within the pixels. This information has 
potential value for the LST retrieval, and, thus, can help decompose the residuals. Another 
possible reason is that the LST of a whole year  can be divided into three temporal components 
(i.e., the annual temperature component-ATC, the diurnal temperature component-DTC, and the 
weather-change temperature component-WTC) [3,56], and ATC is a function of DOY. In contrast, 
the small contributions of DOY to the nighttime models can be concluded to the smaller residuals 
of the models themselves.  
 
Figure 14. The RMSDs of NN ( a), DBN ( b), and CNN ( c) with different layers based on the 
validation set. The â€œconv layerâ€ and â€œfc layerâ€ mean the convolutional layer and fully connected 
layer, respectively. 
Figure 14. The RMSDs of NN (a), DBN (b), and CNN (c) with diï¬€erent layers based on the validation set.
The â€œconv layerâ€ and â€œfc layerâ€ mean the convolutional layer and fully connected layer, respectively.
The best combinations of input parameters are BTs, NDVI, AT-2m, and DOY for the daytime
conditions and BTs and AT-2m for the nighttime conditions. The small contributions of SM, SC,
and TPWV can be explained by the fact that these parameters can be expressed by the MW BTs [51,62].
NDVI can implicitly characterize the proportion of diï¬€erent land cover types within a pixel, and it
is also the reason why NDVI and LCTP have similar contributions. Thus, taking NDVI as an input
can improve the accuracies of the daytime models. In contrast, the diï¬€erent land cover types within
the pixels have similar thermal properties and LSTs at nighttime. Hence, it is understandable that
NDVI has a small contribution to the nighttime models. As for DOY, it is interesting that we found
DOY can improve the accuracies of the daytime models. However, DOY is irrelevant to Equation (5).
The relationship between the MODIS LST (TMODIS) and the CNN LST (fi(xi)) can be written as:
TMODIS = fi(xi) +Î´i (8)
where xi is the input parameters of combination C i; Î´i is the residual corresponding to C i and
denotes the tiny fraction that cannot be interpreted by input parameters; and fi is the conversion
relationship established by CNN based on C i. The purpose of inputting multiple parameters is to
decompose the residuals as much as possible. Generally, the component temperature di ï¬€erences
between barren land and vegetation within the pixels are larger in summer compared to those in
winter. Therefore, it is understandable that the component temperature diï¬€erences can be expressed
as a function with DOY as the independent variable. In other words, DOY helps to characterize
the component temperature diï¬€erences within the pixels. This information has potential value for
the LST retrieval, and, thus, can help decompose the residuals. Another possible reason is that the
LST of a whole year can be divided into three temporal components (i.e., the annual temperature
component-ATC, the diurnal temperature component-DTC, and the weather-change temperature
Remote Sens.2020, 12, 2691 22 of 27
component-WTC) [3,56], and ATC is a function of DOY. In contrast, the small contributions of DOY to
the nighttime models can be concluded to the smaller residuals of the models themselves.
It is noticeable that this study was not the ï¬rst time that CNN was applied to estimate the MW
LST. As described in Section 1, Tan et al. [34] constructed the CNN model. However, only BTs were
used as inputs (i.e., C14 in this study). Figure 6 shows that the accuracy of C14 is lower by 0.8 K during
the daytime and 0.3 K during the nighttime than the best combination. Thus, it can be concluded that
adding some auxiliary data (e.g., AT-2m) on the basis of BTs can signiï¬cantly improve the accuracies of
the retrieval models.
In this study, the ground station FOVs were greatly diï¬€erent from the spatial resolutions of the
MW pixels. To quantify the representativeness errors introduced by the scale mismatching between
the ground stations and the MW pixels, we calculated the errors based on the biases and STDs from
Figure 1 and the validation results of the MODIS LST. Similar to Huang et al. [63], by assuming that
MODIS LST is the â€œtrue LSTâ€ on the 1-km scales, the representativeness error for each station is
given by:
MBEGtoMW = MBEGto1KM + MBE1KMtoMW
STD2
GtoMW = STD2
Gto1KM + STD2
1KMtoMW
(9)
where MBE GtoMW, MBE Gto1KM, and MBE 1KMtoMW are systematic errors introduced by the scale
mismatching between the ground station and the MW pixel, between the ground station and the 1-km
MODIS pixel, and between the 1-km MODIS pixel and the MW pixel, respectively; STD GtoMW,
STDGto1KM, and STD 1KMtoMW are the corresponding STD values. The statistical results of the
representativeness errors of the ï¬ve stations and the corresponding validation results of the CNN
LST are shown in Table 7. It can be seen that in most cases, the MBE CNN and STDCNN were close
to the MBEGtoMW and STDGtoMW (MBE diï¬€erences and STD diï¬€erences were both lower than 1 K),
respectively. Exceptions occurred in the nighttime of SDQ and HMO, and this phenomenon was
caused by frozen soil (see Section 4.3). Therefore, Table 7 further conï¬rms that the main reason for the
errors of the CNN LST against in-situ LST is the scale mismatching between the ground stations and
the MW pixels.
Table 7. The representativeness errors of the ï¬ve stations and the corresponding validation results of
the CNN LST based on the in-situ LST. All MBE and STD values are in K.
Station Daytime Nighttime
MBEGtoMW MBECNN STDGtoMW STDCNN MBEGtoMW MBECNN STDGtoMW STDCNN
CBS 0.77 0.75 2.72 1.96 âˆ’3.29 âˆ’3.06 1.43 1.76
TYU 0.32 âˆ’0.45 3.27 3.43 âˆ’0.09 âˆ’0.31 2.64 2.96
DXI 2.70 1.95 2.47 2.28 âˆ’2.06 âˆ’2.84 1.27 1.91
SDQ 4.19 3.64 3.45 2.99 âˆ’0.61 1.84 1.55 3.64
HMO 3.67 4.03 3.05 3.50 0.14 âˆ’0.32 1.47 2.62
For CNN, we only tried to convolve on the input vector of a single pixel. Another method to build
the CNN model is to use the parameter images (e.g., BT images and NDVI images) of the entire study
area as inputs. In this situation, the spatial relationships between the adjacent pixels may be helpful
in improving model accuracy. Nevertheless, one should keep in mind that it is diï¬ƒcult to apply this
method: MW images have coarse spatial resolutions, resulting in weak spatial connections between the
adjacent pixels (which means a small contribution to the LSTs of the surrounding pixels). In addition,
accurate and cloud-free LST images for large study areas are usually not available, which limits the
extraction of LST for the training samples. Further studies to incorporate the spatial relationships in
the estimation of LST from MW observations are needed.
A well-recognized feature of neural networks is the â€˜black boxâ€™ problem, which means that it
is diï¬ƒcult to understand the physical mechanisms for obtaining the output based on the inputs.
The opacity of neural network makes it hard to further improve the accuracy of the neural network.
Remote Sens.2020, 12, 2691 23 of 27
For the estimation of LST from MW observations, three approaches may contribute to the improvement
of the accuracy. The ï¬rst approach is to increase the sample size. Larger sample size would be helpful
to reduce overï¬tting and, thus, improve the accuracy of the output. In this study, the upscaling method
for the MODIS LST was spatial averaging, which may introduce uncertainty to the LST of the training
samples. Therefore, the second approach is to improve the accuracy of the training samples. The third
approach is to incorporate the physical models into the neural networks by using the physical models
as the boundaries of the networks, to provide initial guesses, and/or to construct new input features for
the networks. For example, the thermal sampling depth correction model developed by Zhou et al. [57]
may be incorporated to help neural networks better estimate the LST over barren land.
Finally, there still exist some issues in this study. The ï¬rst one is the training of the neural
network was performed with the MODIS LST product. Although the MODIS LST has high accuracy,
the uncertainty of MODIS LST will inevitably increase when reprojected to match with the MW pixels.
Therefore, an accurate upscaling method related to land cover types and elevation distribution within
the MW pixels is critical for reducing the uncertainty in the spatial matching process. The second
one is the scale mismatch between the ground stations and the MW pixels. This issue is critical for
the validation of low-resolution LST. Thus, it is essential to ï¬nd an evaluation method of spatial
representativeness of the ground station to realize the conversion of in-situ LST from station FOV to
coarser resolution.
6. Conclusions
This study examined the performances of NN (i.e., the traditional neural network) and two deep
learning methods (i.e., DBN and CNN) in the estimation of LST from satellite MW observations.
The input parameter set for these networks included BTs, soil moistures, NDVI, snow cover, land cover
type percent, air temperature at 2 m above the ground surface, total precipitable water vapor, and DOY.
Based on the training, validation, and test sets derived from the MODIS LST, microwave BTs, and the
aforementioned input parameters, the results demonstrate that CNN outperformed NN and DBN. The
LSTs estimated by CNN from the AMSR-E and AMSR2 data were closer to MODIS LST: the RMSD
values were 3 K during the daytime and 1.74 K at night for AMSR-E; the corresponding RMSD values
were 3.48 K and 2.10 K for AMSR2. In contrast, the RMSDs of NN and DBN were higher by 0.1 K and
0.4 K, respectively. Additionally, CNN was more prominent than NN and DBN for diï¬€erent land cover
types and seasons. Therefore, it should be concluded that CNN performs better than the NN and DBN
in the estimation of LST from satellite MW observations.
More details for the impacts of the input parameters on the performances of CNN were obtained.
Among the surface parameters used to implicitly parameterize the LSE, the NDVI and land cover
type percent have greater impacts than the other parameters during the daytime, and NDVI and
land cover type percent cannot be removed at the same time; nevertheless, their impacts decrease
at nighttime. For the two parameters used to implicitly quantify the atmospheric e ï¬€ects, the air
temperature plays a more important role than the total precipitable water vapor. In addition, DOY is
helpful in improving the accuracy of the CNN model in the daytime. Therefore, the best combinations
of input parameters are the BTs, NDVI, air temperature, and DOY for the daytime and BTs and air
temperature for the nighttime. The CNN LST estimate from AMSR-E with the best combinations yields
RMSDs of 2.19â€“3.58 K for daytime and 1.43â€“2.14 K for nighttime for diverse land cover types.
Thorough validation of the LST estimates of CNN was conducted based on the in-situ LST.
The RMSEs range from 2.10 K to 5.34 K during the daytime and 2.63 K to 4.08 K during the nighttime.
The CNN LST agrees well with the in-situ LST, with R2 values ranging from 0.94 to 0.98. Although the
accuracy of the CNN LST is not as satisfactory as the TIR LST in a few cases, the diï¬€erences between
the CNN LSTs and the in-situ LSTs are mainly due to the di ï¬€erent land cover types and elevation
distributions within the MW pixels containing the stations. Further intercomparison indicates that the
~50% CNN LST estimates are closer to MODIS LSTs than ESAâ€™s GlobTemperature AMSR-E LST and
the average RMSDs are less than 3 K over dense vegetation compared to NASAâ€™s LPDR ATs. Findings
Remote Sens.2020, 12, 2691 24 of 27
from this study will be beneï¬cial for an in-depth understanding of the use of neural networks for
estimating LST from satellite observations.
Author Contributions: Conceptualization, S.W. and J.Z.; methodology, S.W. and T.L.; software, H.W.; validation,
S.W., X.Z. and J.M.; formal analysis, S.W.; investigation, T.L.; resources, J.Z.; data curation, H.W.; writingâ€”original
draft preparation, S.W.; writingâ€”review and editing, J.Z. and H.Z.; visualization, J.Z.; supervision, J.Z.;
project administration, J.Z.; funding acquisition, J.Z. All authors have read and agreed to the published version of
the manuscript.
Funding: This work was supported in part by the National Key Research and Development Program of China
(grant number: 2017YFB0503903), by the National Natural Science Foundation of China (grant number: 41871241),
by the National Key Research and Development Program of China (grant number: 2018YFC1505205), by the
Fundamental Research Funds for the Central Universities of China (grant number: ZYGX2019J069), and by the
ESA-MOST Dragon 5 Cooperation Programme (grant number: 59318).
Acknowledgments: The authors would like to thank the three reviewers for improving the clarity and relevance
of this work. The authors would also like to thank JAXA for providing AMSR-E and AMSR2 data, the EARTH
DATA for providing the MODIS data, GES DISC for providing MERRA-2 and GLDAS data, GlobTemperature
Data Portal for providing AMSR-E LST data, and NASA for providing the LPDR dataset.
Conï¬‚icts of Interest: The authors declare no conï¬‚ict of interest.
References
1. Su, Z. The Surface Energy Balance System (SEBS) for estimation of turbulent heat ï¬‚uxes.Hydrol. Earth Syst. Sci.
2002, 6, 85â€“100. [CrossRef]
2. Li, Z.L.; Tang, B.H.; Wu, H.; Ren, H.; Yan, G.; Wan, Z.; Trigo, I.F.; Sobrino, J.A. Satellite-derived land surface
temperature: Current status and perspectives. Remote Sens. Environ.2013, 131, 14â€“37. [CrossRef]
3. Zhang, X.; Zhou, J.; Gottsche, F.M.; Zhan, W.; Liu, S.; Cao, R. A Method Based on Temporal Component
Decomposition for Estimating 1-km All-Weather Land Surface Temperature by Merging Satellite Thermal
Infrared and Passive Microwave Observations. IEEE Trans. Geosci. Remote Sens. 2019, 57, 4670â€“4691.
[CrossRef]
4. Chen, Y.; Zhan, W.; Quan, J.; Zhou, J.; Zhu, X.; Sun, H. Disaggregation of Remotely Sensed Land Surface
Temperature: A Generalized Paradigm. IEEE Trans. Geosci. Remote Sens.2014, 52, 5952â€“5965. [CrossRef]
5. Wan, Z.; Dozier, J. A generalized split-window algorithm for retrieving land-surface temperature from space.
IEEE Trans. Geosci. Remote Sens.1996, 34, 892â€“905. [CrossRef]
6. Gillespie, A.R.; Rokugawa, S.; Hook, S.J.; Matsunaga, T.; Kahle, A.B. Temperature/Emissivity Separation
Algorithm Theoretical Basis Document, Version 2.4; ATBD Contract NAS5-31372; NASA: Washington, DC, USA,
1999.
7. Qin, Z.; Karnieli, A.; Berliner, P . A mono-window algorithm for retrieving land surface temperature from
Landsat TM data and its application to the Israel-Egypt border region.Int. J. Remote Sens.2001, 22, 3719â€“3746.
[CrossRef]
8. Martin, M.; Ghent, D.; Pires, A.; GÃ¶ttsche, F.M.; Cermak, J.; Remedios, J. Comprehensive In Situ Validation of
Five Satellite Land Surface Temperature Data Sets over Multiple Stations and Years.Remote Sens.2019, 11, 479.
[CrossRef]
9. Chen, S.; Chen, X.; Chen, W.; Su, Y.; Li, D. A simple retrieval method of land surface temperature from
AMSR-E passive microwave dataâ€”A case study over Southern China during the strong snow disaster of
2008. Int. J. Appl. Earth Obs. Geoinf.2011, 13, 140â€“151. [CrossRef]
10. Prigent, C.; Jimenez, C.; Aires, F. Toward â€œall weather,â€ long record, and real-time land surface
temperature retrievals from microwave satellite observations: Microwave land surface temperature.
J. Geophys. Res. Atmos.2016, 121, 5699â€“5717. [CrossRef]
11. Martins, J.P .A.; Trigo, I.F.; Ghilain, N.; Jimenez, C.; GÃ¶ttsche, F.M.; Ermida, S.L.; Olesen, F.S.;
Gellens-Meulenberghs, F.; Arboleda, A. An All-Weather Land Surface Temperature Product Based on
MSG/SEVIRI Observations. Remote Sens.2019, 11, 3044. [CrossRef]
12. Zhou, J.; Dai, F.; Zhang, X.; Zhao, S.; Li, M. Developing a temporally land cover-based look-up table
(TL-LUT) method for estimating land surface temperature based on AMSR-E data over the Chinese landmass.
Int. J. Appl. Earth Obs. Geoinf.2015, 34, 35â€“50. [CrossRef]
Remote Sens.2020, 12, 2691 25 of 27
13. Huang, C.; Duan, S.B.; Jiang, X.G.; Han, X.J.; Leng, P .; Gao, M.F.; Li, Z.L. A physically based algorithm for
retrieving land surface temperature under cloudy conditions from AMSR2 passive microwave measurements.
Int. J. Remote Sens.2019, 40, 1828â€“1843. [CrossRef]
14. McFarland, M.J.; Miller, R.L.; Neale, C.M.U. Land surface temperature derived from the SSM /I passive
microwave brightness temperatures. IEEE Trans. Geosci. Remote Sens.1990, 28, 839â€“845. [CrossRef]
15. Holmes, T.R.H.; De Jeu, R.A.M.; Owe, M.; Dolman, A.J. Land surface temperature from Ka band (37 GHz)
passive microwave observations. J. Geophys. Res.2009, 114, D04113. [CrossRef]
16. Fily, M. A simple retrieval method for land surface temperature and fraction of water surface determination
from satellite microwave brightness temperatures in sub-arctic areas. Remote Sens. Environ.2003, 85, 328â€“338.
[CrossRef]
17. Gao, H.; Fu, R.; Dickinson, R.E.; Juarez, R.I.N. A Practical Method for Retrieving Land Surface Temperature
from AMSR-E Over the Amazon Forest. IEEE Trans. Geosci. Remote Sens.2008, 46, 193â€“199. [CrossRef]
18. Royer, A.; Poirier, S. Surface temperature spatial and temporal variations in North America from homogenized
satellite SMMR-SSM/I microwave measurements and reanalysis for 1979â€“2008. J. Geophys. Res. Atmos.
2010, 115. [CrossRef]
19. Zhao, T.J.; Zhang, L.X.; Shi, J.C.; Jiang, L.M. A physically based statistical methodology for surface soil
moisture retrieval in the Tibet Plateau using microwave vegetation indices.J. Geophys. Res.2011, 116, D08116.
[CrossRef]
20. AndrÃ©, C.; OttlÃ©, C.; Royer, A.; Maignan, F. Land surface temperature retrieval over circumpolar Arctic using
SSM/Iâ€“SSMIS and MODIS data. Remote Sens. Environ.2015, 162, 1â€“10. [CrossRef]
21. Weng, F.; Grody, N.C. Physical retrieval of land surface temperature using the special sensor microwave
imager. J. Geophys. Res. Atmos.1998, 103, 8839â€“8848. [CrossRef]
22. Aires, F.; Prigent, C.; Bernardo, F.; Jim Ã©nez, C.; Saunders, R.; Brunel, P . A Tool to Estimate
Land-Surface Emissivities at Microwave frequencies (TELSEM) for use in numerical weather prediction.
Q. J. R. Meteorol. Soc.2011, 137, 690â€“699. [CrossRef]
23. Galantowicz, J.F.; Moncet, J.L.; Liang, P .; Lipton, A.E.; Uymin, G.; Prigent, C.; Grassotti, C. Subsurface
emission eï¬€ects in AMSR-E measurements: Implications for land surface microwave emissivity retrieval.
J. Geophys. Res.2011, 116, D17105. [CrossRef]
24. Prigent, C.; Rossow, W.B.; Matthews, E.; Marticorena, B. Microwave radiometric signatures of di ï¬€erent
surface types in deserts. J. Geophys. Res. Atmos.1999, 104, 12147â€“12158. [CrossRef]
25. Aires, F.; Prigent, C.; Rossow, W.B.; Rothstein, M. A new neural network approach including ï¬rst guess for
retrieval of atmospheric water vapor, cloud liquid water path, surface temperature, and emissivities over
land from satellite microwave observations. J. Geophys. Res. Atmos.2001, 106, 14887â€“14907. [CrossRef]
26. JimÃ©nez, C.; Prigent, C.; Ermida, S.L.; Moncet, J.L. Inversion of AMSR-E observations for land surface
temperature estimation: 1. Methodology and evaluation with station temperature: AMSR-E LAND
SURFACE TEMPERATURE.J. Geophys. Res. Atmos.2017, 122, 3330â€“3347. [CrossRef]
27. Ermida, S.L.; JimÃ©nez, C.; Prigent, C.; Trigo, I.F.; DaCamara, C.C. Inversion of AMSR-E observations for land
surface temperature estimation: 2. Global comparison with infrared satellite temperature: AMSR-E LAND
SURFACE TEMPERATURE.J. Geophys. Res. Atmos.2017, 122, 3348â€“3360. [CrossRef]
28. Moncet, J.L.; Liang, P .; Galantowicz, J.F.; Lipton, A.E.; Uymin, G.; Prigent, C.; Grassotti, C. Land
Surface Microwave Emissivities Derived from AMSR-E and MODIS Measurements with Advanced
Quality Control. Available online: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2010JD015429
(accessed on 1 July 2019).
29. Hinton, G.E.; Osindero, S.; Teh, Y.W. A Fast Learning Algorithm for Deep Belief Nets. Neural Comput.
2006, 18, 1527â€“1554. [CrossRef]
30. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet classiï¬cation with deep convolutional neural networks.
Commun. ACM2017, 60, 84â€“90. [CrossRef]
31. Li, X.; Liu, S.; Li, H.; Ma, Y.; Wang, J.; Zhang, Y.; Xu, Z.; Xu, T.; Song, L.; Yang, X.; et al. Intercomparison
of Six Upscaling Evapotranspiration Methods: From Site to the Satellite Pixel. J. Geophys. Res. Atmos.
2018, 123, 6777â€“6803. [CrossRef]
32. Shen, H.; Jiang, Y.; Li, T.; Cheng, Q.; Zeng, C.; Zhang, L. Deep learning-based air temperature mapping by
fusing remote sensing, station, simulation and socioeconomic data. Remote Sens. Environ.2020, 240, 111692.
[CrossRef]
Remote Sens.2020, 12, 2691 26 of 27
33. Ge, L.; Hang, R.; Liu, Y.; Liu, Q. Comparing the Performance of Neural Network and Deep Convolutional
Neural Network in Estimating Soil Moisture from Satellite Observations. Remote Sens. 2018, 10, 1327.
[CrossRef]
34. Tan, J.; NourEldeen, N.; Mao, K.; Shi, J.; Li, Z.; Xu, T.; Yuan, Z. Deep Learning Convolutional Neural Network
for the Retrieval of Land Surface Temperature from AMSR2 Data in China. Sensors 2019, 19, 2987. [CrossRef]
[PubMed]
35. Sadeghi, M.; Asanjan, A.A.; Faridzad, M.; Nguyen, P .; Hsu, K.; Sorooshian, S.; Braithwaite, D.
PERSIANN-CNN: Precipitation Estimation from Remotely Sensed Information Using Artiï¬cial Neural
Networksâ€“Convolutional Neural Networks. J. Hydrometeorol.2019, 20, 2273â€“2289. [CrossRef]
36. Prakash, S.; Norouzi, H.; Azarderakhsh, M.; Blake, R.; Tesfagiorgis, K. Global Land Surface Emissivity
Estimation from AMSR2 Observations. IEEE Geosci. Remote Sens. Lett.2016, 13, 1270â€“1274. [CrossRef]
37. Prakash, S.; Norouzi, H.; Azarderakhsh, M.; Blake, R.; Prigent, C.; Khanbilvardi, R. Estimation
of Consistent Global Microwave Land Surface Emissivity from AMSR-E and AMSR2 Observations.
J. Appl. Meteorol. Climatol.2018, 57, 907â€“919. [CrossRef]
38. Zhou, J.; Liang, S.; Cheng, J.; Wang, Y.; Ma, J. The GLASS Land Surface Temperature Product.
IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2019, 12, 493â€“507. [CrossRef]
39. Wan, Z. New reï¬nements and validation of the collection-6 MODIS land-surface temperature/emissivity
product. Remote Sens. Environ.2014, 140, 36â€“45. [CrossRef]
40. Dong, W. EOL Data Archiveâ€”CAMP: Tongyu (Inner Mongolia) Surface Meteorology and Radiation Data
Set. Available online: https://data.eol.ucar.edu/dataset/76.141 (accessed on 13 May 2020).
41. Liu, S.M.; Xu, Z.W.; Zhu, Z.L.; Jia, Z.Z.; Zhu, M.J. Measurements of evapotranspiration from eddy-covariance
systems and large aperture scintillometers in the Hai River Basin, China. J. Hydrol. 2013, 487, 24â€“38.
[CrossRef]
42. Liu, S.M.; Xu, Z.W.; Wang, W.Z.; Jia, Z.Z.; Zhu, M.J.; Bai, J.; Wang, J.M. A comparison of eddy-covariance
and large aperture scintillometer measurements with respect to the energy balance closure problem.
Hydrol. Earth Syst. Sci.2011, 15, 1291â€“1306. [CrossRef]
43. Li, X.; Cheng, G.; Liu, S.; Xiao, Q.; Ma, M.; Jin, R.; Che, T.; Liu, Q.; Wang, W.; Qi, Y.; et al. Heihe Watershed
Allied Telemetry Experimental Research (Hiwater). Bull. Am. Meteorol. Soc.2013, 94, 1145â€“1160. [CrossRef]
44. Liu, S.; Li, X.; Xu, Z.; Che, T.; Xiao, Q.; Ma, M.; Liu, Q.; Jin, R.; Guo, J.; Wang, L.; et al. The Heihe Integrated
Observatory Network: A Basin-Scale Land Surface Processes Observatory in China. Vadose Zone J.2018, 17.
[CrossRef]
45. Yang, J.; Zhou, J.; GÃ¶ttsche, F.M.; Long, Z.; Ma, J.; Luo, R. Investigation and validation of algorithms
for estimating land surface temperature from Sentinel-3 SLSTR data. Int. J. Appl. Earth Obs. Geoinf.
2020, 91, 102136. [CrossRef]
46. Xu, Z.; Liu, S.; Li, X.; Shi, S.; Wang, J.; Zhu, Z.; Xu, T.; Wang, W.; Ma, M. Intercomparison of surface energy ï¬‚ux
measurement systems used during the HiWATER-MUSOEXE.J. Geophys. Res. Atmos.2013, 118, 13140â€“13157.
[CrossRef]
47. Liang, S.L. Quantitative Remote Sensing of Land Surfaces; John Wiley & Sons: Hoboken, NJ, USA, 2004; p. 388.
48. Pearson, R.K. Outliers in process modeling and identiï¬cation.IEEE Trans. Control Syst. Technol.2002, 10, 55â€“63.
[CrossRef]
49. GÃ¶ttsche, F.M.; Olesen, F.S.; Trigo, I.F.; Bork-Unkelbach, A.; Martin, M.A. Long term validation of land surface
temperature retrieved from MSG/SEVIRI with continuous in-situ measurements in Africa. Remote Sens.
2016, 8, 410. [CrossRef]
50. Du, J.; Jones, L.A.; Kimball, J.S. Daily Global Land Surface Parameters Derived from AMSR-E and AMSR2, Version
2; NASA National Snow and Ice Data Center Distributed Active Archive Center: Boulder, CO, USA, 2017.
[CrossRef]
51. Du, J.; Kimball, J.S.; Jones, L.A.; Kim, Y.; Glassy, J.; Watts, J.D. A global satellite environmental data record
derived from AMSR-E and AMSR2 microwave Earth observations. Earth Syst. Sci. Data2017, 9, 791â€“808.
[CrossRef]
52. Jones, L.A.; Ferguson, C.R.; Kimball, J.S.; Zhang, K.; Chan, S.T.K.; McDonald, K.C.; Njoku, E.G.; Wood, E.F.
Satellite Microwave Remote Sensing of Daily Land Surface Air Temperature Minima and Maxima From
AMSR-E. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.2010, 3, 111â€“123. [CrossRef]
Remote Sens.2020, 12, 2691 27 of 27
53. Jones, L.A.; Kimball, J.S.; McDonald, K.C.; Chan, S.T.K.; Njoku, E.G.; Oechel, W.C. Satellite Microwave
Remote Sensing of Boreal and Arctic Soil Temperatures From AMSR-E. IEEE Trans. Geosci. Remote Sens.
2007, 45, 2004â€“2018. [CrossRef]
54. Springenberg, J.T.; Dosovitskiy, A.; Brox, T.; Riedmiller, M. Striving for Simplicity: The All Convolutional
Net. arXiv 2015, arXiv:14126806 Cs.
55. Zheng, D.; Wang, X.; van der Velde, R.; Ferrazzoli, P .; Wen, J.; Wang, Z.; Schwank, M.; Colliander, A.;
Bindlish, R.; Su, Z. Impact of surface roughness, vegetation opacity and soil permittivity on L-band
microwave emission and soil moisture retrieval in the third pole environment. Remote Sens. Environ.
2018, 209, 633â€“647. [CrossRef]
56. Zhan, W.; Zhou, J.; Ju, W.; Li, M.; Sandholt, I.; Voogt, J.; Yu, C. Remotely sensed soil temperatures beneath
snow-free skin-surface using thermal observations from tandem polar-orbiting satellites: An analytical
three-time-scale model. Remote Sens. Environ.2014, 143, 1â€“14. [CrossRef]
57. Zhou, J.; Zhang, X.; Zhan, W.; GÃ¶ttsche, F.M.; Liu, S.; Olesen, F.S.; Hu, W.; Dai, F. A thermal sampling depth
correction method for land surface temperature estimation from satellite passive microwave observation
over barren land. IEEE Trans. Geosci. Remote Sens.2017, 55, 4743â€“4756. [CrossRef]
58. Foody, G.M. Classiï¬cation accuracy comparison: Hypothesis tests and the use of conï¬dence intervals in
evaluations of di ï¬€erence, equivalence and non-inferiority. Remote Sens. Environ. 2009, 113, 1658â€“1663.
[CrossRef]
59. de Beurs, K.M.; Henebry, G.M.; Owsley, B.C.; Sokolik, I. Using multiple remote sensing perspectives to
identify and attribute land surface dynamics in Central Asia 2001â€“2013.Remote Sens. Environ.2015, 170, 48â€“61.
[CrossRef]
60. Wang, P .; Zhao, T.; Shi, J.; Hu, T.; Roy, A.; Qiu, Y.; Lu, H. Parameterization of the freeze/thaw discriminant
function algorithm using dense in-situ observation network data. Int. J. Digit. Earth 2019, 12, 980â€“994.
[CrossRef]
61. Zhao, S.; Zhang, L.; Zhang, Y.; Jiang, L. Microwave emission of soil freezing and thawing observed by a
truck-mounted microwave radiometer. Int. J. Remote Sens.2012, 33, 860â€“871. [CrossRef]
62. Cordisco, E.; Prigent, C.; Aires, F. Snow characterization at a global scale with passive microwave satellite
observations. J. Geophys. Res.2006, 111, D19102. [CrossRef]
63. Huang, G.; Li, X.; Huang, C.; Liu, S.; Ma, Y.; Chen, H. Representativeness errors of point-scale ground-based
solar radiation measurements in the validation of remote sensing products. Remote Sens. Environ.
2016, 181, 198â€“206. [CrossRef]
Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).
--- END OF remotesensing-12-02691-v3.pdf ---

--- START OF waf-d-20-0028.1.pdf ---
A Deep-Learning Model for Automated Detection of Intense Midlatitude Convection Using
Geostationary Satellite Images
JOHN L. CINTINEO,a MICHAEL J. PAVOLONIS,b JUSTIN M. SIEGLAFF,a ANTHONY WIMMERS,a
JASON BRUNNER,a AND WILLARD BELLONa
a Cooperative Institute of Meteorological Satellite Studies, University of Wisconsinâ€”Madison, Madison, Wisconsin
b NOAA/NESDIS/Center for Satellite Applications and Research/Advanced Satellite Products Branch, Madison, Wisconsin
(Manuscript received 19 February 2020, in ï¬nal form 7 October 2020)
ABSTRACT: Intense thunderstorms threaten life and property, impact aviation, and are a challenging forecast problem,
particularly without precipitation-sensing radar data. Trained forecasters often look for features in geostationary satellite
images such as rapid cloud growth, strong and persistent overshooting tops, U- or V-shaped patterns in storm-top tem-
perature (and associated above-anvil cirrus plumes), thermal couplets, intricate texturing in cloud albedo (e.g., â€˜â€˜bubblingâ€™â€™
cloud tops), cloud-top divergence, spatial and temporal trends in lightning, and other nuances to identify intense thun-
derstorms. In this paper, a machine-learning algorithm was employed to automatically learn and extract salient features and
patterns in geostationary satellite data for the prediction of intense convection. Namely, a convolutional neural network
(CNN) was trained on 0.64-mm reï¬‚ectance and 10.35-mm brightness temperature from the Advanced Baseline Imager
(ABI) and ï¬‚ash-extent density (FED) from the Geostationary Lightning Mapper (GLM) on boardGOES-16. Using a
training dataset consisting of over 220 000 human-labeled satellite images, the CNN learned pertinent features that are
known to be associated with intense convection and skillfully discriminated between intense and ordinary convection. The
CNN also learned a more nuanced feature associated with intense convectionâ€”strong infrared brightness temperature
gradients near cloud edges in the vicinity of the main updraft. A successive-permutation test ranked the most important
predictors as follows: 1) ABI 10.35-mm brightness temperature, 2) ABI GLM ï¬‚ash-extent density, and 3) ABI 0.64-mm
reï¬‚ectance. The CNN model can provide forecasters with quantitative information that often foreshadows the occurrence
of severe weather, day or night, over the full range of instrument-scan modes.
SIGNIFICANCE STATEMENT: Trained human forecasters are particularly adept at picking out indicators of intense
thunderstorms in weather satellite imagery. While previous algorithms have been developed to detect certain aspects of
intense thunderstorms, this research is unique as it uses deep learning to incorporate the detection of all satellite-based
features of intense thunderstorms, mimicking human pattern recognition. The model described in this research can
provide forecasters rapid guidance on evolving severe weather threats day or night, even in the absence of precipitation-
sensing weather radar.
KEYWORDS: Deep convection; Satellite observations; Neural networks; Machine learning
1. Introduction
Since the advent of weather satellites, researchers have been
investigating signatures of intense convection from satellite
images (e.g.,Purdom 1976; Adler and Fenn 1979; Menzel and
Purdom 1994; Schmit et al. 2005, 2015). Forecasters frequently
scrutinize satellite imagery to help infer storm dynamics and
diagnose and forecast the intensity of thunderstorms, which
can generate a variety of hazards. Intense convective updrafts
frequently penetrate the tropopause, resulting in overshooting
cloud tops. These features may block strong upper-level wind
ï¬‚ow, which is diverted around the overshooting tops, carrying
cloud debris from the updraft summit, resulting in U- or V-shaped
thermal couplets in infrared brightness temperature imagery (e.g.,
SetvÃ¡k et al. 2013; Wang 2007; Brunner et al. 2007). Furthermore,
high-refresh sequences of geostationary satellite images have
been used to retrieve cloud-top divergence and cloud-top vorticity
and subsequently detect supercell thunderstorms (Apke et al.
2016). Textural patterns at cloud top have also been used to infer
updraft strength (Bedka and Khlopenkov 2016). In the pres-
ence of strong upper-level ï¬‚ow, some overshoots generate
above-anvil cirrus plumes (AACP) downstream from the
overshooting top as a result of internal gravity wave breaking
and are apparent in visible satellite imagery (Wang 2003; Wang
et al. 2016; Homeyer et al. 2017; Bedka et al. 2018). AACPs in
visible imagery are responsible for cold-U features in satellite
infrared imagery, together forming a robust indicator of on-
going or imminent severe weather hazards such as large hail,
strong downburst wind gusts, and tornadoes.
Total lightning information is also known to be useful for
diagnosing and forecasting intense convection. The electrical
energy manifested in lightning ï¬‚ashes is related to the kinetic
energy and overall vigor of thunderstorm updrafts. Updrafts
provide an environment for mixed-phase precipitation pro-
cesses and a mechanism for microphysical charge transfer and
cloud-scale charge separation, generating large electrical po-
tential differences. An increasing rate of total lightning ï¬‚ashes
in a storm is often a good indicator of an intensifying convec-
tive updraft (e.g.,Schultz et al. 2011).
Corresponding author : John L. Cintineo, john.cintineo@ssec.
wisc.edu
DECEMBER 2020 C I N T I N E O E T A L . 2567
DOI: 10.1175/WAF-D-20-0028.1
/C2112020 American Meteorological Society. For information regarding reuse of this content and general copyright information, consult theAMS Copyright
Policy (www.ametsoc.org/PUBSReuseLicenses).
In severe weather warning operations, an operational fore-
caster is confronted with far more data than can be manually
analyzed. Automated methods can help forecasters manage
data overload and can provide insights that may have otherwise
gone unnoticed. While automated algorithms that identify tar-
geted features of satellite-based observations of intense convec-
tion have been successfully developed and tested (e.g.,Schultz
et al. 2011; Bedka and Khlopenkov 2016; Apke et al. 2016), no
algorithm or system has been able to integrate all of the severe
weather-pertinent features into a single product. In an effort to
simplify algorithm development andconsolidate salient satellite-
based features of thunderstorms into a single output, we utilize a
deep-learning approach that mimics expert human pattern rec-
ognition of intense convection in satellite imagery. The goal of this
approach is to quantify convective intensity automatically, saving
forecasters time in identifying, diagnosing, and prioritizing threats.
Deep learning is a branch of machine-learning methods
based on artiï¬cial neural networks with feature learning, or the
ability to automatically ï¬nd salient features in data (e.g.,
Schmidhuber 2015). Deep-learning models, such as convolu-
tional neural networks (CNN), have the ability to encode
spatiotemporal features at multiple scales and levels of abstrac-
tion with the ultimate goal of encoding features that maximize
performance (Fukushima 1980). In a fully connected neural
network, each neuron in layerk is connected to all neurons in the
adjacent layers (k 2 1a n dk 1 1), and each connection is asso-
ciated with a learned weight. The learned weights are used in
linear combinations (the value at neuronj in layerk is a linear
combination of the values at all neurons in layerk 2 1, and the
weights in the linear combination are those associated with con-
nections between neuronj and the neurons in layerk 2 1). Each
linear combination is followed by anonlinear activation function,
which allows the network to learn nonlinear relationships.
In a CNN, the neurons in each layer are arranged in a spatial
grid, with the same number of dimensions as the input data
(e.g., if the input data are 2D satellite grids, the neurons in each
layer are in a 2D grid as well). Each neuron has a â€˜â€˜receptive
ï¬eld,â€™â€™ the subdomain of the previous layer to which it is con-
nected. In general, the subdomain is smaller than the entire
domain. That is, neuron j in layer k is connected only to
neurons in adjacent layers (k 2 1 andk 1 1) that are in the
same spatial neighborhood. In this work, neurons in the initial
input layer are pixels of an image. Deep-learning models have
yielded excellent performance on image recognition tasks for
nonmeteorological phenomena (e.g.,Krizhevsky et al. 2012;
Litjens et al. 2017; Li et al. 2018) and we seek to apply such
methods to weather satellite imagery.
There has already been success with deep-learning methods
for synoptic-scale front detection (Lagerquist et al. 2019), hail
size estimation in numerical weather prediction (NWP) model
output (Gagne et al. 2019), and tornado prediction (Lagerquist
et al. 2020). To the authorsâ€™ knowledge, this is the ï¬rst appli-
cation of deep learning on weather satellite imagery targeting
convection intensity of individual thunderstorms. In this
paper, a CNN model was trained in a supervised manner to
generate an â€˜â€˜intense convection probabilityâ€™â€™ (ICP). One
beneï¬t of CNNs, and deep learning in general, is the greatly
reduced need for feature engineering (i.e., turning gridded data
into scalar features or predictors in machine-learning models),
which can be analytically challenging, difï¬cult to optimize for
predictive skill, and lacking in formalized evaluation tools. This not
only saves considerable time but makes the model more objective
by not superimposing scientistsâ€™ preconceived notions of what
features are important in an image, which also presents an op-
portunity to learn new insights into physical phenomena. The
model learns from the training data the salient spatiotemporal
features that result in the best ï¬t, using a numerical optimization
process called backpropagation (Goodfellow et al. 2016). After
discussing the construction of the CNN, we characterize its per-
formance and show that the model learned several features,
including a number of features that human experts most often
associate with intense convection, as well as a lesser-known feature.
2. Data and methods
a. Meteorological data
GOES-16 Advanced Baseline Imager (ABI;Schmit et al.
2005) and Geostationary Lightning Mapper (GLM;Rudlosky
et al. 2019; Goodman et al. 2013) radiance and ï¬‚ash data were
collected for 29 convectively active days in the Mayâ€“July 2018
TABLE 1. Dates, sample size, and fraction of sample that is labeled as â€˜â€˜intenseâ€™â€™ convection, for the training, validation, and test datasets.
Training Validation Test
Dates 6â€“8 May 2018 1 May 2018 10 May 2018
11 May 2018 2 May 2018 23 Jun 2018
13â€“15 May 2018 3 May 2018 2 Jul 2018
18â€“19 May 2018 4 May 2018
29 May 2018 5 May 2018
31 Mayâ€“1 Jun 2018 14 Jun 2018
8 Jun 2018 15 Jun 2018
11 Jun 2018 20 Jul 2018
17 Jun 2018
19 Jun 2018
9 Jul 2018
29 Jul 2018
Sample size 153 364 51 178 18 329
â€˜â€˜Intenseâ€™â€™ class fraction 10.1% 11.6% 14.2%
2568 WEATHER AND FORECASTING V OLUME 35
timeframe (Table 1). Dates were selected to include a variety
of satellite viewing angles and geography (seeFig. 1), each
representing a â€˜â€˜convective dayâ€™â€™ from 1200 UTC of the listed
date to 1200 UTC of the next date. The channel 2 (CH02)
0.64-mm reï¬‚ectance
1 and channel 13 (CH13) 10.35-mmb r i g h t -
ness temperature were calculated using attributes from ABI
Level-1b ï¬les for all ABI contiguous U.S. (CONUS) sector
ï¬les for each convective date (288 ï¬les per day, or 5-min
temporal resolution). Please seeTable 2for a summary of the
raw input datasets. This paper focuses on these two ABI
channels since operational forecasters most often use them
to analyze developing and mature convection in satellite
data. Future work may assess the impact of other ABI chan-
nels. GLM Level-2 ï¬les, which contain lightning ï¬‚ash, group,
and event point data, were processed with an open-source
software package called glmtools (Bruning 2019). This soft-
ware package was used to create gridded ï¬elds for several
GLM attributes: ï¬‚ash-extent density (FED), ï¬‚ash-centroid
density (FCD), total optical energy (TOE), and average ï¬‚ash
area (AFA); please see the example imagery inFig. 2. The
GLM ï¬elds were created for theGOES-16 ABI CONUS sec-
tor geostationary projection at 2-km spatial resolution.
2
To locate thunderstorms in the training data, NOAA/CIMSS
ProbSevere ï¬les were used. ProbSevere is a machine-learning
nowcasting system in the United States for severe weather
using radar, lightning, sat ellite, and NWP data as inputs
(Cintineo et al. 2020). The ProbSevere ï¬les include the cen-
troid time and latitude/longitude of radar-identiï¬ed convec-
tive cells every 2 min. The ProbSevere thunderstorm objects
are based on Multi-Radar Multi-Sensor (MRMS) system
imagery (Smith et al. 2016). A â€˜â€˜thunderstormâ€™â€™ is deï¬ned as a
convective cell that was successfully tracked for at least 45 min
by the automated procedure utilized by ProbSevere (e.g.,
Cintineo et al. 2014). The thunderstorm must also have had a
ï¬‚ash rate of 2 ï¬‚ashes per minute or greater (the ï¬‚ash rate is the
sum of the ï¬‚ashes within the object polygon) at some point
during the automated tracking period, as inferred from the
Earth Networks Total Lightning dataset used by ProbSevere
(e.g., Cintineo et al. 2018). The radar object centroid time and
location were used to automatically generate ;64 km 3
64 km
3 storm-centered image patches from the ABI and
GLM CONUS sector data (at 5-min temporal resolution),
resulting in 222 854 image patches from 14 745 different
storms. Severe hail, wind, and tornado reports were also gath-
ered from NOAAâ€™s Storm Events Database (NOAA 2019)a n d
linked to the storm-image patches via ProbSevere radar objects
(e.g., Cintineo et al. 2020). Reports were linked to ProbSevere
objects using a62-min search window around the report time,
with each report being associated with the closest radar object
centroid within the temporal search window. The severe reports
were not used in labeling or validation, but simply as a way to
characterize the dataset (seesection 2b) and estimate potential
lead time (seesection 3a).
b. Data labeling and partitioning
The GOES-16 image patches were generated for each ABI
channel and GLM product listed above. An image patch size of
64 km 3 64 km was heuristically chosen to represent the
â€˜â€˜storm scale.â€™â€™ The 64 km3 64 km domain was the same for all
channels and resulted in 1283 128 pixel images for ABI CH02
and 32 3 32 pixel images for ABI CH13 and the GLM
channels.
FIG. 1. All 14 746 thunderstorm tracks for the training, validation, and testing datasets. The
different colors are used to help distinguish individual storm tracks.
1 This is the reï¬‚ectance factor that is not corrected for solar zenith
angle. The paper refers to this as â€˜â€˜reï¬‚ectanceâ€™â€™ throughout, for short.
2 The GLM regridding was performed at 2 km in this paper to
ensure the ï¬nal grids conveyed the full shape/size of the original
GLM pixels as they aligned to the ABI ï¬xed grid.
3 The patch size in kilometers is approximate, as the ABI channel
spatial resolutions are nominal and valid at the satellite subpoint.
The actual spatial resolution decreases away from the subpoint.
DECEMBER 2020 C I N T I N E O E T A L . 2569
Images created for ABI CH13 brightness temperature and
CH02 reï¬‚ectance enabled manual storm labeling.4 Human
experts performed the labeling using a custom tool built on the
React Javascript library (seeFig. 3for an example). The ex-
perts (three of the coauthors of this paper) labeled all images as
either â€˜â€˜intenseâ€™â€™ (22 505 images) or â€˜â€˜ordinaryâ€™â€™ (200 366 im-
ages) convection based on the presence, or lack thereof, of
features within the patch widely accepted as being associated
with strong midlatitude convection (e.g., overshooting tops,
cold-U, cloud-top divergence, AACP, high visible texture).
MRMS merged composite reï¬‚ectivity (MergedRef; a col-
umn maximum of radar reï¬‚ectivity at each horizontal grid
point G, based on nearby radars observing pointG) was also
contoured over the images to provide extra context for humans
labeling the intensity of a storm, but the CNN only utilizes
GOES-16 satellite data. In the absence of a clear satellite in-
dicator for intense convection, we looked for corresponding
strong reï¬‚ectivity cores (50â€“601dBZ), giving careful attention
not to consider MergedRef too highly when radar beam
blockage was present, or the storm was on the edge of MRMS
domain. Label selections were linked to a database for expe-
dient cataloging of the dataset. While these images were useful
for labeling, they were not the same images used to train the
CNNâ€”the actual ABI and GLM numerical-data patches were
stored in separate ï¬les.
Based on the National Weather Service (NWS)-deï¬ned se-
vere criteria of hail diameter$ 1 in. (25.4 mm), wind gust$
50 kt (25.72 m s
21), or the presence of a tornado, 55.5% of the
intense class images were from severe storms (irrespective of
when a severe report occurred), while only 5.6% of the or-
dinary class images were from severe storms. This analysis
conï¬rms that storms that exhibit one or more of the storm
top features (e.g., overshooting tops, cold-U, cloud-top di-
vergence, AACP, high visible texture) targeted by the hu-
man experts are much more likely to produce veriï¬ed severe
weather than storms where such clearly deï¬ned features
were absent.
The 29 days of labeled data were divided into three groupsâ€”
training, validation, and testing. The groups consisted of 18
(70%), 8 (22%), and 3 (8%) days, respectively (Table 1). The
proportion of intense-labeled storms was 10.1%, 11.6%, and
14.2%, for the training, validation and testing sets, respec-
tively. An independent set of dates, as opposed to a random
method, was used to minimize collinearity between images in
each group. In machine learning, the training set is the sample
of data used to ï¬t the model. This is how the model learns and
encodes spatial features, using backpropagation to minimize
the loss function in the training set. Backpropagation computes
the gradient of the loss function with respect to each weight of
the CNN. The validation set is used to provide an independent
assessment of a trained model, which is useful in selecting
hyperparameters (see section 2d). However, by choosing hy-
perparameter values that optimize performance on the vali-
dation set, the hyperparameters can be overï¬t to the validation
set, just like model weights (those adjusted by training) can be
overï¬t to the training set. Thus, the selected model is also
evaluated on the testing set, which is independent of the data
used to ï¬t both the model weights and hyperparameters.
c. Model architecture
CNNs use a multilayered architecture to learn spatial fea-
tures (e.g.,Fig. 4). This architecture is typically broken down
into three fundamental types of layers: convolutional layers,
pooling layers, and fully connected layers. The convolutional
and pooling layers turn input into â€˜â€˜feature maps,â€™â€™ or trans-
formations of the data. The â€˜â€˜mapsâ€™â€™ received by the ï¬rst con-
volutional layer are the ABI and GLM image patches. Maps
received by deeper layers have been transformed by one or
more convolutional ï¬lters and activations, creating abstrac-
tions of the data.
Convolution is formally deï¬ned by Eq. (4) inLagerquist
et al. (2019), which operates spatially and in a multivariate
fashion on a set of input grids, encoding spatial patterns that
combine the input variables. Each convolutional ï¬lter in the
model has a different set of weights, which are initialized
randomly. Activation is a nonlinear function applied to the
feature maps after each convolutional layer, elementwise. The
activations are an important step, as a CNN would only learn
linear relationships in the data without applying the activa-
tions. The nonlinear activation applied after every convolu-
tional layer in this model is the rectiï¬ed linear unit, ReLU
(Nair and Hinton 2010).
After two sets of convolutions and activations, pooling
layers are applied, which downsample each feature map
TABLE 2. Summary of raw input data. The horizontal spacing values are for theGOES-16 satellite subpoint (the point on Earth directly
below the satellite; on the equator at 758W). Abbreviations: Advanced Baseline Imager (ABI), Geostationary Lightning Mapper (GLM),
ï¬‚ash-extent density (FED), ï¬‚ash-centroid density, (FCD), total optical energy (TOE), and average ï¬‚ash area (AFA).
Dataset Time step Horizontal spacing
GOES-16 ABI 0.64-mm reï¬‚ectance 5 min 2 km
GOES-16 ABI 10.35-mm brightness temperature 5 min 0.5 km
GOES-16 GLM FED, FCD, TOE, AFA 5 min 2 km
4 The manual labeling of small storm-centric image patches was
elected in this work, rather than an automated pixel-by-pixel la-
beling of larger images [often used in semantic segmentation; e.g.,
Ronneberger et al. (2015)] or automated image-patch labeling,
because of uncertainty or inconsistency in radar and reports-based
datasets. While pixel-by-pixel manual labeling is possible, it is more
labor intensive than assigning one label per small image patch (the
approach of this paper).
2570 WEATHER AND FORECASTING V OLUME 35
independently (e.g., Li et al. 2020). The model of this paper
uses a maximum ï¬lter with a window size of 23 2 pixels,
halving the spatial resolution for each pooling (these have become
fairly standard choices). This pooling operation enables deeper
convolutional layers to learn larger-scale features in the data and
helps the model become invariant to small spatial translation in
the inputs. The series of convolutions, nonlinear activations, and
pooling operations allow the model to learn higher-level ab-
stractions at deeper layers of the network.
After a series of convolution, activation, and pooling layers, the
feature maps of the network are ï¬‚attened into a 1D vector and
passed to a series of fully connected layers to create the ï¬nal pre-
dictions. The model of this paper uses ReLU for the activations of
the ï¬rst two fully connected layers and uses the sigmoid function for
the activation of the ï¬nal fully connected layer with a single output,
forcing the ï¬nal prediction tobe a probability between [0, 1].
We used the Keras Python API with TensorFlow backend to
perform the training and evaluation of CNNs (Chollet 2015).
This is a binary classiï¬cation problem (â€˜â€˜intenseâ€™â€™ or â€˜â€˜ordinaryâ€™â€™
convection are the classes), so the loss function chosen to min-
imize was binary cross-entropy [Eq.(1)]. The termp
i is the
predicted probability of intense convection,yi is the label (1 if
intense, 0 otherwise) for theith example,N is the number of
examples, andÂ« is the binary cross-entropy, ranging from [0,â€˜):
Â« 52 1
N /C229
N
i51
[yi log(pi) 1(1 2yi) log(12pi)]. (1)
d. Hyperparameter tuning
A hyperparameter is a parameter whose value is set before
the learning process begins for training a CNN. There are many
FIG.2 . GOES-16 ABI and GLM imagery of a supercell in central Texas. ABI 0.64-mm background image
overlaid with a (top left) semitransparent 10.3-mm brightness temperature image, (top right) GLM ï¬‚ash-extent
density, (bottom left) GLM total optical energy, and (bottom right) GLM average ï¬‚ash area.
DECEMBER 2020 C I N T I N E O E T A L . 2571
design components to creating a CNN, including the number
and types of layers, convolutional ï¬lter size, the number of
convolutional ï¬lters, regularization techniques, image padding
techniques, learning rate, mini batch size (the amount of
samples the network sees before a weight change is made),
activation function, image patch sizes, the number of epochs
(passes through the data), and others, not to mention different
combinations of input predictors. While many general-purpose
CNN architectures exist (e.g., ResNet [He et al. 2016]), we
found that starting simple and iteratively building a CNN
FIG. 3. Example images in the tool that was used to create the labeled dataset of â€˜â€˜intenseâ€™â€™ and â€˜â€˜ordinaryâ€™â€™
convection classes. A different version of these images, without overlays, text, and color bars, was used for training,
validation, and testing. Contours are NEXRAD reï¬‚ectivity from the Multi-Radar Multi-Sensor (MRMS) system.
The 30- (cyan), 40- (yellow), 50- (magenta), and 60-dBZ (brown) reï¬‚ectivity contours are shown. The human-
assigned labels are uploaded to a database.
FIG. 4. Schematic of the convolutional neural network described in this paper. The blue boxes and pink pyramids represent 33 3 pixel
convolutional ï¬lters acting over the feature maps (or input images, initially). The dimensions for the gray boxes indicate the horizontal
dimensions (the equal dimensions in each box) and the number of feature maps after 2D-convolutional and maximum pooling layers are
applied (or the number of input grids, initially). In the case where one dimension is present, it is the length of the 1D vector. After several
blocks of convolutions and poolings, the encoded ABI and GLM features are then â€˜â€˜ï¬‚attened,â€™â€™ or made into a 1D vector, and concat-
enated with the scalar input vector. The concatenated vector is processed through several fully connected layers to generate a probability
of intense convection. This image was created athttp://alexlenail.me/NN-SVG/AlexNet.html.
2572 WEATHER AND FORECASTING V OLUME 35
worked best for this problem (e.g., iteratively adding blocks of
2D convolution1 pooling layers and other components until
performance on the validation data decreased) as opposed to
using a more sophisticated architecture. Given the inï¬nite
number of CNN hyperparameter combinations, our proposed
architecture is perhaps suboptimal, but works well in practice
(see Table 3for the ï¬nal hyperparameter conï¬guration).
The criterion used to attempt to optimize hyperparameters
was the maximum critical success index (CSI) of the validation
set. The CSI is the ratio of true positives (â€˜â€˜hitsâ€™â€™) to the sum of
true positives, false positives (â€˜â€˜false alarmsâ€™â€™), and false negatives
(â€˜â€˜missesâ€™â€™) for a given probability threshold. It is bound between
[0, 1], with 1 representing perfectskill. It is an excellent metric for
rare-occurring classes, since it does not reward true negatives. The
hyperparameters we attempted to optimize were the: 1) number
of convolutional layers, 2) convolutional ï¬lter size, 3) number of
convolutional ï¬lters in the initial convolutional layer, 4) applica-
tion of the dropout operation to the fully connected layers
(Hinton et al. 2012), 5) application of batch normalization to the
convolutional layers, 6) application ofL
2 regularization (Hoerl
and Kennard 2000), and 7) inclusion of multiple GLM ï¬elds.
Hyperparameter changes that improved the maximum validation
CSI (by at least 0.0025) were included in the ï¬nal model archi-
tecture. There is one CSI per probability threshold, so the prob-
ability threshold with the maximum CSI was chosen.
Batch normalization (Ioffe and Szegedy 2015) is applied el-
ementwise to each of the feature maps to mitigate the inherent
vanishing-gradient problem (seeSchmidhuber 2015, section 5.9)
in neural networks and speed up learning. However, it did not
improve the maximum CSI. TheL
2 regularization adds a weight
term to the loss function [Eq.(1)] that is the sum of the squared
weights in all convolutional layers multiplied by the parameterl.
This method is meant to help the model learn smaller weights
and become less sensitive to small changes in the input predic-
tors (i.e., make it more stable to small changes). The values for
l tested were 10
24,1 023,a n d1 022, yet none improved the val-
idation set CSI, perhaps because dropout in the fully connected
layers provided sufï¬cient regularization. Some ï¬ndings that did
improve model performance included:
(i) The 3 3 3 convolutional ï¬lters were better than 53 5
ï¬lters. The smaller ï¬lters may allow the network to learn
features at the ï¬nest scale in the ï¬rst convolutional layer,
then larger-scale features in deeper layers after pooling
has been applied.
(ii) Eight convolutional ï¬lters for the initial convolutional
layer were better than 16, 32, or 64. This may be because it
reduces the number of weights in the CNN, leading to
faster convergence.
(iii) Two convolutional layers per block were better than one
or three, as two layers allowed the network to learn more
complex abstractions at each spatial scale before pooling,
whereas one layer did not allow this and three layers per
block led to too many weights.
(iv) Dropout applied to the fully connected layers was better
than no dropout. Dropout randomly zeroes out fractionF
of a layerâ€™s outputs, whereF is the dropout rate. This is
meant to force weights in layerL (with dropout) learn
more independently of other weights in layer L and
reduce overï¬tting to the training data. The fractions tested
were 0, 0.3, and 0.5 (there was little difference in perfor-
mance between 0.3 and 0.5). The dropout likely prevented
overï¬tting.
Upon testing various CNN inputs, we found that ABI
CH02 reï¬‚ectance (0.64 mm), CH13 brightness temperature
(10.35 mm), and GLM FED, along with the scalar values of
satellite-zenith angle, solar-zenith angle, latitude, and longitude,
provided the best performance in discerning intense convection
in the validation dataset. The inclusion of the TOE, FCD, and
AFA from the GLM did not improve performance on the vali-
dation dataset. The ABI channels were jointly processed
through a set of six convolution and maximum pooling blocks,
whereas the FED was processed through a separate set of four
convolutional and maximum pooling blocks. The ABI and GLM
convolutional bases were then joined with the scalar data and
connected to three fully connected layers with 128, 16, and 1
node(s) (Fig. 4). Future work will examine if metrics that are
derived from two or more ABI channels (e.g., brightness tem-
perature differences, reï¬‚ectance ratios, etc.) and/or time series
can be used to improve model performance.
One somewhat unique aspect of this model is the combina-
tion of two convolutional bases. Initially, FED, CH02 reï¬‚ec-
tance, and CH13 brightness temperature were separate channels
TABLE 3. Select hyperparameters used for the training of the convolutional neural network.
Hyperparameter Value
Loss function Binary cross-entropy
Learning rate 0.01; reduced by 90% if no improvement in validation loss after 2 epochs
Total number of epochs 14 (early stopping if no loss improvement after 6 epochs)
Batches per epoch 511
Examples per batch 300
Filter window 3 3 3 pixels for each Conv2D ï¬lter
Optimizer Rectiï¬ed Adam (RAdam)
Dropout ratio 50% (used for ï¬rst two fully connected layers only)
Nonlinear activation Rectiï¬ed linear unit (ReLU) for all convolutional layers and ï¬rst two fully connected layers; sigmoid
for ï¬nal fully connected layer.
Padding Feature maps are zero-padded such that the size of the output feature maps is the same as the size of
the input feature maps
Graphics processing unit One NVIDIA TITAN V
D
ECEMBER 2020 C I N T I N E O E T A L . 2573
in one convolutional base (one stack of convolutional and
pooling layers), having upsampled FED and CH13brightness
temperature to 0.5-km horizontal grid spacing. The single
convolutional base formulation performed poorly compared
to a model that excluded GLM. However, when the GLM in-
put was processed using a separate stack of convolutional and
pooling layers, the ABI1 GLM model performance notice-
ably improved relative to an ABI-only model (maximum CSI
improved by 0.035, or 6.3%). This outcome illustrates that care
must be taken when utilizing images from multiple data
sources.
e. Model evaluation and interpretation
1) STATISTICAL VERIFICATION
Standard performance metrics were computed separately
for the validation and testing labeled data partitions (i.e., the
64 km3 64 km image patches). The computed metrics include
accuracy [Eq. (5)], CSI [Eq. (6)], frequency bias [Eq. (7)],
Peirce score (PS) [Eq.(8)], Brier skill score (BSS), and the area
under the receiver operating characteristic curve [area under
the ROC curve (AUC;Metz 1978]. The ROC curve, perfor-
mance diagram, and attributes diagram are also presented:
POD 5 TP
TP 1FN, (2)
POFD 5 FP
FP 1TN, (3)
FAR 5 FP
FP 1TP, (4)
accuracy 5 TP 1TN
TP 1FP 1FN 1TN, (5)
CSI 5 TP
TP 1FP 1FN, (6)
bias 5 TP 1FP
TP 1FN, (7)
PS 5POD 2POFD; (8)
success ratio51 2FAR: (9)
In Eqs.(2)â€“(7), TP is the number of true positives, TN is the
number of true negatives, FP is the number of false positives,
and FN is the number of false negatives, deï¬ned based on a
probability threshold. The CNN produces a single prediction
for each image patch, and the probability threshold binarizes
the prediction into the â€˜â€˜yesâ€™â€™ and â€˜â€˜noâ€™â€™ classes (i.e., for prob-
ability thresholdp*, and predictionp, p $ p* becomes â€˜â€˜yes,â€™â€™
and p , p* becomes â€˜â€˜noâ€™â€™). POD is the â€˜â€˜probability of
detection,â€™â€™ â€˜â€˜hit rate,â€™â€™ â€˜â€˜true positive rate,â€™â€™ or â€˜â€˜recall.â€™â€™ POFD is
the â€˜â€˜probability of false detectionâ€™â€™ or â€˜â€˜false positive rate.â€™â€™ FAR
is the â€˜â€˜false alarm ratioâ€™â€™ or â€˜â€˜false discovery rate.â€™â€™
The accuracy simply measures how well a given probability
threshold is able to discriminate between intense and ordinary
convection. It ranges between [0, 1], with 1 being perfectly
accurate. The training dataset consists of 10.1% intense-
labeled samples, so accuracy can be trivially optimized to
0.899 by always predicting â€˜â€˜no.â€™â€™ The CSI is accuracy without
correct nulls or TNs, and ranges between [0, 1], with 1 being
perfect. The frequency bias ranges from [0,â€˜), with 1 being
perfectly unbiased, values. 1 meaning that the intense label is
predicted more often than it occurs, and values, 1 meaning
that the intense label is predicted less often than it occurs. The
Peirce score [Eq. (8)] is the POD minus the POFD, which
ranges from [21, 1], with 1 being perfect, 0 indicating no skill,
and 21 indicating a POD50 and a POFD51 (i.e., no TPs and
the maximum amount of FPs for a given probability threshold).
The BSS [Eq.(10)] examines the Brier score (BS) of the
model versus a reference Brier score, which is Eq.(11) evalu-
ated withf
t equal to the frequency of the intense class in the
training data. The BSS ranges between (2â€˜,1], with 1 being
perfect and 0 indicating no skill compared to the reference
Brier score, while decreasing values (toward 2â€˜) indicate
deterioration of skill compared to the reference Brier score:
BSS 51 2 BS
BSreference
. (10)
The Brier score itself measures the mean squared probability
error [Eq.(11)]:
BS 5 1
N /C229
N
t51
(ft 2ot)2 , (11)
where ft is the probability that was forecast,ot is the actual
outcome of the event at instancet (0 if ordinary and 1 if in-
tense) andN is the number of forecasts. The Brier score ranges
from [0,1] with a perfect score being 0. While the BS and BSS
combine all probability forecasts to compute scalar metrics of
skill conditioned on the forecasts, an attributes diagram (Hsu
and Murphy 1986) provides this information per probability
bin. In this way, users can see which forecast probabilities are
well calibrated and which are not.
The ROC curve plots the POD versus the POFD, from
which the area under the curve can be computed. The ROC
curve examines how well the model does at distinguishing
between classes (15 perfect separation; 0.55 no separation; a
random model) and is not sensitive to poorly calibrated model
predictions. Whereas the accuracy, bias, CSI, and PS are
computed on a single probability threshold, the AUC is inte-
grated over all probability thresholds, giving a more holistic
characterization of model performance.
The performance diagram plots the POD versus the success
ratio [Eq.(9)] for different probability thresholds, with greater
CSI in the top-right corner, corresponding to greater POD and
greater success ratio (or lesser FAR). For the ROC curve and
the performance diagram, each point corresponds to one
probability threshold.
2) INTENSE CONVECTION PROBABILITY GRIDS
In addition to the statistical metrics described in the previous
paragraph, intense convection probability grids were created
for a number of additional independent scenes from 2019 (all
of the training, validation, and testing data were from 2018). To
create the ICP grids, a sliding-window approach was used.
Within each scene, moving in both the latitudinal and longi-
tudinal directions, a 64 km364 km window was used to extract
2574 WEATHER AND FORECASTING V OLUME 35
the ABI and GLM data patches. The stride of the movement
for the sliding window was four 2-km ABI pixels. This creates
an oversampling of predicted probabilities, with one value
every 8 km, whereas the model was trained on storm-centric
64 km 3 64 km patches. Contours of selected probability
thresholds were then derived from the resulting grid of ICP and
subsequently overlaid on the corresponding ABI imagery.
Because the model was trained on storm-centric patches, it also
learned the parallax
5 relationship between the satellite data
and radar-identiï¬ed storms. This is evident in small displace-
ments of ICP contours to the south and east of the highlighted
storms, which can be observed at the higher satellite viewing
angles in storms (as shown inFigs. 13and 14). The ICP con-
tours can also include sections of the storm that may not be
intense, which is due to the fact that each 64 km3 64 km patch
generates a single probability; that is, the probability is repre-
sentative for the entire patch. It should be noted that the ver-
iï¬cation metrics mentioned insection 2e(1) were computed
only for the 64 km3 64 km images of the validation and testing
datasets, not for the ICP grids, which would require truth labels
at every pixel. Thus, the ICP grids provide a more qualitative
yet visual aspect of veriï¬cation. Nevertheless, this sliding-
window approach is one possible technique enabling predic-
tions that would not require a radar network.
3) SALIENCY MAPS AND LAYER-WISE RELEVANCE
PROPAGATION
Saliency maps (Simonyan et al. 2014; McGovern et al. 2019)
and relevance maps (Binder et al. 2016) are another form of
model analysis utilized. The objective is to identify the spatial
features within each input image that most inï¬‚uence the model
results. The saliency of predictorx at image coordinate (i, j) with
respect to the intense-convection predictionp,i s â€ºp/[â€ºx(i, j)].
Saliency uses backpropagation to determine how changes in
each x(i, j) impact the modelâ€™s output probability. One disad-
vantage is that it is a linear approximation around x(i,j),
meaning the saliency indicates how the model prediction
changes whenx is perturbed only slightly. It can be both positive
and negative. As a complement to saliency, layer-wise relevance
propagation (LRP;Alber et al. 2019) is a framework that also
uses backpropagation to identify the most relevant or important
pixels; that is, the pixels that contribute the most to a given
prediction. Relevance (like saliency) indicates how much each
predictor contributes to the positive class only for the hyper-
parameters we chose (seesection 3c). One other important dif-
ference between saliency and relevance is that saliency indicates
which predictors are most important for changing the prediction,
while relevance signals which predictors (or regions of those
predictors) are most important for the prediction actually made.
4) PERMUTATION TESTS
Finally, in an effort to rank predictor importance, two per-
mutation tests were applied to the trained CNN:Breiman
(2001, hereafterB01) andLakshmanan et al. (2015, hereafter
L15). In B01, samples are permuted (or randomized) one
predictor at a time; the computed loss in performance is
recorded and compared to the performance on unpermuted
data; each predictor is returned to its unpermuted state before
the next predictor is permuted. The permutations occur by
shufï¬‚ing spatial maps of a predictorx across examples, so
that after permutation, each example is matched to the
wrong map ofx, but the correct maps of all other predictors.
This removes the statistical linkage between the permuted
predictor x and the output classiï¬cation. After each pre-
dictor is permuted individually, the most important pre-
dictor is the one which decreased the performance the most
(i.e., incurred the highest â€˜â€˜costâ€™â€™).
The method ofL15 carries B01 method a step further, by
executing successive permutations. It ranks predictor impor-
tance in this way:
(i) The most important predictor (rank ofk 5 1) is obtained
by using the permutation method ofB01.
(ii) Given thek most important predictors, the (k 1 1)th most
important predictor can be found by keeping the k
predictor(s) permuted and permuting each of the remain-
ing predictors, one at a time. The predictor that results in the
greatest loss in skill carries the (k 1 1)th rank and remains
permuted for the remainder of the test.
If performance diminishes appreciably when a predictor is
permuted, this indicates that the predictor is important. If
performance does not decline appreciably, the predictor is ei-
ther unimportant or some information in the predictor is re-
dundant with information contained in other predictors. The
L15 method helps discern correlated predictors. For example,
for two very important and highly correlated predictors,x
1 and
x2, theB01 method may rank them both as unimportant (rel-
ative to other predictors), since permuting only one destroys
very little information. Oncex
1 has been permanently per-
muted in theL15 method, x2 should immediately be considered
important, since the redundant information inx1 has been re-
moved by permutation. However, there is a chance that this
may not happen until later iterations of theL15 algorithm,
causing neitherx
1 nor x2 to be considered as highly important
relative to other predictors.
3. Results
a. Veriï¬cation metrics
The scalar evaluation metrics are summarized for the validation
and testing datasets inFigs. 5and 6, respectively. The probability
threshold used for both datasets was the threshold that maximized
CSI on the validation data, which was 51%. The statistical eval-
uation was also partitioned into â€˜â€˜dayâ€™â€™ and â€˜â€˜nightâ€™â€™ using a solar-
zenith angle threshold of 858; â€˜â€˜dayâ€™â€™ is solar-zenith angle less than
or equal to 858, and â€˜â€˜nightâ€™â€™ is solar-zenith angle greater than 858.
The model CSI was greater at night, which may be a result of the
fact that there is more mature convection present at night, which
may be easier for the model to distinguish, even in the absence of
the CH02 reï¬‚ectance. This may be a result of reduced GLM de-
tection efï¬ciency during the daytime, as well.
5 Parallax is a displacement in the apparent position of clouds
viewed along two different lines of sight (in this case, lines of sight
from the geostationary satellite and the ground-based radar).
DECEMBER 2020 C I N T I N E O E T A L . 2575
For the entire validation dataset (combined night and day),
the ROC curve shows an inï¬‚ection point at POFD5 0.10 and
POD 5 0.95 with a Peirce score. 0.8 (Fig. 7), while the per-
formance diagram (Fig. 8) shows a maximum CSI of 0.59 and
bias of 1.01 at ICP threshold5 51%. The attributes diagram in
Fig. 9shows that the model is generally well-calibrated for the
validation data, but the CNN exhibits some overforecasting
bias between the 40% and 90% probability bins (i.e., the fre-
quency of events in these probability ranges is less than the
forecast probability). The testing dataset predictions, while
skillful, exhibit a very large underforecasting bias. It is un-
known why there is a large difference in the calibration be-
tween the validation and testing datasets. This is possibly due
to differing frequencies of certain storm morphologies in the
two datasets (e.g., supercells, linear storms), but more work is
needed to discern if that is the case.
Since severe weather reports were linked to the storm-image
patches in the testing dataset, a lead time analysis was per-
formed to assess the ICPâ€™s potential to provide an alert prior to
the occurrence of severe weather. For the three days in the
testing set, 318 independent storms produced severe reports.
Lead time to the initial severe report was measured in minutes
from the ï¬rst occurrence of the 50% and 90% ICP thresholds.
Of the 318 storms, 153 reached 50% and 126 reached 90% prior
to the initial report. Using the bootstrapping technique (Efron
and Tibshirani 1986) 5000 times, 95% conï¬dence intervals for
the median lead time were created for each ICP threshold. At
the 50% ICP threshold, the median lead time to the initial
severe report was 24 min (95% conï¬dence interval: 18â€“30 min);
at the 90% ICP threshold, the median lead time was 21 min
(95% conï¬dence interval: 18â€“26 min). While a limited sample,
these numbers are comparable to the lead times to initial se-
vere weather reports recorded inBedka et al. (2018)when
measured from AACP occurrence.
b. Intense convection probability grids
ICP grids (Figs. 10â€“14) were created for several independent
scenes, using the method insection 2e. The contours created
from the grids, when overlaid on ABI imagery, can provide
insight on model performance and may be an effective way to
visualize results for eventual users of the product. The selected
independent cases encompass a range of meteorological condi-
tions, geography, and satellite viewing angles. Additional cases,
with animations, are available on the Cooperative Institute for
Meteorological Satellite Studies (CIMSS) Satellite Blog (Cintineo
2019). The background ABI image utilized in the ICP grids is the
0.64-mm reï¬‚ectance and 10.35- mm brightness temperature
FIG. 5. Summary of scalar veriï¬cation metrics for the validation
data, partitioned by time of day. â€˜â€˜Dayâ€™â€™ signiï¬es the part of the
sample with a solar-zenith angle# 858, whereas â€˜â€˜Nightâ€™â€™ signiï¬es
the part of the sample with a solar-zenith angle. 858. â€˜â€˜Allâ€™â€™ is for
the entire sample. These metrics were computed based on a
probability threshold of 51% (which maximized validation data
CSI). Black bars are 95% conï¬dence intervals determined by
bootstrapping 1000 times.
FIG.6 .A si nFig. 5, but for the testing data. The metrics were
computed based on a probability threshold of 51% (which maxi-
mized validation data CSI).
FIG. 7. ROC curve and Peirce score for the validation and testing
datasets (solid red and dashed orange lines, respectively). Red and
orange circles represent the locations of select probability values
(5%, every 10% from 10% to 90%, and 95%). Python code from
Lagerquist and Gagne (2019)was used to help create the plot.
2576 WEATHER AND FORECASTING V OLUME 35
â€˜â€˜sandwichâ€™â€™ product (unless otherwise stated). Sandwich im-
age composites are created by stacking the reï¬‚ectance and
brightness temperature images with transparency and brightness
adjustments, which allows human experts to simultaneously ex-
tract textural information from the visible reï¬‚ectance image and
temperature information from the infrared image (ValachovÃ¡
and SetvÃ¡k 2017). All background images are fromGOES-16
CONUS scans, unless stated otherwise. Reports are plotted
on a given image if the time of occurrence is within 60 min after
the start of the ABI scan.Figure 10f annotates examples
of overshooting tops, the cold-U, and AACP signatures.
However, we recommendHomeyer et al. (2017)and Bedka
et al. (2018)to readers who desire to become more familiar
with the visual identiï¬cation of these phenomena.
1) WYOMINGâ€”10 S EPTEMBER 2019
Figure 10 shows convective storm development in eastern
Wyoming between 1941 and 2301 UTC 10 September 2019. At
1941 UTC (Fig. 10a), the 50% ICP value is exceeded in the
vicinity of overshooting tops. By 2011 UTC (Fig. 10b), the anvil
cloud has expanded greatly, overshooting tops are still present,
strong brightness temperature gradients are evident on the
anvil edge, the ICP is$90% for much of the cloud, and hail and
tornado reports are imminent. At 2236 UTC (Fig. 10d), the
storm approaching the Nebraska border has an ICP$90% and
is associated with more tornado reports. A developing storm
with ICP$ 50% is present in the southwest part of the domain
at 2236 UTC, which will also turn tornadic. At 2246 UTC
(Fig. 10e), the storm to the southwest continues to develop with
noticeable overshooting tops. By 2301 UTC (Fig. 10f), the
southwestern storm attains an ICP$ 90%, while the storm to
its northeast still exhibited overshooting top/cold-U/AACP
features and ICP$ 90%.
2) MISSOURIâ€”26 A UGUST 2019
On 26 August 2019, a strong multicellular line of storms was
surging southeastward through the Kansas City, Missouri,
metropolitan area around 1601 UTC (Fig. 11a) with a â€˜â€˜bubbly-
likeâ€™â€™ texture in the visible reï¬‚ectance associated with over-
shooting tops (the ICP of the Kansas City storm was$90%).
Shortly thereafter, multiple severe wind reports were recorded
south of Kansas City, Missouri. By 1801 UTC, two elevated
ICP regions with overshooting tops and gravity waveâ€“like
patterns are apparent (Fig. 11b). Multiple severe wind reports
were associated with both high ICP regions shown in the
1921 UTC image (Fig. 11c). Later, the western storm segment
was moving into Arkansas with a strong overshooting top, vi-
sual evidence of an AACP, and ICP$ 90% (Fig. 11d). The
eastern storm weakened, but another storm quickly developed
in its wake with a maximum ICP$50% and a very pronounced
overshooting top and thermal couplet (Fig. 11d). Immediately
thereafter, the new storm intensiï¬ed (ICP$ 90%; Fig. 11e),
FIG. 8. Performance diagram for the validation and testing
datasets (solid red and dashed orange lines, respectively).
Intersections with the dashed gray lines indicate the frequency bias.
Red and orange circles represent the locations of select probability
values. Python code fromLagerquist and Gagne (2019)was used to
help create the plot.
FIG. 9. An attributes diagram for the validation and testing da-
tasets. Solid red and dashed orange lines represent the mean for the
validation and testing datasets, respectively, while the shaded red
and orange regions denote 95% conï¬dence intervals determined
by bootstrapping 1000 times. The inset image shows the frequency
of probability forecasts for the testing dataset only. The diagonal
gray 1-to-1 line represents perfect reliability or forecast calibration.
The horizontal gray line represents the â€˜â€˜climatologyâ€™â€™ or frequency
of the intense convection class for the training dataset (also called
the line of â€˜â€˜no resolutionâ€™â€™). The diagonal blue line represents the
line of â€˜â€˜no skillâ€™â€™ with respect to climatology, or where Brier skill
score is zero. This line separates the area where forecasts con-
tribute positively to the Brier skill score (shaded blue) and where
forecasts contribute negatively to the Brier skill score (white).
Python code fromLagerquist and Gagne (2019)was used to help
create the plot.
D
ECEMBER 2020 C I N T I N E O E T A L . 2577
with severe hail and wind reports following. The ICP of the
storm that moved into Arkansas decreased from above 90% to
below 25% (Fig. 11f), consistent with loss of robust textural
and thermal patterns. No severe reports were associated with
this storm after the ICP dropped below 25%. This example
illustrates that the CNN results are consistent with manual
interpretation of ABI imagery even when merging anvils from
multiple updraft regions complicate the scene.
3) KANSAS/MISSOURIâ€”15â€“16 AUGUST 2019
On 15â€“16 August 2019, a cold front initiated very strong
storms in northern Kansas. Between 2316 (Fig. 12a) and
0041 UTC (Fig. 12c) the model produces high probabilities
(ICP $ 50%) in regions associated with overshooting tops and
strong brightness temperature gradients on the edge of anvil
clouds. In the absence of sunlight, ABI CH13 brightness tem-
perature and the GLM FED are the only image inputs to the
CNN (Figs. 12dâ€“i), as the CH02 reï¬‚ectance becomes a trivial
predictor (i.e., it contains a value of zero everywhere). Even in
the absence of sunlight, the CNN continues to provide results
that are consistent with human interpretation of the imagery,
as the model favors regions with overshooting tops and cold-U
features, which were generally associated with severe reports
(Figs. 12dâ€“i). Robust FED cores were also present, which
boosted the ICP, particularly for the storms in Missouri
(Figs. 12gâ€“i).
4) ARIZONAâ€”23 S EPTEMBER 2019
In response to moisture and instability associated with a
500-hPa shortwave trough, numerous storms developed in
western Arizona on 23 September 2019. At 1631 UTC, the ICP
was $50% for two of the storms, likely due to the presence of
clear overshooting tops and moderate-to-strong brightness
temperature gradients around the cloud-top edges near the
primary overshoot region (Fig. 13a). By 1706 UTC, the west-
ernmost storm had an expanded area of ICP$ 50%, while the
eastern storm ICP decreased to,25% as cloud-top tempera-
tures warmed, the textural features softened, and the bright-
ness temperature gradient weakened (Fig. 13b). By 1721 UTC,
the western storm intensiï¬ed (ICP$ 90%), consistent with the
appearance of a pronounced overshooting top and AACP
(Fig. 13c). While features such as overshooting tops, the cold-U
FIG. 10. (a)â€“(f) A series of intense convection probability contours for storms in Wyoming and Nebraska on 10 Sep 2019. TheGOES-16
ABI visible reï¬‚ectance and a semitransparent infrared window image are used as the background. Severe weather reports (ï¬lled circles)
occurred within 60 min after the satellite scan time. Panel (f) depicts examples of the above-anvil cirrus plume (AACP), cold-U signature,
and overshooting tops.
2578 WEATHER AND FORECASTING V OLUME 35
pattern, and brightness temperature gradients help the CNN
produce good predictions, ambiguity in such features may also
lead to a bad prediction (e.g., the easternmost storm in
Fig. 13a).
5) ALASKAâ€”28 J UNE 2019
At 0249 UTC 28 June 2019, the NWS in Juneau, Alaska,
issued the ofï¬ceâ€™s ï¬rst ever severe thunderstorm warning.
Since this scene was outside of the GLM ï¬eld of view, a sep-
arate CNN was trained with ABI CH02 reï¬‚ectance and CH13
brightness temperature images (i.e., no GLM), along with
the scalar data discussed insection 2d.T h en e wC N Nw a s
deployed on this scene using GOES-17 1-min mesoscale
ABI scans (but the CNN was trained usingGOES-16 data).
Shortly before the storm was warned, it exhibited a cold-
ring feature (SetvÃ¡k et al. 2010), which is more apparent in
animations on the CIMSS Satellite Blog (Bachmeier 2019).
Despite lower proba bilities, the CNN correctly discrimi-
nates the intensity of the storm relative to the surrounding
convection, with the storm attaining a maximum ICP of
36% at 0243 UTC, while all neighboring convection ex-
hibited ICP , 5%. The lower probabilities may be, in part,
due to the absence of the GLM or the very high satellite
viewing angles which were not present in the training data.
Nevertheless, this example demonstrates that the CNN may be
able to generalize reasonably well to new geographic locations
and satellites (however, a model without sensor-speciï¬c scalar
data would still need to be evaluated).
c. Saliency and relevance maps
The saliency and relevance were computed for each two-
dimensional predictor using a number of samples from the
testing dataset. The pixel-wise saliency and relevance values
for ten true positive storm samples (Figs. 15 and 16) reveal
important features the model has learned. Saliency depicts how
changes in a predictor (increasing or decreasing the pixel
values) will increase the ï¬nal probability of intense convection,
while the relevance quantiï¬es the degree to which each pixel in
each predictor contributes to the predicted probability. The
relevance calculations use the â€˜â€˜alpha-betaâ€™â€™ rule ofa 5 1 and
b 5 0 (seeMontavon et al. 2019), which does not yield negative
relevance scores, but resulted in more coherent output than
rules withb . 0.
Features conducive to intense convection that were identi-
ï¬ed in the CH13 brightness temperature relevance map for ten
true positives (Figs. 15 and 16) include strong overshooting
tops (patches A, D, E, F, G, H, and I), portions of thermal
couplets and cold-U patterns (patches E and F), strong cloud-
edge brightness temperature gradients (patches B, C, D, and
H), and warm clear air pixels around anvil clouds (patches F,
H, I, and J). While robust overshooting tops and cold-U fea-
tures have been known for decades to be associated with
FIG. 11. As inFig. 10, but for storms in Missouri on 26 Aug 2019. Severe weather reports (ï¬lled circles) occurred within 60 min after the
satellite scan time.
DECEMBER 2020 C I N T I N E O E T A L . 2579
intense convection, it is encouraging that the CNN correctly
learned and encoded elements of these features. Cloud-edge
brightness temperature gradients are less known to be as-
sociated with intense convection, yet the model asserts that
these are important features in intense storms, even though
the human experts did not consciously consider this feature
when labeling the images. Furthermore, the model correctly
asserts other features known to be associated with in-
tense convection (e.g., overshooting tops), which provides
credibility that strong cloud-edge brightness temperature
FIG. 12. A series of intense convection probability contours for storms in Kansas and Missouri on 15â€“16 Aug 2019. (a)â€“(c) TheGOES-16
ABI visible reï¬‚ectance and a semitransparent infrared window image are used as the background when sunlight is present. In the absence
of sunlight, (d)â€“(f) the infrared window alone serves as the background, while (g)â€“(i) the GLM ï¬‚ash-extent density is also shown for the
corresponding image in the second row. Each orange rectangle encapsulates the same ABI scan time. Severe weather reports (ï¬lled
circles) occurred within 60 min after the image time.
2580 WEATHER AND FORECASTING V OLUME 35
gradients are not simply an important feature discovered by
accident.
The CH13 saliency indicates which pixels to make colder
(blue) or warmer (red) in the 10.35-mm brightness temperature
to increase the ICP (Figs. 15and 16). While all storm samples
indicate that colder overshooting tops would help, patches C,
E, and H indicate stronger cloud-edge brightness temperature
gradients would be conducive to higher ICP.
The relevance maps for CH02 reï¬‚ectance indicate that the
CNN identiï¬es â€˜â€˜bubbly-likeâ€™â€™ texture features in overshooting
tops and cloud tops in general as important (Figs. 15and 16), as
well as less cloudy pixels near the edge of anvil clouds (patches
A, B, I, and J)â€”the latter perhaps highlighting the importance
of storm isolation. While not shown, the saliency for CH02
reï¬‚ectance demonstrated that higher texture in regions within
and near overshooting tops (e.g., emanating gravity waves)
would increase the ICP of the samples. In other words, the
model has learned that increased texture in relatively high-
texture regions is important, which is a similar basis for pre-
vious visible cloud-top texture rating research (Bedka and
Khlopenkov 2016) and agrees well with the conclusion of
SandmÃ¦l et al. (2019), that higher texture is correlated with
stronger upward motion. More work is needed to evaluate the
effect of solar-zenith angle on the contribution of CH02 re-
ï¬‚ectance to the CNN.
From a lightning-mapping perspective, the relevance maps
for the GLM ï¬‚ash-extent density (Figs. 15 and 16) seem to
indicate that higher values of ï¬‚ash-extent density are more
FIG. 13. As inFig. 10, but for storms in Arizona on 23 Sep 2019. This storm was warned by the U.S. National Weather Service, but no severe
hazards were reported.
FIG. 14. As inFig. 10, but for storms in the Alaska Panhandle on 28 Jun 2019. Note that the
contoured probabilities are for the 5%, 15%, and 25% thresholds.
DECEMBER 2020 C I N T I N E O E T A L . 2581
relevant than lower values, in general, with marked increases in
relevance where ï¬‚ash-extent density is at least 10 ï¬‚ashes per
5 min (see patches B, G, I, and J). The saliency map for ï¬‚ash-
extent density is not shown, but was much noisier than the ABI
channels, with no clear patterns emerging, making physical
interpretation difï¬cult.
From the ï¬ve false positive storm patches inFig. 17(each
with a probability . 87%), similar saliency and relevance
patterns emerged as being important, such as overshooting
tops, brightness temperature gradients, and more textured
CH02 reï¬‚ectance. Compared to the ten true positives, the false
positives appear to have weaker overshooting tops in the CH13
brightness temperature and less overall texture in the CH02
reï¬‚ectance. For the ï¬ve false positives shown, regions of high
relevance were found for the GLM ï¬‚ash-extent density, indi-
cating perhaps that the model erroneously put too much
FIG. 15. Saliency and relevance plots for ABI CH13 brightness temperature (BT) and LRP plots for ABI CH02 reï¬‚ectance (ref.) and
GLM ï¬‚ash-extent density (FED) for ï¬ve true positive storm samples from the validation dataset shown in rows labeled from A to E. The
images in the ï¬rst column are CH13 BT/CH02 reï¬‚ectance â€˜â€˜sandwichâ€™â€™ imagery with GLM ï¬‚ash-extent density contours for 10, 20, and
40 ï¬‚ashes per 5 min overlaid in shades of purple. The predicted probability for each image patch was.99%.
2582 WEATHER AND FORECASTING V OLUME 35
importance on these regions of elevated ï¬‚ash-extent density,
contributing to the false positive predictions.
Because the hyperparameters chosen for the LRP analysis
only show which pixels contribute to the probability of the
intense convection class, relevance is near zero everywhere for
each predictor for the ï¬ve false negatives inFig. 18, as each
storm patch had a probability, 1%. Compared to the true
positives, these storm patches had less area of cold brightness
temperatures, less pronounced (or absent) overshooting tops,
and smaller areas of high-textured CH02-reï¬‚ectance. Two of
the patches inFigs. 18b and 18dhad diminished GLM ï¬‚ash-
extent density, compared with most of the true positives. These
patches also appear to be at less mature stages of development
than the true or false positives. The CH13 brightness temper-
ature saliency shows that larger and colder cloud-top regions
would increase the probability of intense convection.
Interestingly, one feature that the model did not appear to
explicitly associate with intense convection is the AACP
(Bedka et al. 2018), particularly its unique manifestation in the
CH02 reï¬‚ectance. However, model testing indicates that many
FIG. 16. As inFig. 15, but for ï¬ve different true positive samples from the validation dataset. The predicted probability for each image
patch was.99%.
DECEMBER 2020 C I N T I N E O E T A L . 2583
storms with AACPs are correctly identiï¬ed as intense con-
vection (see Figs. 10â€“14 and Cintineo 2019). While not ex-
plicitly mapped out by the diagnostic tools, the driving force
behind the AACP (an intense overshoot) and its infrared
presentation (cold-U) are key features identiï¬ed by the model.
d. Permutation tests
Two permutation tests were performed on the trained CNN
using Keras code examples fromLagerquist and Gagne (2019).
The cost function used in the permutation test is negative AUC;
since the permutation test aims to minimize the cost function, in
this case it aims to maximize AUC. Since ABI CH02 0.64-mm
reï¬‚ectance is a trivial predictor after sunset, the permutation tests
were performed on storm samples from the validation dataset
where the solar-zenith angle was less than 858 (n 5 36 900). The
B01 m e t h o dc a nb et h o u g h to fa s ,â€˜ â€˜ t h es k i l la sar e s u l to fp e r -
muting only the kth predictor,â€™â€™ with thekth-most important
predictor resulting in thekth-greatest decrease in AUC. TheL15
method can be thought of as, â€˜â€˜the skill as a result of permuting
the kth predictorand each more important predictor.â€™â€™
For both the L15 and B01 permutation methods (see
Fig. 19), the â€˜â€˜No permutationâ€™â€™ bar represents the original
FIG. 17. As inFig. 15, but for ï¬ve false positive storm samples from the validation dataset. The predicted probability for each image patch
was .87%.
2584 WEATHER AND FORECASTING V OLUME 35
AUC value for the full CNN model for this daytime-only
sample (AUC50.986). Since the ï¬rst step of theL15 method is
identically the B01 method, it was found that ABI CH13
brightness temperature was the most important predictor for
both methods, with an AUC5 0.700 after permutation of that
channel. The B01 method found that CH02 reï¬‚ectance and
FED were the next two important predictors, followed by the
four scalar predictors. TheL15 method found that FED was
the 2nd most important predictor, followed by CH02. This is
likely due to the high correlation that exists between the ABI
channels, relative to the correlation between ABI channels and
FED. The satellite-zenith angle was the fourth most important
predictor in each test, while the mean latitude and mean lon-
gitude were swapped in importance for the two methods. By
itself, theB01 test shows that CH02 reï¬‚ectance contains more
information than FED, but theL15 test reveals that once CH13
brightness temperature samples are randomized, the CH02
reï¬‚ectance does not contain as much additional or indepen-
dent information as FED.
4. Discussion and conclusions
A machine-learning model that exploits the rich spatial and
spectral information provided by theGOES-16 ABI and the
lightning mapping provided by theGOES-16 GLM was de-
veloped with the goal of automatically identifying intense
midlatitude convection consistent with human expert inter-
pretation of the satellite images. Over 220 000 images were
manually labeled to enable training of a convolutional neural
network (CNN), which learned to make skillful predictions of
intense convection both day and night.
The CNN learned several features, identiï¬able using a
combination of high-resolution visible reï¬‚ectance imagery,
infrared window imagery, and lightning mapping imagery, that
are known to be associated with intense convection. Model
diagnostic tools and examples showed that the model learned
to recognize overshooting tops, portions of cold-U thermal
patterns, cold rings, and robust lightning activity near updraft
regions. The model also learned to recognize cloud-edge bright-
ness temperature gradients as important, which is a new and
unique result for satellite interpretation of intense convection.
Strong gradients in the brightness temperature may arise from a
combination of a strong overshooting top associated with the
updraft region (producing a very cold minimum in the brightness
temperature) and strong upper-level winds limiting upstream
propagation of the anvil cloud. The strength of the upper-level
winds may be an indication of the amount deep-layer shear, well
known to either support or diminish intense convection. However,
further analysis of model attributes and associated physical pro-
cesses is the subject of future work.
A successive rank permutation test revealed that the most
important predictors were ABI infrared-window brightness tem-
perature (ï¬rst), GLM ï¬‚ash-extent density imagery (second), and
ABI visible reï¬‚ectance (third), while the permutation test with
replacement (Breiman 2001) ranked the top three predictors as
ABI infrared-window brightness temperature (ï¬rst), ABI visible
reï¬‚ectance (second), and GLM ï¬‚ash-extent density (third).
Despite the potential for reduced lightning detection efï¬ciency in
FIG. 18. Saliency plots for ABI CH13 brightness temperature
(BT) for ï¬ve false negative storms. The images in the ï¬rst column
are CH13 BT/CH02 reï¬‚ectance â€˜â€˜sandwichâ€™â€™ imagery with GLM
ï¬‚ash-extent density contours for 10, 20, and 40 ï¬‚ashes per 5 min
overlaid in shades of purple. The predicted probability for each
image patch was,1%.
D
ECEMBER 2020 C I N T I N E O E T A L . 2585
some optically deep clouds (e.g.,B u r n e t te ta l .2 0 1 8), the GLM
added appreciable skill to the model. The model also exhibited
gainful lead time to initial severe weather reports in the testing
dataset (median of 21 min with a 95% conï¬dence interval of 18â€“
26 min, measured from the 90% probability threshold).
Given the clear importance of the infrared-window bright-
ness temperature, there may be interest in the community to
train a model with solely infrared imager channels, for the goal
of deploying it on the historical record of spaceborne infrared
imager observations to investigate climatological trends in intense
convection. However, more work is needed to discern if the
infrared-window brightness temperature alone can faithfully
identify intense convection, and what the effect of reduced spatial
resolution may be (many historical imagers have 4-km horizontal
resolution for infrared channels), which could prove signiï¬cant.
The CNN model output could complement and enhance
radar interrogation of storms and could be incorporated into
nowcasting applications such as ProbSevere, with the goal of
improving severe weather warnings. The CNN model may be
particularly valuable for diagnosing and nowcasting convec-
tion in regions with limited or no radar coverage (see the ex-
ample from Alaska, Fig. 14). In addition, the model shows
promise for generalizing to other satellite sensors with very
similar spectral and spatial attributes to the GOES-R ABI
(e.g., Himawari-8, Meteosat Third Generation, and GEO
KOMPSAT-2A), eventually allowing for objective large-scale
monitoring of convection, and possibly providing a new
framework for the study of convection.
Acknowledgments. The authors acknowledge the National
Oceanic and Atmospheric Administration (NOAA) High
Performance Computing and Communications Program (HPCC)
for supporting this research. We are also grateful to Eric Bruning
(Texas Tech University) for his assistance with the glmtools
software package, as well as to three anonymous reviewers for
their helpful comments, improving the manuscript. TheGOES-16
data used in this study can be freely obtained from NOAAâ€™s
Comprehensive Large Array Data Stewardship System (CLASS;
online athttps://www.class.noaa.gov/). The ProbSevere data in
this study may be obtained from UW-CIMSS upon request of the
corresponding author. The views, opinions, and ï¬ndings con-
tained in this paper are those of the authors and should not be
construed as an ofï¬cial National Oceanic and Atmospheric
Administration or U.S. government position, policy, or decision.
REFERENCES
Adler, R. F., and D. D. Fenn, 1979:Thunderstorm intensity as deter-
mined from satellite data.J. Appl. Meteor., 18, 502â€“517,https://
doi.org/10.1175/1520-0450(1979)018,0502:TIADFS.2.0.CO;2.
Alber, M., and Coauthors, 2019: iNNvestigate neural networks!
J. Mach. Learn. Res. , 20, 1â€“8.
Apke, J. M., J. R. Mecikalski, and C. P. Jewett, 2016: Analysis of
mesoscale atmospheric ï¬‚ows above mature deep convec-
tion using super rapid scan geostationary satellite data.
J. Appl. Meteor. Climatol. , 55, 1859â€“1887, https://doi.org/
10.1175/JAMC-D-15-0253.1 .
Bachmeier, S., 2019: NWS Juneau, Alaska issues their ï¬rst-ever
severe thunderstorm warningâ€”Based on satellite imagery.
CIMSS Satellite Blog, accessed 1 December 2019.https://
cimss.ssec.wisc.edu/satellite-blog/archives/34080 .
Bedka, K., and K. Khlopenkov, 2016: A probabilistic multispectral
pattern recognition method for detection of overshooting
cloud tops using passive satellite imager observations.J. Appl.
Meteor. Climatol. , 55, 1983â€“2005, https://doi.org/10.1175/
JAMC-D-15-0249.1 .
â€”â€”, E. M. Murillo, C. R. Homeyer, B. Scarino, and H. Mersiovsky,
2018: The above-anvil cirrus plume: An important severe
FIG. 19. The (left) successive permutation rank test and (right) single permutation rank test for the trained
convolutional neural network of this paper, using 36 900 daytime-only storm samples from the validation dataset.
â€˜â€˜No permutationâ€™â€™ represents the original AUC without any predictors permuted. For both panels, predictors
increase in importance moving from the bottom to the top. ABI5 Advanced Baseline Imager; GLM 5
Geostationary Lightning Mapper; BT5 brightness temperature.
2586 WEATHER AND FORECASTING V OLUME 35
weather indicator in visible and infrared satellite imagery.
Wea. Forecasting , 33, 1159â€“1181, https://doi.org/10.1175/
WAF-D-18-0040.1 .
Binder, A., G. Montavon, S. Lapuschkin, K. R. Muller, and
W. Samek, 2016: Layer-wise relevance propagation for neural
networks with local renormalization layers. Int. Conf. on
Artiï¬cial Neural Networks andMachine Learningâ€”ICANN
2016, Barcelona, Spain, Springer, 63â€“71.
Breiman, L., 2001: Random forests.Mach. Learn., 45, 5â€“32,https://
doi.org/10.1023/A:1010933404324.
Bruning, E., 2019: glmtools. GitHub, accessed 15 July 2018,https://
github.com/deeplycloudy/glmtools.
Brunner, J. C., S. A. Ackerman, A. S. Bachmeier, and R. M. Rabin,
2007: A quantitative analysis of the enhanced-V feature in
relation to severe weather. Wea. Forecasting , 22, 853â€“872,
https://doi.org/10.1175/WAF1022.1.
Burnett, C., R. F. Garret, W. M. MacKenzie Jr., M. Seybold,
J. Fulbright, and J. D. Sims, 2018: GLM observations through
optically deep clouds: A case study.22nd Conf. on Satellite
Meteorology and Oceanography, Austin, TX, Amer. Meteor.
Soc., 628, https://ams.confex.com/ams/98Annual/webprogram/
Paper333686.html.
Chollet, F., 2015: Keras. GitHub, accessed 10 February 2019,https://
github.com/keras-team/keras.
Cintineo, J. L., 2019: The probability of â€˜â€˜intense convectionâ€™â€™ using
geostationary satellite data.CIMSS Satellite Blog,S .B a c h m e i e r ,
Ed., Cooperative Institute of Meteorological Satellite Studies,
https://cimss.ssec.wisc.edu/satellite-blog/archives/34480.
â€”â€”, M. J. Pavolonis, J. M. Sieglaff, and D. T. Lindsey, 2014: An
empirical model for assessing the severe weather potential of
developing convection.Wea. Forecasting, 29, 639â€“653,https://
doi.org/10.1175/WAF-D-13-00113.1.
â€”â€”, and Coauthors, 2018: The NOAA/CIMSS ProbSevere model:
Incorporation of total lightning and validation.Wea. Forecasting,
33, 331â€“345,https://doi.org/10.1175/WAF-D-17-0099.1.
â€” â€” ,M .P .P a v o l o n i s ,J .M .S i e g l a f f ,A .W i m m e r s ,a n dJ .C .
Brunner, 2020: NOAA ProbSevere v2.0â€”ProbHail, ProbWind,
and ProbTor.Wea. Forecasting, 35, 1523â€“1543,https://doi.org/
10.1175/WAF-D-19-0242.1.
Efron, B., and R. Tibshirani, 1986: Bootstrap methods for standard
errors, conï¬dence intervals, and other measures of statis-
tical accuracy. Stat. Sci. , 1
, 54â€“75,https://doi.org/10.1214/ss/
1177013815 .
Fukushima, K., 1980: Neocognitronâ€”A self-organizing neural
network model for a mechanism of pattern-recognition unaf-
fected by shift in position.Biol. Cybern. , 36, 193â€“202,https://
doi.org/10.1007/BF00344251.
Gagne, D. J., II, S. E. Haupt, D. W. Nychka, and G. Thompson,
2019: Interpretable deep learning for spatial analysis of severe
hailstorms. Mon. Wea. Rev. , 147, 2827â€“2845, https://doi.org/
10.1175/MWR-D-18-0316.1.
Goodfellow, I., Y. Bengio, and A. Courville, Eds., 2016: Back-
propagation and other differentiation algorithms. Deep
Learning , MIT Press, 200â€“220.
Goodman, S. J., and Coauthors, 2013: The GOES-R Geostationary
Lightning Mapper (GLM). Atmos. Res. , 125â€“126, 34â€“49,
https://doi.org/10.1016/j.atmosres.2013.01.006.
He, K. M., X. Y. Zhang, S. Q. Ren, and J. Sun, 2016: Deep residual
learning for image recognition.2016 IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR) , Seattle, WA, IEEE,
770â€“778.
Hinton, G., N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, 2012: Improving neural networks by
preventing co-adaptation of feature detectors. arXiv e-prints,
https://arxiv.org/pdf/1207.0580.pdf.
Hoerl, A. E., and R. W. Kennard, 2000: Ridge regression: Biased
estimation for nonorthogonal problems.Technometrics, 42,
80â€“86, https://doi.org/10.1080/00401706.2000.10485983.
Homeyer, C. R., J. D. McAuliffe, and K. M. Bedka, 2017: On the
development of above-anvil cirrus plumes in extratropical
convection. J. Atmos. Sci. , 74, 1617â€“1633, https://doi.org/
10.1175/JAS-D-16-0269.1.
Hsu, W. R., and A. H. Murphy, 1986: The attributes diagramâ€”A
geometrical framework for assessing the quality of probability
forecasts.Int. J. Forecasting, 2, 285â€“293,https://doi.org/10.1016/
0169-2070(86)90048-8.
Ioffe, S., and C. Szegedy, 2015: Batch normalization: Accelerating
deep network training by reducing internal covariate shift.Int.
Conf. on Machine Learning , Lille, France, JMLR, 448â€“456.
Krizhevsky, A., I. Sutskever, and G. E. Hinton, 2012: ImageNet
classiï¬cation with deep convolutional neural networks.
Commun. ACM, 60, 84â€“90,https://doi.org/10.1145/3065386.
Lagerquist, R., and D. J. Gagne II, 2019: Interpretation of deep
learning for predicting thunderstorm rotation: Python tutorial.
GitHub, accessed 4 March 2019,https://github.com/djgagne/
ams-ml-python-course/blob/master/module_4/ML_Short_Course_
Module_4_Interpretation.ipynb.
â€”â€”, A. McGovern, and D. J. Gagne, 2019: Deep learning for spa-
tially explicit prediction of synoptic-scale fronts.
Wea. Forecasting,
34, 1137â€“1160,https://doi.org/10.1175/WAF-D-18-0183.1.
â€”â€”, â€”â€”, C. R. Homeyer, D. J. Gagne II, and T. Smith, 2020:
Deep learning on three-dimensional multiscale data for next-
hour tornado prediction. Mon. Wea. Rev. , 148, 2837â€“2861,
https://doi.org/10.1175/MWR-D-19-0372.1.
Lakshmanan, V., C. Karstens, J. Krause, K. Elmore, A. Ryzhkov, and
S. Berkseth, 2015: Which polarimetric variables are important for
weather/no-weather discrimination?J. Atmos. Oceanic Technol.,
32, 1209â€“1223,https://doi.org/10.1175/JTECH-D-13-00205.1.
Li, F., J. Johnson, and S. Yeung, 2020: CS231n convolutional neural
networks for visual recognition. GitHub, accessed 10 October
2019, http://cs231n.github.io/convolutional-networks/.
Li, Y., H. K. Zhang, X. Z. Xue, Y. A. Jiang, and Q. Shen, 2018: Deep
learning for remote sensing image classiï¬cation: A survey.Wiley
Interdiscip. Rev., 8, e1264,https://doi.org/10.1002/widm.1264.
Litjens, G., and Coauthors, 2017: A survey on deep learning in
medical image analysis.Med. Image Anal. , 42, 60â€“88,https://
doi.org/10.1016/j.media.2017.07.005.
McGovern, A., R. Lagerquist, D. J. Gagne, G. E. Jergensen, K. L.
Elmore, C. R. Homeyer, and T. Smith, 2019: Making the black
box more transparent: Understanding the physical implica-
tions of machine learning.Bull. Amer. Meteor. Soc., 100, 2175â€“
2199, https://doi.org/10.1175/BAMS-D-18-0195.1.
M e n z e l ,W .P . ,a n dJ .F .W .P u r d o m ,1994: Introducing GOES-Iâ€”The
1st of a new-generation of geostationary operational environ-
mental satellites.Bull. Amer. Meteor. Soc., 75, 757â€“782,https://
doi.org/10.1175/1520-0477(1994)075,0757:IGITFO.2.0.CO;2.
Metz, C., 1978: Basic principles of ROC analysis.Semin. Nucl. Med.,
8, 283â€“298,https://doi.org/10.1016/S0001-2998(78)80014-2.
Montavon, G., A. Binder, S. Lapuschkin, W. Samek, and K. MÃ¼ller,
2019: Layer-wise relevance propagation: An overview.
Explainable AI: Interpreting, Explaining and Visualizing
Deep Learning, W. Samek et al., Eds., Springer, 193â€“210,
https://doi.org/10.1007/978-3-030-28954-6_10 .
Nair, V., and G. Hinton, 2010: Rectiï¬ed linear units improve restricted
Boltzmann machines.Proc. 27th Int. Conf. on Machine Learning,
Haifa, Israel, International Machine Learning Society, 807â€“814.
DECEMBER 2020 C I N T I N E O E T A L . 2587
NOAA, 2019: Storm events database. NOAA, accessed 12 January
2020, https://www.ncdc.noaa.gov/stormevents/.
Purdom, J., 1976: Some uses of high-resolution GOES imagery
in mesoscale forecasting of convection and its behavior.
Mon. Wea. Rev. , 104, 1474â€“1483, https://doi.org/10.1175/
1520-0493(1976)104 ,1474:SUOHRG .2.0.CO;2 .
Ronneberger, O., P. Fischer, and T. Brox, 2015: U-Net: Convolutional
networks for biomedical imagesegmentation. MICCAI 2015:
Medical Image Computing and Computer-Assisted Intervention,
N. Navab et al., Eds., Springer, 234â€“241,https://doi.org/10.1007/
978-3-319-24574-4_28.
Rudlosky, S. D., S. J. Goodman, K. S. Virts, and E. C. Bruning,
2019: Initial geostationary lightning mapper observations.
Geophys. Res. Lett. , 46, 1097â€“1104, https://doi.org/10.1029/
2018GL081052.
SandmÃ¦l, T. N., C. R. Homeyer, K. M. Bedka, J. M. Apke, J. R.
Mecikalski, and K. Khlopenkov, 2019: Evaluating the ability
of remote sensing observations to identify signiï¬cantly severe
and potentially tornadic storms.J. Appl. Meteor. Climatol. , 58,
2569â€“2590, https://doi.org/10.1175/JAMC-D-18-0241.1.
Schmidhuber, J., 2015: Deep learning in neural networks: An
overview. Neural Networks, 61,8 5 â€“ 1 1 7 ,https://doi.org/10.1016/
j.neunet.2014.09.003.
Schmit, T. J., M. M. Gunshor, W. P. Menzel, J. J. Gurka, J. Li, and
A. S. Bachmeier, 2005: Introducing the next-generation ad-
vanced baseline imager on GOES-R. Bull. Amer. Meteor.
Soc. , 86, 1079â€“1096, https://doi.org/10.1175/BAMS-86-
8-1079 .
â€”â€”, and Coauthors, 2015: Rapid refresh information of signiï¬cant
events: Preparing users for the next generation of geosta-
tionary operational satellites. Bull. Amer. Meteor. Soc. , 96,
561â€“576, https://doi.org/10.1175/BAMS-D-13-00210.1.
Schultz, C. J., W. A. Petersen, and L. D. Carey, 2011: Lightning and
severe weather: A comparison between total and cloud-to-ground
lightning trends.Wea. Forecasting, 26, 744â€“755,https://doi.org/
10.1175/WAF-D-10-05026.1.
SetvÃ¡k, M., and Coauthors, 2010: Satellite-observed cold-ring-
shaped features atop deep convective clouds.Atmos. Res. ,
97, 80â€“96,https://doi.org/10.1016/j.atmosres.2010.03.009.
â€”â€”, K. Bedka, D. T. Lindsey, A. Sokol, Z. Charvat, J. Stâ€™astka,
and P. K. Wang, 2013: A-Train observations of deep convec-
tive storm tops. Atmos. Res.
, 123, 229â€“248, https://doi.org/
10.1016/j.atmosres.2012.06.020.
Simonyan, K., A. Vedaldi, and A. Zisserman, 2014: Deep inside
convolutional networks: Visualising image classiï¬cation models
and saliency maps. Workshop at Int. Conf. on Learning
Representations , Banff, Canada, ICLR, 1â€“8, https://iclr.cc/
archive/2014/workshop-proceedings/ .
Smith, T. M., and Coauthors, 2016: Multi-Radar Multi-Sensor
(MRMS) severe weather and aviation products initial oper-
ating capabilities. Bull. Amer. Meteor. Soc. , 97, 1617â€“1630,
https://doi.org/10.1175/BAMS-D-14-00173.1.
Valachov Ã¡,M . ,a n dM .S e t vÃ¡k, 2017: Satellite monitoring of
the convective storms: Forecastersâ€™ point of view. Czech
Hydrometeorological Institute, 48 pp.,https://www.ecmwf.int/
sites/default/ï¬les/elibrary/2017/17282-satellite-monitoring-
convective-storms-fore casters-point-view.pdf .
Wang, P. K., 2003: Moisture plumes above thunderstorm anvils
and their contributions to cross-tropopause transport of water
vapor in midlatitudes. J. Geophys. Res. , 108, 4194, https://
doi.org/10.1029/2002JD002581.
â€”â€”, 2007: The thermodynamic structure atop a penetrating con-
vective thunderstorm. Atmos. Res. , 83, 254â€“262, https://
doi.org/10.1016/j.atmosres.2005.08.010.
â€”â€”, K. Y. Cheng, M. Setvak, and C. K. Wang, 2016: The origin of
the gullwing-shaped cirrus above an Argentinian thunder-
storm as seen in CALIPSO images.J. Geophys. Res. Atmos. ,
121, 3729â€“3738,https://doi.org/10.1002/2015JD024111.
2588 WEATHER AND FORECASTING V OLUME 35
--- END OF waf-d-20-0028.1.pdf ---

